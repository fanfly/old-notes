\chapter{Linear Transformations}
\section{Linear Transformations, Null Spaces and Ranges}
\begin{definition}
  Let $f: X \to Y$ be a function.
  \begin{itemize}
    \item $f$ is \emph{injective} (i.e., $f$ is an \emph{injection})
      if $T(x) = T(x')$ implies $x = x'$ for $x, x' \in X$.
    \item $f$ is \emph{surjective} (i.e., $f$ is a \emph{surjection})
      if for each $y \in Y$, there exists some $x \in X$ with $T(x) = y$.
    \item $f$ is \emph{bijective} (i.e., $f$ is a \emph{bijection})
      if $f$ is injective and surjective.
  \end{itemize}
\end{definition}
\begin{remark}
  If both domain and codomain of a function are vector spaces, then the
  function is usually said to be a \emph{transformation}.
  Furthermore, it is said to be an \emph{operator} if its domain and codomain
  are the same.
\end{remark}

\begin{definition}
  Let $V$ and $W$ be vector spaces over $F$.
  A transformation $T: V \to W$ is \emph{linear} if the following statements
  hold.
  \begin{enumerate}
    \item $T(x + y) = T(x) + T(y)$ for all $x, y \in V$.
    \item $T(ax) = aT(x)$ for all $a \in F$ and $x \in V$.
  \end{enumerate}
  The set of all linear transformations from $V$ to $W$ is denoted by
  $\mathcal{L}(V, W)$.
  In the case that $V = W$, we write $\mathcal{L}(V)$ for short.
\end{definition}

\begin{example}
  The \emph{zero transformation} from $V$ to $W$ is the transformation
  $O_{V, W}: V \to W$ that satisfies $O_{V, W}(x) = 0_W$
  for all $x \in V$.
  It is clear that $O_{V, W} \in \mathcal{L}(V, W)$.
\end{example}
\begin{example}
  The \emph{identity transformation} on $V$ is the transformation
  $I_V: V \to V$ that satisfies $I_V(x) = x$ for all $x \in V$.
  It is clear that $I_{V} \in \mathcal{L}(V)$.
\end{example}
\begin{example}
  Recall that $\mathcal{P}(F)$ is the set of polynomials with coefficients in
  $F$.
  \begin{itemize}
    \item The differential operator
      $D: \mathcal{P}(\mathbb{R}) \to \mathcal{P}(\mathbb{R})$
      with $D(f) = f'$ for $f \in \mathcal{P}(\mathbb{R})$, where $f'$ is the
      derivative of $f$, is linear.
    \item The operator
      $T: \mathcal{P}(\mathbb{R}) \to \mathcal{P}(\mathbb{R})$ such that
      for $f \in \mathcal{P}(\mathbb{R})$,
      \begin{equation*}
        (T(f))(x) = \int_0^x f(t) dt
      \end{equation*}
      for all $x \in \mathbb{R}$, is linear.
  \end{itemize}
\end{example}

\begin{theorem}
  If $V$ and $W$ are vector spaces over $F$, then $\mathcal{L}(V, W)$ is also
  a vector space over $F$.
\end{theorem}
\begin{proof}
  $\mathcal{L}(V, W)$ is a vector space because it is a subspace of
  $\mathcal{F}(V, W)$, which is proved as follows.
  \begin{enumerate}
    \item If $T_1, T_2 \in \mathcal{L}(V, W)$, then $T_1 + T_2$ is linear
      because
      \begin{align*}
        (T_1 + T_2)(x + y)
        &= T_1(x + y) + T_2(x + y) \\
        &= T_1(x) + T_1(y) + T_2(x) + T_2(y) \\
        &= T_1(x) + T_2(x) + T_1(y) + T_2(y) \\
        &= (T_1 + T_2)(x) + (T_1 + T_2)(y)
      \end{align*}
      and
      \begin{align*}
        (T_1 + T_2)(cx)
        &= T_1(cx) + T_2(cx) \\
        &= cT_1(x) + cT_2(x) \\
        &= c(T_1(x) + T_2(x)) \\
        &= c(T_1 + T_2)(x)
      \end{align*}
      hold for $x, y \in V$ and $c \in F$.
    \item If $T \in \mathcal{L}(V, W)$ and $a \in F$, then $aT$ is linear
      because
      \begin{align*}
        (aT)(x + y)
        &= aT(x + y) \\
        &= a(T(x) + T(y)) \\
        &= aT(x) + aT(y) \\
        &= (aT)(x) + (aT)(y)
      \end{align*}
      and
      \begin{align*}
        (aT)(cx) = aT(cx) = a(cT(x)) = c(aT(x)) = c(aT)(x)
      \end{align*}
      hold for $x, y \in V$ and $c \in F$.
    \item It is clear that $O_{V, W} \in \mathcal{L}(V, W)$.
      \qedhere
  \end{enumerate}
\end{proof}

\begin{theorem}\label{thm:linear-span}
  Let $V$ and $W$ be vector spaces over $F$, and let $T: V \to W$ be linear.
  Let $S$ be a subset of $V$ and let $U$ be a subspace of $V$.
  Then the following statements are true.
  \begin{enumerate}
    \item If $n$ is a nonnegative integer, then for $a_1, \dots, a_n \in F$
      and $x_1, \dots, x_n \in V$, we have
      \begin{equation*}
        T\left(\sum_{i=1}^n a_ix_i\right) = a_i\sum_{i=1}^n T(x_i).
      \end{equation*}
    \item If $S$ spans $U$, then $T(S)$ spans $T(U)$.
  \end{enumerate}
\end{theorem}
\begin{proof} \leavevmode
  \begin{enumerate}
    \item The proof is by induction on $n$. For $n = 0$, it holds trivially.
      If the statement is true for some $n \geq 0$, then we have
      \begin{align*}
        T(a_1x_1 + \cdots + a_nx_n + a_{n+1}x_{n+1})
        &= T(a_1x_1 + \cdots + a_nx_n) + T(a_{n+1}x_{n+1}) \\
        &= a_1T(x_1) + \cdots + a_nT(x_n) + a_{n+1}T(x_{n+1}).
      \end{align*}
      Thus, the statement is true for nonnegative integer $n$.
    \item We prove that $\mathrm{span}(T(S)) = T(U)$.
      If $y \in \mathrm{span}(T(S))$, then there exist
      $a_i \in F$, $x_i \in S$ for $i \in \{1, \dots, n\}$ such that
      \begin{equation*}
        y = \sum_{i=1}^n a_iT(x_i)
          = T\left(\sum_{i=1}^n a_ix_i\right) \in T(U),
      \end{equation*}
      so $\mathrm{span}(T(S)) \subseteq T(U)$.

      If $y \in T(U)$, then there exist
      $a_i \in F$, $x_i \in S$ for $i \in \{1, \dots, n\}$ such that
      \begin{equation*}
        y = T\left(\sum_{i=1}^n a_ix_i\right)
          = \sum_{i=1}^n a_iT(x_i) \in \mathrm{span}(T(S)),
      \end{equation*}
      so $T(U) \subseteq \mathrm{span}(T(S))$.
      Thus, $\mathrm{span}(T(S)) = T(U)$. \qedhere
  \end{enumerate}
\end{proof}

\begin{definition}
  Let $V$ and $W$ be vector spaces over $F$, and let $T: V \to W$ be linear.
  \begin{itemize}
    \item The \emph{null space} $\mathcal{N}(T)$ of $T$ is the set of vectors
      $x \in V$ with $T(x) = 0_W$; that is,
      \begin{equation*}
        \mathcal{N}(T) = \{x \in V: T(x) = 0_W\}.
      \end{equation*}
    \item The \emph{range} $\mathcal{R}(T)$ of $T$ is the image of $V$ under
      $T$; that is,
      \begin{equation*}
        \mathcal{R}(T) = \{T(x): x \in V\}.
      \end{equation*}
  \end{itemize}
\end{definition}
\begin{example}
  Let $D: \mathcal{P}(\mathbb{R}) \to \mathcal{P}(\mathbb{R})$ be the
  differential operator. Then
  \begin{equation*}
    \mathcal{N}(D) = \{a_0: a_0 \in \mathbb{R}\}
    \quad \text{and}
    \quad \mathcal{R}(D) = \mathcal{P}(\mathbb{R}).
  \end{equation*}
\end{example}

\begin{theorem}\label{thm:kernel-range-subspace}
  Let $V$ and $W$ be vector spaces over $F$, and let $T: V \to W$ be linear.
  Then $\mathcal{N}(T)$ and $\mathcal{R}(T)$ are subspaces of $V$ and $W$,
  respectively.
\end{theorem}
\begin{proof} \leavevmode
  \begin{enumerate}
    \item Let $x, x' \in \mathcal{N}(T)$ and $a \in F$. Then we have
      $T(x + x') = T(x) + T(x') = 0_W + 0_W = 0_W$,
      $T(ax) = aT(x) = a0_W = 0_W$
      and $T(0_V) = 0_W$.
      Thus, $\mathcal{N}(T)$ is a subspace of $V$.
    \item Let $y, y' \in \mathcal{R}(T)$ and $a \in F$. There exist
      $x, x' \in V$ with $y = T(x)$ and $y' = T(x')$. Then we have
      $y + y' = T(x) + T(x') = T(x + x')$,
      $ay = aT(x) = T(ax)$
      and $0_W = T(0_V)$.
      Thus, $\mathcal{R}(T)$ is a subspace of $W$. \qedhere
  \end{enumerate}
\end{proof}

\begin{definition}
  Let $V$ and $W$ be vector spaces over $F$, and let $T: V \to W$ be linear.
  \begin{itemize}
    \item The \emph{nullity} of $T$, denoted by $\mathrm{nullity}(T)$,
      is the dimension of $\mathcal{N}(T)$.
    \item The \emph{rank} of $T$, denoted by $\mathrm{rank}(T)$,
      is the dimension of $\mathcal{R}(T)$.
  \end{itemize}
\end{definition}

\begin{theorem}[Rank-nullity Theorem]\label{thm:rank-nullity}
  Let $V$ and $W$ be vector spaces over $F$, and let $T: V \to W$ be linear.
  If $V$ is finite-dimensional, then
  $\mathrm{nullity}(T) + \mathrm{rank}(T) = \mathrm{dim}(V)$.
\end{theorem}
\begin{proof}
  Let $S$ be a basis for $V$ and $Q$ a basis for $\mathcal{N}(T)$.
  By corollary to replacement theorem (\Cref{thm:replacement}), there is
  $R \subseteq S \setminus Q$ such that $Q \cup R$ is a basis for $V$.
  Since $|R| = |Q \cup R| - |Q| = \mathrm{dim}(V) - \mathrm{nullity}(T)$,
  the theorem holds if $|R| = \mathrm{dim}(\mathcal{R}(T))$.
  
  If there exist different $x, x' \in R$ with $T(x) = T(x')$, then we have
  $T(x - x') = T(x) - T(x') = 0_W$, and thus
  $x - x' \in \mathcal{N}(T) = \mathrm{span}(Q)$.
  It follows that $x \in \mathrm{span}(Q \cup \{x'\})$,
  contradiction to the fact that $S$ is linearly independent.
  Thus, $|R| = |T(R)|$. We claim that $T(R)$ is a basis for $\mathcal{R}(T)$.

  First we prove that $T(R)$ spans $\mathcal{R}(T)$.
  By \Cref{thm:linear-span} (b) and the fact that $T(Q) = \{0_V\}$, we have
  \begin{align*}
    \mathcal{R}(T) &= T(\mathrm{span}(Q \cup R)) \\
                   &= \mathrm{span}(T(Q \cup R)) \\
                   &= \mathrm{span}(T(Q)) + \mathrm{span}(T(R)) \\
                   &= \mathrm{span}(T(R)).
  \end{align*}

  Then we prove that $T(R)$ is linearly independent.
  Suppose that
  \begin{equation*}
    a_1T(x_1) + \cdots + a_nT(x_n) = 0_W
  \end{equation*}
  holds for some $a_1, \dots, a_n \in F$ and some different
  $x_1, \dots, x_n \in R$ with $n \geq 1$.
  Then by \Cref{thm:linear-span} we have $T(a_1x_1 + \cdots + a_nx_n) = 0_W$,
  and thus $a_1x_1 + \cdots a_nx_n \in \mathcal{N}(T)$.
  Hence, there exist some $b_1, \dots, b_m \in F$ and some different
  $y_1, \dots, y_m \in Q$ such that
  \begin{equation*}
    a_1x_1 + \cdots + a_nx_n = b_1y_1 + \cdots + b_my_m.
  \end{equation*}
  That is,
  \begin{equation*}
    a_1x_1 + \cdots + a_nx_n + (-b_1)y_1 + \cdots + (-b_m)y_m = 0_V.
  \end{equation*}
  Since $Q \cup R$ is linearly independent, we have
  $a_1 = \cdots = a_n = b_1 = \cdots = b_m = 0_F$, implying that $T(R)$ is
  linearly independent.

  Thus, $T(R)$ is a basis for $\mathcal{R}(T)$, and we can conclude that
  $\mathrm{rank}(T) = |T(R)| = |R| = |Q \cup R| - |Q|$,
  which completes the proof.
\end{proof}

\section{Invertibility and Isomorphisms}
\begin{definition}
  Let $X$ and $Y$ be sets and let $f: X \to Y$ be a function.
  \begin{itemize}
    \item A function $g: Y \to X$ is a \emph{left inverse} of $f$ if
      $g \circ f = I_X$.
      We say that $f$ is \emph{left invertible} if it has a left inverse.
    \item A function $g: Y \to X$ is a \emph{right inverse} of $f$ if
      $f \circ g = I_Y$.
      We say that $f$ is \emph{right invertible} if it has a right inverse.
    \item A function $g: R \to S$ is an \emph{inverse} of $f$ if it is a left
      inverse and a right inverse of $f$.
      We say that $f$ is \emph{invertible} if it has an inverse.
  \end{itemize}
\end{definition}

\begin{proposition}\label{prop:invertible}
  The following statements are true.
  \begin{enumerate}
    \item A function is left invertible if and only if it is injective.
    \item A function is right invertible if and only if it is surjective.
    \item A function is invertible if and only if it is bijective.
  \end{enumerate}
\end{proposition}
\begin{proof} \leavevmode
  \begin{enumerate}
    \item $(\Rightarrow)$
      Suppose that $f: X \to Y$ is left invertible.
      Let $g: Y \to X$ be an left inverse of $f$.
      Then for each $x, x' \in X$ that satisfy $f(x) = f(x')$, we have
      $x = g(f(x)) = g(f(x')) = x'$.

      $(\Leftarrow)$
      Suppose that $f: X \to Y$ is injective.
      Then there exists a function $g: Y \to X$ such that $g(f(x)) = x$ holds
      for all $x \in X$, implying $g$ is a left inverse of $f$.
    \item $(\Rightarrow)$
      Suppose that $f: X \to Y$ is right invertible.
      Let $g: Y \to X$ be an right inverse of $f$.
      Then $y = f(g(y))$ for all $y \in Y$, and thus $f$ is surjective.

      $(\Leftarrow)$
      Suppose that $f: X \to Y$ is surjective.
      Then there exists a function $g: Y \to X$ such that $f(g(y)) = y$ for
      all $y \in Y$, implying $g$ is a right inverse of $g$.
    \item Straightforward from (a) and (b). \qedhere
  \end{enumerate}
\end{proof}

\begin{definition}
  Let $V$ and $W$ be vector spaces over $F$.
  \begin{itemize}
    \item A linear transformation $T: V \to W$ is called an \emph{isomorphism}
      from $V$ onto $W$ if it is invertible.
    \item We say that $V$ is \emph{isomorphic} to $W$, denoted by $V \cong W$,
      if there is an isomorphism from $V$ onto $W$.
  \end{itemize}
\end{definition}

\begin{proposition}
  Let $V$ and $W$ be vector spaces over $F$. Then $V \cong W$ if and only if
  $W \cong V$.
\end{proposition}
\begin{proof}
  If $V \cong W$, then there exists $T \in \mathcal{L}(V, W)$ that is
  invertible.
  Because $T^{-1}$ is linear and invertible, it is an isomorphism from $W$ onto
  $V$, and thus $W \cong V$.
  The other side can be proved similarly.
\end{proof}

\begin{theorem}
  Let $V$ and $W$ be finite-dimensional vector spaces over $F$.
  Then $V \cong W$ if and only if $\dim(V) = \dim(W)$.
\end{theorem}
\begin{proof}
  $(\Rightarrow)$
  Let $T$ be an isomorphism from $V$ onto $W$.
  Since $T$ is invertible, we have $\nullity(T) = 0$.
  Thus, by rank-nullity theorem (\Cref{thm:rank-nullity}) we have
  $\rank(T) = \dim(V)$.
  Furthermore, we have $\R(T) = W$ since $T$ is bijective by
  \Cref{prop:invertible}.
  Therefore, $\dim(V) = \dim(W)$.

  $(\Leftarrow)$
  To be completed.
\end{proof}