\chapter{Linear Transformations}
\section{Linear Transformations}
\begin{definition}
  \label{def:linear-transformation}
  Let $V$ and $W$ be vector spaces over a field $F$.
  A transformation $T: V \to W$ is said to be \emph{linear} if
  \begin{equation*}
    T(ax + y) = aT(x) + T(y)
  \end{equation*}
  holds for any scalar $a \in F$ and any vectors $x, y \in V$.
  The set of all linear transformations from $V$ to $W$ is denoted by
  $\mathcal{L}(V, W)$, and $\mathcal{L}(V)$ for short if $V = W$.
\end{definition}

\begin{proposition}
  \label{prop:linear-transformation}
  Let $V$ and $W$ be vector spaces over a common field $F$.
  Let ${T: V \to W}$ be linear.
  Then we have the following properties.
  \begin{enumerate}
    \item $T(0_V) = 0_W$.
    \item For nonnegative integer $n$,
    \begin{equation*}
      T\left(\sum_{i=1}^n a_ix_i\right) = \sum_{i=1}^n a_iT(x_i)
    \end{equation*}
    hold for any $a_1, \dots, a_n \in F$ and $x_1, \dots, x_n \in V$.
  \end{enumerate}
\end{proposition}
\begin{proof}
  \leavevmode
  \begin{enumerate}
    \item Since
    \begin{equation*}
      T(0_V) + T(0_V)
      = 1_FT(0_V) + T(0_V)
      = T(1_F0_V + 0_V)
      = T(0_V),
    \end{equation*}
    we have $T(0_V) = 0_W$ by \Cref{prop:vector-space-addition} (b).
    
    \item The proof is by induction on $n$.
    The induction basis with $n = 0$ is proved by
    \begin{equation*}
      T\left(\sum_{i=1}^0 a_ix_i\right)
      = T(0_V)
      = 0_W
      = \sum_{i=1}^0 a_iT(x_i).
    \end{equation*}
    Now assume the induction hypothesis that the property holds for $n = k$.
    Then it follows that
    \begin{align*}
      T\left(\sum_{i=1}^{k+1}a_ix_i\right)
      &= T\left(a_{k+1}x_{k+1} + \sum_{i=1}^k a_ix_i\right) \\
      &= a_{k+1}T(x_{k+1}) + T\left(\sum_{i=1}^k a_ix_i\right)
         \tag{linearity of $T$} \\
      &= a_{k+1}T(x_{k+1}) + \sum_{i=1}^k a_iT(x_i)
         \tag{induction hypothesis} \\
      &= \sum_{i=1}^{k+1} a_iT(x_i),
    \end{align*}
    which completes the proof.
    \qedhere
  \end{enumerate}
\end{proof}

\begin{theorem}
  \label{thm:linear-transformation-space}
  If $V$ and $W$ are vector spaces over a field $F$, then $\mathcal{L}(V, W)$
  is also a vector space over $F$.
\end{theorem}
\begin{proof}
  For any $c \in F$ and $T_1, T_2 \in \mathcal{L}(V, W)$, since
  \begin{align*}
    (cT_1 + T_2)(ax + y)
    &= cT_1(ax + y) + T_2(ax + y) \tag{linearity of $cT_1 + T_2$} \\
    &= c(aT_1(x) + T_1(y)) + (aT_2(x) + T_2(y)) \tag{linearity of $T_1$ and $T_2$} \\
    &= acT_1(x) + cT_1(y) + aT_2(x) + T_2(y) \\
    &= a(cT_1(x) + T_2(x)) + (cT_1(y) + T_2(y)) \\
    &= a(cT_1 + T_2)(x) + (cT_1 + T_2)(y) \tag{linearity of $cT_1 + T_2$}
  \end{align*}
  holds for each $a \in F$ and $x, y \in V$,
  we have $cT_1 + T_2 \in \mathcal{L}(V, W)$.
  Furthermore, $0_{\mathcal{F}(V, W)} \in \mathcal{L}(V, W)$.
  Thus, $\mathcal{L}(V, W)$ is a subspace of $\mathcal{F}(V, W)$.
\end{proof}

\begin{theorem}
  \label{thm:linear-span}
  Let $V$ and $W$ be vector spaces and let $T: V \to W$ be linear.
  Then for any subset $S$ of $V$, we have
  \begin{equation*}
    T(\spn(S)) = \spn(T(S)).
  \end{equation*}
\end{theorem}
\begin{proof}
  If $y \in T(\spn(S))$, then there exist $a_i \in F$ and $x_i \in S$ for each
  $1 \leq i \leq n$ such that
  \begin{equation*}
    y
    = T\left(\sum_{i=1}^n a_ix_i\right)
    = \sum_{i=1}^n a_iT(x_i)
    \in \spn(T(S)).
  \end{equation*}
  Thus, $T(\spn(S)) \subseteq \spn(T(S))$.
  
  On the other hand, if $y \in \spn(T(S))$, then there exist $a_i \in F$ and
  $x_i \in S$ for each $1 \leq i \leq n$ such that
  \begin{equation*}
    y
    = \sum_{i=1}^n a_iT(x_i)
    = T\left(\sum_{i=1}^n a_ix_i\right)
    \in T(\spn(S)).
  \end{equation*}
  Thus, $\spn(T(S)) \subseteq T(\spn(S))$, which completes the proof.
\end{proof}

\section{Rank and Nullity}
\begin{definition}
  \label{def:range}
  Let $V$ and $W$ be vector spaces.
  The \emph{range} of a transformation $T: V \to W$, denoted by
  $\mathcal{R}(T)$, is defined by
  \begin{equation*}
    \mathcal{R}(T) = \{y \in W : \text{$y = T(x)$ for some $x \in V$}\}.
  \end{equation*}
\end{definition}

\begin{proposition}
  \label{prop:range}
  Let $V$ and $W$ be vector spaces over a field $F$.
  If $T: V \to W$ is linear, then $\mathcal{R}(T)$ is a subspace of $W$.
\end{proposition}
\begin{proof}
  For each $a \in F$ and $y, y' \in \mathcal{R}(T)$, there exist $x, x' \in V$
  such that $y = T(x)$ and $y' = T(x')$.
  Since
  \begin{equation*}
    ay + y' = aT(x) + T(x') = T(ax + x'),
  \end{equation*}
  we have $ay + y' \in \mathcal{R}(T)$.
  Furthermore, $0_W = T(0_V) \in \mathcal{R}(T)$.
  Thus, $\mathcal{R}(T)$ is a subspace of $W$.
\end{proof}

\begin{definition}
  \label{def:null-space}
  Let $V$ and $W$ be vector spaces.
  The \emph{null space} of a transformation $T: V \to W$, denoted by
  $\mathcal{N}(T)$, is defined by
  \begin{equation*}
    \mathcal{N}(T) = \{x \in V : T(x) = 0_W\}.
  \end{equation*}
\end{definition}

\begin{proposition}
  \label{prop:null-space}
  Let $V$ and $W$ be vector spaces over a field $F$.
  If $T: V \to W$ is linear, then $\mathcal{N}(T)$ is a subspace of $V$.
\end{proposition}
\begin{proof}
  For each $a \in F$ and $x, x' \in \mathcal{N}(T)$, we have
  \begin{equation*}
    T(ax + x') = aT(x) + T(x') = a0_W + 0_W = 0_W.
  \end{equation*}
  Thus, $ax + x' \in \mathcal{N}(T)$.
  Furthermore, $0_V \in \mathcal{N}(T)$ since $T(0_V) = 0_W$.
  Thus, $\mathcal{N}(T)$ is a subspace of $V$.
\end{proof}

\begin{definition}
  \label{def:types-of-function}
  Let $X$ and $Y$ be sets.
  Let $f: X \to Y$ be a function.
  \begin{itemize}
    \item $f$ is \emph{injective} if $T(x) = T(x')$ implies $x = x'$
    for all $x, x' \in X$.
    \item $f$ is \emph{surjective} if there exists $x \in X$ with $T(x) = y$
    for each $y \in Y$.
    \item $f$ is \emph{bijective} if $f$ is injective and surjective.
  \end{itemize}
\end{definition}

\begin{proposition}
  \label{thm:linear-injection}
  Let $V$ and $W$ be vector spaces and let ${T: V \to W}$ be linear.
  Let $S$ be a subset of $V$.
  Then the following statements are true.
  \begin{enumerate}
    \item $T$ is injective if and only if $\mathcal{N}(T) = \{0_V\}$.
    \item If $T$ is injective, then $S$ is linearly dependent if and only of
    $T(S)$ is linearly dependent.
  \end{enumerate}
\end{proposition}
\begin{proof}
  \leavevmode
  \begin{enumerate}
    \item
    ($\Rightarrow$)
    We have $T(0_V) = 0_W$ since $T$ is linear.
    If $T(x) = 0_W$, then $x = 0_V$ since $T$ is injective.
    Thus, $\mathcal{N}(T) = \{0_V\}$.

    ($\Leftarrow$)
    Suppose that $x, y \in V$ be vectors with $T(x) = T(y)$.
    Since
    \begin{equation*}
      T(x - y) = T(x) - T(y) = 0_W,
    \end{equation*}
    we have $x - y \in \mathcal{N}(T)$, and thus $x - y = 0_V$,
    implying $x = y$.
    Thus, $T$ is injective.

    \item
    ($\Rightarrow$)
    If $x \in \spn(S \setminus \{x\})$ for some $x \in S$, then
    \begin{align*}
      T(x)
      &\; \in \; T(\spn(S \setminus \{x\})) \\
      &\; = \; \spn(T(S \setminus \{x\})) \tag{$T$ is linear} \\
      &\; = \; \spn(T(S) \setminus \{T(x)\}). \tag{$T$ is injective}
    \end{align*}

    ($\Leftarrow$)
    If $T(x) \in \spn(T(S) \setminus \{T(x)\})$ for some $x \in S$, then
    \begin{align*}
      T(x)
      &\; \in \; \spn(T(S) \setminus \{T(x)\}) \\
      &\; = \; \spn(T(S \setminus \{x\})) \tag{$T$ is injective} \\
      &\; = \; T(\spn(S \setminus \{x\})). \tag{$T$ is linear}
    \end{align*}
    Thus, $x \in \spn(S \setminus \{x\})$ since $T$ is injective.
    \qedhere
  \end{enumerate}
\end{proof}

\begin{definition}
  \label{def:rank-nullity}
  Let $V$ and $W$ be vector spaces. Let $T: V \to W$ be linear.
  \begin{itemize}
    \item The \emph{rank} of $T$, denoted by $\rank(T)$,
      is the dimension of $\mathcal{R}(T)$.
    \item The \emph{nullity} of $T$, denoted by $\nullity(T)$,
      is the dimension of $\mathcal{N}(T)$.
  \end{itemize}
\end{definition}

\begin{definition}
  \label{def:restriction}
  Let $f: X \to Y$ be a function.
  Let $D$ be a subset of $X$.
  Then the \emph{restriction} of $f$ to $D$ is the function $f': D \to Y$
  with $f'(x) = f(x)$ for each $x \in D$.
\end{definition}

\begin{proposition}
  \label{thm:restriction-linear}
  Let $V$ and $W$ be vector spaces and let $T: V \to W$ be linear.
  Let $U$ be a subspace of $V$.
  Then the restriction of $T$ to $U$ is linear.
\end{proposition}
\begin{proof}
  Let $T': U \to W$ be the restriction of $T$ to $U$.
  Then $T'$ is linear since for each $a \in F$ and $x, y \in U$, we have
  \begin{equation*}
    T'(ax + y)
    = T(ax + y)
    = aT(x) + T(y)
    = aT'(x) + T'(y).
    \qedhere
  \end{equation*}
\end{proof}

\begin{theorem}[Rank-nullity Theorem]
  \label{thm:rank-nullity}
  Let $V$ and $W$ be finite-dimensional vector spaces over $F$.
  Let $T: V \to W$ be linear.
  Then we have
  \begin{equation*}
    \nullity(T) + \rank(T) = \dim(V).
  \end{equation*}
\end{theorem}
\begin{proof}
  Let $S$ be a basis for $V$ and $Q$ a basis for $\mathcal{N}(T)$.
  By replacement theorem (\Cref{thm:replacement}), there is
  $R \subseteq S \setminus Q$ such that $Q \cup R$ is a basis for $V$.
  
  We prove that $T(R)$ is a basis for $\mathcal{R}(T)$.
  First,
  \begin{align*}
    \mathcal{R}(T)
    &= T(\spn(Q \cup R)) \\
    &= \spn(T(Q \cup R)) \\
    &= \spn(T(Q) \cup T(R)) \\
    &= \spn(T(R)). \tag{$T(Q) = \{0_V\}$}
  \end{align*}

  Now we prove that $T(R)$ is linearly independent.
  Let $T'$ be the restriction of $T$ to $R$.
  Since $R$ is linearly independent, it suffices to prove that $T'$ is
  injective.
  Suppose that $T(x) = T(x')$ for some $x, x' \in R$.
  Then we have $T(x - x') = T(x) - T(x') = 0_W$, and thus
  $x - x' \in \mathcal{N}(T) = \spn(Q)$.
  It follows that $x$ is a linear combination of $Q \cup \{x'\}$.
  If $x \neq x'$, then
  \begin{equation*}
    x \in \spn(Q \cup \{x'\}) \subseteq \spn(Q \cup R \setminus \{x\}),
  \end{equation*}
  contradiction to the fact that $Q \cup R$ is linearly independent.
  Thus, $T'$ is injective, implying $T(R)$ is linearly independent.

  Note that $|T(R)| = |R|$ since $T'$ is injective.
  Thus,
  \begin{equation*}
    \nullity(T) + \rank(T)
    = |Q| + |T(R)|
    = |Q| + |R|
    = \dim(V).
    \qedhere
  \end{equation*}
\end{proof}

\section{Isomorphisms}
\begin{definition}
  \label{def:composition}
  Let $X, Y, Z$ be sets.
  Let $f: X \to Y$ and $g: Y \to Z$ be functions.
  Then the \emph{composition} of $f$ and $g$ is the function $gf: X \to Z$ such
  that
  \begin{equation*}
    (gf)(x) = g(f(x))
  \end{equation*}
  for all $x \in X$.
\end{definition}

\begin{definition}
  \label{def:identity-function}
  The \emph{identity function} over a set $X$ is a function $I_X: X \to X$
  with $I_X(x) = x$ for all $x \in X$.
\end{definition}

\begin{definition}
  \label{def:invertibility}
  Let $X$ and $Y$ be sets.
  A function $f: X \to Y$ is said to be \emph{invertible} if there exists a
  function $f^{-1}: Y \to X$, called the \emph{inverse} of $f$, such that
  \begin{equation*}
    f^{-1}f = I_X
    \quad \text{and} \quad
    ff^{-1} = I_Y.
  \end{equation*}
\end{definition}

\begin{proposition}
  \label{prop:inverse}
  Let $X$ and $Y$ be sets.
  Let $f: X \to Y$ and $g: Y \to X$ be functions.
  \begin{enumerate}
    \item If $f$ is invertible, then $f^{-1}$ is invertible.
    \item If $f$ is invertible, then $f^{-1}$ is linear.
    \item If $f$ is invertible, then either $gf = I_X$ or $fg = I_Y$ implies
    $g = f^{-1}$.
    \item $f$ is invertible if and only if $f$ is bijective.
  \end{enumerate}
\end{proposition}
\begin{proof}
  \leavevmode
  \begin{enumerate}
    \item Straightforward from \Cref{def:invertibility}.
    
    \item For $a \in F$ and $y, y' \in Y$, we have
    \begin{align*}
      f^{-1}(ay + y')
      &= f^{-1}(af(f^{-1}(y)) + f(f^{-1}(y'))) \tag{$ff^{-1} = I_Y$} \\
      &= f^{-1}(f(af^{-1}(y) + f^{-1}(y'))) \tag{linearity of $f$} \\
      &= af^{-1}(y) + f^{-1}(y'). \tag{$f^{-1}f = I_X$}
    \end{align*}
    Thus, $f^{-1}$ is linear.
    
    \item
    If $gf = I_X$, then
    \begin{equation*}
      g = gI_Y = g(ff^{-1}) = (gf)f^{-1} = I_Xf^{-1} = f^{-1}.
    \end{equation*}
    If $fg = I_Y$, then
    \begin{equation*}
      g = I_Xg = (f^{-1}f)g = f^{-1}(fg) = f^{-1}I_Y = f^{-1}.
    \end{equation*}
    
    \item
    ($\Rightarrow$)
    Suppose that $f$ is invertible.
    Then $f$ is injective since for each $x, x' \in X$ with $f(x) = f(x')$,
    we have
    \begin{equation*}
      x = (f^{-1}f)(x) = f^{-1}(f(x)) = f^{-1}(f(x')) = (f^{-1}f)(x') = x'.
    \end{equation*}
    Also, $f$ is surjective since for each $y \in Y$, we have
    \begin{equation*}
      y = (ff^{-1})y = f(f^{-1}(y)).
    \end{equation*}

    ($\Leftarrow$)
    If $f$ is bijective, then for each $y \in Y$ there exists a unique element
    $x \in X$ with $f(x) = y$.
    Thus, there exists a function $g: Y \to X$ such that
    \begin{equation*}
      g(f(x)) = x
    \end{equation*}
    for each $x \in X$.
    For any $y \in Y$, if $x \in X$ is the element such that $f(x) = y$,
    then we have
    \begin{equation*}
      f(g(y)) = f(g(f(x))) = f(x) = y.
    \end{equation*}
    Thus, $f$ is invertible since $gf = I_X$ and $fg = I_Y$. \qedhere
  \end{enumerate}
\end{proof}

\begin{definition}
  \label{def:isomorphism}
  Let $V$ and $W$ be vector spaces.
  An \emph{isomorphism} from $V$ onto $W$ is a invertible linear transformation
  from $V$ to $W$.
  If there is an isomorphism from $V$ onto $W$, then $V$ and $W$ are said to be
  \emph{isomorphic}, denoted by $V \cong W$.
\end{definition}

\begin{lemma}
  \label{lem:same-dimension}
  Let $V$ and $W$ be finite-dimensional vector spaces with $\dim(V) = \dim(W)$.
  Let $T: V \to W$ be linear.
  Then $T$ is injective if and only if $T$ is surjective.
\end{lemma}
\begin{proof}
  ($\Rightarrow$)
  If $T$ is injective, then $\mathcal{N}(T) = \{0_V\}$, implying
  $\nullity(T) = 0$.
  Then we have
  \begin{equation*}
    \dim(\mathcal{R}(T))
    = \rank(T)
    = \dim(V) - \nullity(T)
    = \dim(W) - 0
    = \dim(W).
  \end{equation*}
  Since $\mathcal{R}(T)$ is a subspace of $W$ with
  $\dim(\mathcal{R}(T)) = \dim(W)$, we can conclude that $\mathcal{R}(T) = W$
  by \Cref{prop:subspace-dimension}.

  ($\Leftarrow$)
  If $T$ is surjective, then $\mathcal{R}(T) = W$.
  Thus,
  \begin{equation*}
    \nullity(T)
    = \dim(V) - \rank(T)
    = \dim(W) - \dim(W)
    = 0,
  \end{equation*}
  implying $\mathcal{N}(T) = \{0_V\}$.
  It follows that $T$ is injective.
\end{proof}

\begin{lemma}
  \label{lem:linear-uniqueness}
  Let $V$ and $W$ be finite-dimensional vector spaces over a field $F$.
  Let $S = \{x_1, x_2, \dots, x_n\}$ be a basis for $V$ and let
  $y_1, y_2, \dots, y_n$ be vectors in $W$.
  Then there exists a unique $T \in \mathcal{L}(V, W)$ with $T(x_i) = y_i$
  for each $i \in \{1, \dots, n\}$.
\end{lemma}
\begin{proof}
  Let $T$ be the transformation that satisfies
  \begin{equation*}
    T(a_1x_1 + a_2x_2 + \cdots + a_nx_n) = a_1y_1 + a_2y_2 + \cdots + a_ny_n
  \end{equation*}
  for any $a_1, a_2, \dots, a_n \in F$.
  It is obvious that $T(x_i) = y_i$ for each $i \in \{1, \dots, n\}$,
  and $T$ is linear since
  \begin{align*}
    T\left(c\sum_{i=1}^na_ix_i + \sum_{i=1}^nb_ix_i\right)
    &= T\left(\sum_{i=1}^n(ca_i + b_i)x_i\right) \\
    &= \sum_{i=1}^n(ca_i + b_i)y_i \\
    &= c\sum_{i=1}^na_iy_i + \sum_{i=1}^nb_iy_i \\
    &= cT\left(\sum_{i=1}^na_ix_i\right) + T\left(\sum_{i=1}^nb_ix_i\right)
  \end{align*}
  holds for any scalars $a_1, a_2, \dots, a_n, b_1, b_2, \dots, b_n, c \in F$.
  To see the uniqueness, if $T' \in \mathcal{L}(V, W)$ satisfies
  $T'(x_i) = y_i$ for each $i \in \{1, \dots, n\}$, then we have
  \begin{align*}
    T'(a_1x_1 + \cdots + a_nx_n)
    &= a_1T'(x_1) + \cdots + a_nT'(x_n) \\
    &= a_1T(x_1) + \cdots + a_nT(x_n) \\
    &= T(a_1x_1 + \cdots + a_nx_n). \\
  \end{align*}
  for any $a_1, \dots, a_n \in F$.
  Thus, $T' = T$.
\end{proof}

\begin{theorem}
  \label{thm:isomorphism}
  Let $V$ and $W$ be finite-dimensional vector spaces over a field $F$.
  Then $V \cong W$ if and only if $\dim(V) = \dim(W)$.
\end{theorem}
\begin{proof}
  $(\Rightarrow)$
  Let $T$ be an isomorphism from $V$ onto $W$.
  Since $T$ is invertible, $T$ is bijective.
  Then we have $\rank(T) = \dim(W)$ since $\mathcal{R}(T) = W$.
  Furthermore, since $T$ is injective, we have $\nullity(T) = 0$,
  and it follows that $\rank(T) = \dim(V)$ by rank-nullity theorem
  (\Cref{thm:rank-nullity}).
  Thus, $\dim(V) = \rank(T) = \dim(W)$.

  $(\Leftarrow)$
  Suppose that $S = \{x_1, x_2, \dots, x_n\}$ is a basis for $V$ and
  $R = \{y_1, y_2, \dots, y_n\}$ is a basis for $W$.
  Then by \Cref{lem:linear-uniqueness} there exists $T \in \mathcal{L}(V, W)$
  such that $T(x_i) = y_i$ for each $i \in \{1, \dots, n\}$.
  Since $R$ is a basis for $W$, for each $y \in W$ there exist
  scalars $a_1, \dots, a_n \in F$ such that
  \begin{equation*}
    y
    = \sum_{i=1}^n a_iy_i
    = \sum_{i=1}^n a_iT(x_i)
    = T\left(\sum_{i=1}^n a_ix_i\right).
  \end{equation*}
  It follows that $T$ is surjective, and we can conclude that $T$ is bijective
  by \Cref{lem:same-dimension}.
  Thus, $T$ is an isomorphism from $V$ onto $W$, implying $V \cong W$.
\end{proof}

\section{Coordinates and Matrix Representations}
\begin{definition}
  \label{def:ordered-basis}
  Let $V$ be an finite-dimensional vector space over a field $F$ with
  $\dim(V) = n$.
  An \emph{ordered basis} for $V$ is a finite sequence
  \begin{equation*}
    \beta = (x_1, x_2, \dots, x_n)
  \end{equation*}
  of vectors in $V$ such that the set $S = \{x_1, x_2, \dots, x_n\}$ is a basis
  for $V$.
\end{definition}

\begin{examples}
  \leavevmode
  \begin{itemize}
    \item The \emph{standard ordered basis} for $F^n$ is $(e_1, \dots, e_n)$,
    where $e_i$ is the $n$-tuple whose $i$-th component is $1_F$ and the other
    components are all $0_F$.
    \item The \emph{standard ordered basis} for $\mathcal{P}_n(F)$ is
    $(t^0, t^1, \dots, t^n)$.
  \end{itemize}
\end{examples}

\begin{definition}
  \label{def:coordinate}
  Let $V$ be a finite-dimensional vector space over a field $F$.
  Let $\beta = (x_1, \dots, x_n)$ be an ordered basis for $V$.
  Then we define $\phi_\beta: V \to F^n$ such that
  \begin{equation*}
    \phi_\beta(x)
    = \begin{pmatrix} a_1 \\ a_2 \\ \vdots \\ a_n \end{pmatrix}
    \quad \text{for each}\ x \in V\ \text{with}\ x = \sum_{i=1}^n a_ix_i,
  \end{equation*}
  where $a_1, a_2, \dots, a_n \in F$.
  For each vector $x$ in $V$, $\phi_\beta(x)$ is called the \emph{coordinate}
  of $x$ with respect to $\beta$, denoted by $[x]_\beta$.
\end{definition}

\begin{proposition}
  \label{prop:coordinate}
  Let $\beta = (x_1, \dots, x_n)$ be an ordered basis for a vector space $V$
  over $F$.
  Then $\phi_\beta$ is an isomorphism from $V$ onto $F^n$.
\end{proposition}
\begin{proof}
  $\phi_\beta$ is linear since
  \begin{align*}
    \phi_\beta\left(c\sum_{i=1}^na_ix_i + \sum_{i=1}^nb_ix_i\right)
    &= \phi_\beta\left(\sum_{i=1}^n(ca_i + b_i)x_i\right)
     = \begin{pmatrix} ca_1 + b_1 \\ \vdots \\ ca_n + b_n \end{pmatrix}
     = c\begin{pmatrix} a_1 \\ \vdots \\ a_n \end{pmatrix}
       + \begin{pmatrix} b_1 \\ \vdots \\ b_n \end{pmatrix} \\
    &= c \cdot \phi_\beta\left(\sum_{i=1}^na_ix_i\right)
       + \phi_\beta\left(\sum_{i=1}^nb_ix_i\right)
  \end{align*}
  holds for any $a_1, \dots, a_n, b_1, \dots, b_n, c \in F$.
  Also, $\phi_\beta$ is invertible since there exists
  $\phi_\beta^{-1}: F^n \to V$ with
  \begin{equation*}
    \phi_\beta^{-1} \left(
      \begin{pmatrix} a_1 \\ a_2 \\ \vdots \\ a_n \end{pmatrix}
    \right)
    = a_1x_1 + a_2x_2 + \cdots + a_nx_n
  \end{equation*}
  for any $a_1, a_2, \dots, a_n \in F$.
  Thus, $\phi_\beta$ is an isomorphism.
\end{proof}

\begin{definition}
  \label{def:matrix-representation}
  Let $V$ and $W$ be finite-dimensional vector spaces over a field $F$.
  Let
  \begin{equation*}
    \beta = (x_1, \dots, x_n)
    \quad \text{and} \quad
    \gamma = (y_1, \dots, y_m)
  \end{equation*}
  be ordered basis for $V$ and $W$, respectively.
  Then we define $\Phi_\beta^\gamma: \mathcal{L}(V, W) \to F^{m \times n}$
  by
  \begin{equation*}
    \Phi_\beta^\gamma(T) =
    \begin{pmatrix}
      a_{11} & a_{12} & \cdots & a_{1n} \\
      a_{21} & a_{22} & \cdots & a_{2n} \\
      \vdots & \vdots & \ddots & \vdots \\
      a_{m1} & a_{m2} & \cdots & a_{mn}
    \end{pmatrix},
  \end{equation*}
  for each $T \in \mathcal{L}(V, W)$, where
  \begin{equation*}
    \begin{gathered}
      T(x_1) = a_{11}y_1 + a_{21}y_2 + \cdots + a_{m1}y_m \\
      T(x_2) = a_{12}y_1 + a_{22}y_2 + \cdots + a_{m2}y_m \\
      \vdots \\
      T(x_n) = a_{1n}y_1 + a_{2n}y_2 + \cdots + a_{mn}y_m
    \end{gathered}
  \end{equation*}
  hold.
  For each linear $T: V \to W$, the matrix $\Phi_\beta^\gamma(T)$ is called the
  \emph{matrix representation} of $T$ with respect to $\beta$ and $\gamma$,
  denoted by $[T]_\beta^\gamma$.
\end{definition}

\begin{proposition}
  \label{prop:matrix-representation}
  Let $\beta = (x_1, \dots, x_n)$ and $\gamma = (y_1, \dots, y_m)$ be ordered
  bases for a vector spaces $V$ and $W$ over $F$, respectively.
  Then for any $T \in \mathcal{L}(V, W)$, we have
  \begin{equation*}
    \Bigl([T]_\beta^\gamma\Bigr)_{ij} = \Bigl([T(x_j)]_\gamma\Bigr)_i
  \end{equation*}
  for any $i \in \{1, \dots, m\}$ and $j \in \{1, \dots, n\}$.
\end{proposition}
\begin{proof}
  Let
  \begin{equation*}
    [T]_\beta^\gamma =
    \begin{pmatrix}
      a_{11} & a_{12} & \cdots & a_{1n} \\
      a_{21} & a_{22} & \cdots & a_{2n} \\
      \vdots & \vdots & \ddots & \vdots \\
      a_{m1} & a_{m2} & \cdots & a_{mn}
    \end{pmatrix}.
  \end{equation*}
  Since $T(x_j) = a_{1j}y_1 + a_{2j}y_2 + \cdots + a_{mj}y_m$, we have
  \begin{equation*}
    [T(x_j)]_\gamma
    = \begin{pmatrix} a_{1j} \\ a_{2j} \\ \vdots \\ a_{mj} \end{pmatrix}.
  \end{equation*}
  Thus,
  \begin{equation*}
    \Bigl([T(x_j)]_\gamma\Bigr)_i = a_{ij}
  \end{equation*}
  holds, which completes the proof.
\end{proof}

\begin{theorem}
  \label{thm:matrix-representation}
  Let $\beta$ and $\gamma$ be ordered bases for a vector spaces $V$ and $W$
  over $F$, respectively.
  Then $\Phi_\beta^\gamma$ is an isomorphism from $\mathcal{L}(V, W)$ onto
  $F^{m \times n}$.
\end{theorem}
\begin{proof}
  Let $\beta = (x_1, \dots, x_n)$ and $\gamma = (y_1, \dots, y_m)$.
  Note that $\Phi_\beta^\gamma$ is linear since for any $c \in F$ and
  $T_1, T_2 \in \mathcal{L}(V, W)$, we have
  \begin{align*}
    \Bigl([cT_1 + T_2]_\beta^\gamma\Bigr)_{ij}
    &= \Bigl([(cT_1 + T_2)(x_j)]_\gamma\Bigr)_i
       \tag{\Cref{prop:matrix-representation}} \\
    &= \Bigl([cT_1(x_j) + T_2(x_j)]_\gamma\Bigr)_i \\
    &= \Bigl(c[T_1(x_j)]_\gamma + [T_2(x_j)]_\gamma\Bigr)_i
       \tag{$\phi_\gamma$ is linear} \\
    &= c\Bigl([T_1(x_j)]_\gamma\Bigr)_i + \Bigl([T_2(x_j)]_\gamma\Bigr)_i \\
    &= c\Bigl([T_1]_\beta^\gamma\Bigr)_{ij}
       + \Bigl([T_2]_\beta^\gamma\Bigr)_{ij}
       \tag{\Cref{prop:matrix-representation}} \\
    &= \Bigl(c[T_1]_\beta^\gamma + [T_2]_\beta^\gamma\Bigr)_{ij}
  \end{align*}
  for any $i \in \{1, \dots, m\}$ and $j \in \{1, \dots, n\}$.
  To prove that $\Phi_\beta^\gamma$ is invertible, let
  \begin{equation*}
    A =
    \begin{pmatrix}
      a_{11} & a_{12} & \cdots & a_{1n} \\
      a_{21} & a_{22} & \cdots & a_{2n} \\
      \vdots & \vdots & \ddots & \vdots \\
      a_{m1} & a_{m2} & \cdots & a_{mn}
    \end{pmatrix}
  \end{equation*}
  be an arbitrary matrix in $F^{m \times n}$.
  By \Cref{lem:linear-uniqueness}, there exists a unique linear transformation
  $T: V \to W$ such that
  \begin{equation*}
    T(x_j) = \sum_{i=1}^n a_{ij}y_j
  \end{equation*}
  for each $j \in \{1, \dots, n\}$, and it follows that $[T]_\beta^\gamma = A$.
  Thus, there exists
  $(\Phi_\beta^\gamma)^{-1}: F^{m \times n} \to \mathcal{L}(V, W)$
  such that $(\Phi_\beta^\gamma)^{-1}(A) = T$ with $[T]_\beta^\gamma = A$ for
  each $A \in F^{m \times n}$, which completes the proof.
\end{proof}

\begin{corollary}
  \label{cor:matrix-representation}
  If $V$ and $W$ are finite-dimensional vector spaces over $F$ with
  $\dim(V) = n$ and $\dim(W) = m$, then $\mathcal{L}(V, W)$ is
  finite-dimensional with $\dim(\mathcal{L}(V, W)) = mn$.
\end{corollary}
\begin{proof}
  Straightforward from \Cref{thm:isomorphism} and
  \Cref{thm:matrix-representation}.
\end{proof}

\section{Matrix Multiplication}
\begin{definition}
  Let $F$ be a field and let $A \in F^{\ell \times m}$ and
  $B \in F^{m \times n}$ be matrices.
  The \emph{product} of $A$ and $B$, denoted by $AB$, is a matrix in
  $F^{\ell \times n}$ that satisfies
  \begin{equation*}
    (AB)_{ik} = \sum_{j=1}^m A_{ij}B_{jk}
  \end{equation*}
  for $i \in \{1, \dots, \ell\}$ and $k \in \{1, \dots, n\}$.
\end{definition}

\begin{proposition}
  Let $U, V, W$ be vector spaces over $F$.
  If $T_1: {U \to V}$ and $T_2: {V \to W}$ are linear, then so is $T_2T_1$.
\end{proposition}
\begin{proof}
  For $a \in F$ and $x, y \in U$, we have
  \begin{align*}
    (T_2T_1)(ax + y)
    &= T_2(T_1(ax + y)) \tag{composition of $T_1$ and $T_2$} \\
    &= T_2(aT_1(x) + T_1(y)) \tag{linearity of $T_1$} \\
    &= aT_2(T_1(x)) + T_2(T_1(y)) \tag{linearity of $T_2$} \\
    &= a(T_2T_1)(x) + (T_2T_1)(y). \tag{composition of $T_1$ and $T_2$}
  \end{align*}
  Thus, $T_2T_1$ is linear.
\end{proof}

\begin{theorem}
  \label{thm:matrix-multiplication}
  Let $U, V, W$ be finite-dimensional vector spaces with ordered bases
  \begin{equation*}
    \alpha = (x_1, \dots, x_n),
    \quad
    \beta = (y_1, \dots, y_m),
    \quad \text{and} \quad
    \gamma = (z_1, \dots, z_\ell),
  \end{equation*}
  respectively.
  If $T_1: U \to V$ and $T_2: V \to W$ are linear, then
  \begin{equation*}
    [T_2T_1]_\alpha^\gamma = [T_2]_\beta^\gamma [T_1]_\alpha^\beta.
  \end{equation*}
\end{theorem}
\begin{proof}
  Let $A = [T_2]_\beta^\gamma$, $B = [T_1]_\alpha^\beta$ and
  $C = [T_2T_1]_\alpha^\gamma$.
  Then
  \begin{equation*}
    T_2(y_j) = \sum_{i=1}^\ell A_{ij}z_i,
    \quad
    T_1(x_k) = \sum_{j=1}^m B_{jk}y_j,
    \quad \text{and} \quad
    (T_2T_1)(x_k) = \sum_{i=1}^\ell C_{ik}z_i
  \end{equation*}
  hold for any $j \in \{1, \dots, m\}$ and $k \in \{1, \dots, n\}$.
  Since for each $k \in \{1, \dots, n\}$,
  \begin{align*}
    \sum_{i=1}^\ell C_{ik}z_i
    &= (T_2T_1)(x_k) \\
    &= T_2(T_1(x_k)) \\
    &= T_2\left(\sum_{j=1}^m B_{jk}y_j\right) \\
    &= \sum_{j=1}^m B_{jk} T_2(y_j) \\
    &= \sum_{j=1}^m B_{jk} \sum_{i=1}^\ell A_{ij}z_i \\
    &= \sum_{i=1}^\ell \left(\sum_{j=1}^m A_{ij}B_{jk}\right)z_i,
  \end{align*}
  we have
  \begin{equation*}
    C_{ik} = \sum_{j=1}^m A_{ij}B_{jk}
  \end{equation*}
  for each $i \in \{1, \dots, \ell\}$ and $k \in \{1, \dots, n\}$.
  Thus, $C = AB$.
\end{proof}

\begin{corollary}
  Let $V$ and $W$ be finite-dimensional vector spaces with ordered bases
  $\beta$ and $\gamma$ over a field $F$, respectively.
  If $T: V \to W$ is linear, then
  \begin{equation*}
    [T(x)]_\gamma = [T]_\beta^\gamma[x]_\beta
  \end{equation*}
  for each $x \in V$.
\end{corollary}
\begin{proof}
  Let $\alpha = (1_F)$ be an ordered basis for $F$.
  For each $x \in V$, let $\varphi: F \to V$ be the linear transformation with
  $\varphi(c) = cx$ for each $c \in F$.
  By \Cref{def:matrix-representation}, we have
  \begin{equation*}
    [\varphi]_\alpha^\beta = [\varphi(1_F)]_\beta
    \quad \text{and} \quad
    [T\varphi]_\alpha^\gamma = [(T\varphi)(1_F)]_\gamma.
  \end{equation*}
  Thus, it follows that
  \begin{align*}
    [T(x)]_\gamma
    &= [T(\varphi(1_F))]_\gamma \\
    &= [(T\varphi)(1_F)]_\gamma \\
    &= [T\varphi]_\alpha^\gamma \\
    &= [T]_\beta^\gamma[\varphi]_\alpha^\beta
       \tag{\Cref{thm:matrix-multiplication}} \\
    &= [T]_\beta^\gamma[\varphi(1_F)]_\beta \\
    &= [T]_\beta^\gamma[x]_\beta.
    \qedhere
  \end{align*}
\end{proof}

\section{Left-Multiplication Transformations}
\begin{definition}
  Let $A \in F^{m \times n}$ be a matrix.
  The \emph{left-multiplication transformation} of $A$, denoted by $L_A$,
  is the transformation from $F^n$ to $F^m$ with
  \begin{equation*}
    L_A(x) = Ax
  \end{equation*}
  for each $x \in F^n$.
\end{definition}

\begin{proposition}
  \label{prop:left-multiplication-transformation}
  Let $\alpha$, $\beta$ and $\gamma$ be standard ordered bases for $F^n$, $F^m$
  and $F^\ell$, respectively.
  Then the following statements are true.
  \begin{enumerate}
    \item $L_A$ is linear for each $A \in F^{m \times n}$.
    \item $[L_A]_\alpha^\beta = A$ for each $A \in F^{m \times n}$.
    \item $L_{cA+B} = cL_A + L_B$ for each $c \in F$ and
    $A, B \in F^{m \times n}$.
    \item $L_{AB} = L_AL_B$ for each $A \in F^{\ell \times m}$ and
    $B \in F^{m \times n}$.
    \item $L_{I_n} = I_{F^n}$.
  \end{enumerate}
\end{proposition}
\begin{proof}
  \leavevmode
  \begin{enumerate}
    \item $L_A$ is linear since for any $c \in F$ and $x, y \in F^n$,
    \begin{align*}
      \Bigl[L_A(cx + y)\Bigr]_i
      &= \Bigl[A(cx + y)\Bigr]_i \\
      &= \sum_{j=1}^n A_{ij}\bigl[cx + y\bigr]_j \\
      &= \sum_{j=1}^n A_{ij}(cx_j + y_j) \\
      &= c\sum_{j=1}^n A_{ij}x_j + \sum_{j=1}^n A_{ij}y_j \\
      &= c\bigl[Ax\bigr]_i + \bigl[Ay\bigr]_i \\
      &= \bigl[cAx + Ay\bigr]_i \\
      &= \Bigl[cL_A(x) + L_A(y)\Bigr]_i
    \end{align*}
    holds for each $i \in \{1, \dots, m\}$.
    
    \item Let $T \in \mathcal{L}(V, W)$ be the transformation with
    $[T]_\alpha^\beta = A$.
    Then we have
    \begin{equation*}
      T(x) = [T(x)]_\beta = [T]_\alpha^\beta[x]_\alpha = Ax
    \end{equation*}
    for each $x \in F^n$ since $\alpha$ and $\beta$ are standard ordered bases.
    Thus, $T = L_A$.

    \item It is proved by
    \begin{equation*}
      [L_{cA + B}]_\alpha^\beta
      = cA + B
      = c[L_A]_\alpha^\beta + [L_B]_\alpha^\beta
      = [cL_A + L_B]_\alpha^\beta.
    \end{equation*}

    \item It is proved by
    \begin{equation*}
      [L_{AB}]_\alpha^\gamma
      = AB
      = [L_A]_\beta^\gamma[L_B]_\alpha^\beta
      = [L_AL_B]_\alpha^\gamma.
    \end{equation*}
    
    \item Since
    \begin{equation*}
      L_{I_n}(x)
      = I_nx
      = x
      = I_{F^n}(x)
    \end{equation*}
    holds for each $x \in F^n$, $L_{I_n} = I_{F^n}$.
    \qedhere
  \end{enumerate}
\end{proof}

\begin{lemma}
  \label{lem:composition}
  Let $U, V, W, X$ be vector spaces.
  Let
  \begin{equation*}
    T_1, T_1' \in \mathcal{L}(U, V),
    \quad
    T_2, T_2' \in \mathcal{L}(V, W),
    \quad \text{and} \quad
    T_3 \in \mathcal{L}(W, X)
  \end{equation*}
  be linear transformations and let $c \in F$ be a scalar.
  Then the following statements are true.
  \begin{enumerate}
    \item $T_1I_U = T_1 = I_VT_1$.
    \item $T_2(T_1 + T_1') = T_2T_1 + T_2T_1'$.
    \item $(T_2 + T_2')T_1 = T_2T_1 + T_2'T_1$.
    \item $c(T_2T_1) = (cT_2)T_1 = T_2(cT_1)$.
    \item $T_3(T_2T_1) = (T_3T_2)T_1$.
  \end{enumerate}
\end{lemma}
\begin{proof}
  \leavevmode
  \begin{enumerate}
    \item Since
    \begin{equation*}
      (T_1I_U)(x) = T_1(I_U(x)) = T_1(x) = I_V(T_1(x)) = (I_VT_1)(x)
    \end{equation*}
    holds for each $x \in U$, we have $T_1I_U = T_1 = I_VT_1$.
    
    \item Since
    \begin{align*}
      (T_2(T_1 + T_1'))(x)
      &= T_2((T_1 + T_1')(x)) \tag{composition} \\
      &= T_2(T_1(x) + T_1'(x)) \tag{addition} \\
      &= T_2(T_1(x)) + T_2(T_1'(x)) \tag{linearity} \\
      &= (T_2T_1)(x) + (T_2T_1')(x) \tag{composition}\\
      &= (T_2T_1 + T_2T_1')(x) \tag{addition}
    \end{align*}
    holds for each $x \in U$, we have $T_2(T_1 + T_1') = T_2T_1 + T_2T_1'$.

    \item Since
    \begin{align*}
      ((T_2 + T_2')T_1)(x)
      &= (T_2 + T_2')(T_1(x)) \tag{composition} \\
      &= T_2(T_1(x)) + T_2'(T_1(x)) \tag{addition} \\
      &= (T_2T_1)(x) + (T_2'T_1)(x) \tag{composition} \\
      &= (T_2T_1 + T_2'T_1)(x) \tag{addition}
    \end{align*}
    holds for each $x \in U$, we have $(T_2 + T_2')T_1 = T_2T_1 + T_2'T_1$.

    \item Since
    \begin{align*}
      \begin{array}{ccccc}
        (c(T_2T_1))(x) &=& c(T_2T_1)(x) &=& cT_2(T_1(x)) \\[.5em]
        ((cT_2)T_1)(x) &=& (cT_2)(T_1(x)) &=& cT_2(T_1(x)) \\[.5em]
        (T_2(cT_1))(x) &=& T_2(cT_1(x)) &=& cT_2(T_1(x))
      \end{array}
    \end{align*}
    hold for each $x \in U$, we have $c(T_2T_1) = (cT_2)T_1 = T_2(cT_1)$.
    
    \item Since
    \begin{align*}
      (T_3(T_2T_1))(x)
      &= T_3((T_2T_1)(x)) \tag{composition of $T_3$ and $T_2T_1$} \\
      &= T_3(T_2(T_1(x))) \tag{composition of $T_2$ and $T_1$} \\
      &= (T_3T_2)(T_1(x)) \tag{composition of $T_3$ and $T_2$} \\
      &= ((T_3T_2)T_1)(x) \tag{composition of $T_3T_2$ and $T_1$}
    \end{align*}
    holds for each $x \in U$, we have $T_3(T_2T_1) = (T_3T_2)T_1$.
    \qedhere
  \end{enumerate}
\end{proof}

\begin{theorem}
  \label{thm:composition}
  Let $A, A' \in F^{k \times \ell}$, $B, B' \in F^{\ell \times m}$ and
  $C \in F^{m \times n}$ be matrices and let $c \in F$ be a scalar.
  Then the following statements are true.
  \begin{enumerate}
    \item $AI_\ell = A = I_kA$.
    \item $A(B + B') = AB + AB'$.
    \item $(A + A')B = AB + A'B$.
    \item $c(AB) = (cA)B = A(cB)$.
    \item $A(BC) = (AB)C$.
  \end{enumerate}
\end{theorem}
\begin{proof}
  Straightforward from \Cref{lem:composition}.
\end{proof}

\section{Invertible Matrices}
\begin{definition}
  \label{def:invertible-matrix}
  A matrix $A \in F^{n \times n}$ is \emph{invertible} if $L_A$ is invertible.
  If $A$ is invertible, then it has an \emph{inverse}, denoted by $A^{-1}$,
  which is the matrix in $F^{n \times n}$ such that
  \begin{equation*}
    L_{A^{-1}} = (L_A)^{-1}.
  \end{equation*}
\end{definition}

\begin{proposition}
  \label{prop:invertible-matrix}
  The following statements are true for matrices $A, B \in F^{n \times n}$.
  \begin{enumerate}
    \item If $A$ is invertible, then $AA^{-1} = I_n = A^{-1}A$.
    \item If $AB = I_n$, then $A$ and $B$ are invertible.
    Furthermore, $A = B^{-1}$ and $B = A^{-1}$.
  \end{enumerate}
\end{proposition}
\begin{proof}
  \leavevmode
  \begin{enumerate}
    \item We have
    \begin{equation*}
      L_{AA^{-1}} = L_AL_{A^{-1}} = L_A(L_A)^{-1} = I_{F^n} = L_{I_n}
    \end{equation*}
    and
    \begin{equation*}
      L_{A^{-1}A} = L_{A^{-1}}L_A = (L_A)^{-1}L_A = I_{F^n} = L_{I_n},
    \end{equation*}
    implying $AA^{-1} = I_n = A^{-1}A$.
    
    \item Since $AB$ is invertible, $L_{AB} = L_AL_B$ is injective and
    surjective.
    Thus, ${L_A: F^n \to F^n}$ is injective and ${L_B: F^n \to F^n}$ is
    surjective.
    It follows that $L_A$ and $L_B$ are bijective by \Cref{lem:same-dimension},
    and thus are invertible, implying $A$ and $B$ are invertible.
    By \Cref{prop:inverse} (c), we have $L_A = (L_B)^{-1}$ and
    $L_B = (L_A)^{-1}$.
    Thus, we have $A = B^{-1}$ and $B = A^{-1}$.
    \qedhere
  \end{enumerate}
\end{proof}