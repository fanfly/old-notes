\chapter{Diagonalization}
\section{Eigenvalues and Eigenvectors}
\begin{definition}
  Let $T: V \to V$ be a linear operator on a vector space $V$ over a field $F$.
  If
  \begin{equation*}
    T(x) = \lambda x
  \end{equation*}
  holds for some scalar $\lambda \in F$ and some vector
  $x \in V \setminus \{0_V\}$, then $(\lambda, x)$ is called an
  \emph{eigenpair} of $T$, with $\lambda$ and $x$ called an
  \emph{eigenvalue} and an \emph{eigenvector} of $T$, respectively.
\end{definition}

\begin{definition}
  Let $V$ be a finite-dimensional vector space over a field $F$.
  Let $T \in \mathcal{L}(V)$.
  An \emph{eigenbasis} of $V$ for $T$ is an ordered basis of $V$ in which every
  vector is an eigenvector of $T$.
\end{definition}

\begin{theorem}
  \label{thm:diagonalization}
  Let $V$ be a vector space over a field $F$ and let $T: V \to V$ be linear.
  Let $\beta = (x_1, x_2, \dots, x_n)$ be an ordered basis for $T$.
  Let $\lambda_1, \lambda_2, \dots, \lambda_n \in F$ be scalars.
  Then
  \begin{equation*}
    [T]_\beta^\beta =
    \begin{pmatrix}
      \lambda_1 & 0 & \cdots & 0 \\
      0 & \lambda_2 & \cdots & 0 \\
      \vdots & \vdots & \ddots & \vdots \\
      0 & 0 & \cdots & \lambda_n
    \end{pmatrix}
  \end{equation*}
  if and only if
  $T(x_i) = \lambda_ix_i$ for each $i \in \{1, 2, \dots, n\}$.
\end{theorem}
\begin{proof}
  ($\Rightarrow$)
  For each $i \in \{1, 2, \dots, n\}$, we have
  \begin{equation*}
    [T(x_i)]_\beta = \lambda_ie_i = [\lambda_ix_i]_\beta.
  \end{equation*}
  Thus, $T(x_i) = \lambda_ix_i$.
  ($\Leftarrow$)
  For each $i \in \{1, 2, \dots, n\}$, we have
  $[T(x_i)]_\beta = [\lambda_ix_i]_\beta = \lambda_ie_i$,
  and it follows that
  \begin{equation*}
    [T]_\beta^\beta =
    \begin{pmatrix}
      \lambda_1 & 0 & \cdots & 0 \\
      0 & \lambda_2 & \cdots & 0 \\
      \vdots & \vdots & \ddots & \vdots \\
      0 & 0 & \cdots & \lambda_n
    \end{pmatrix}.
    \qedhere
  \end{equation*}
\end{proof}

\begin{corollary}
  Let $V$ be a finite-dimensional vector space and let $T: V \to V$ be linear.
  Let $\beta$ be an ordered basis of $T$.
  Then $[T]_\beta^\beta$ is diagonal if and only if $\beta$ is an eigenbasis
  of $V$ for $T$.
\end{corollary}
\begin{proof}
  Straightforward from \Cref{thm:diagonalization}.
\end{proof}

\begin{definition}
  Let $A \in F^{n \times n}$.
  If
  \begin{equation*}
    Ax = \lambda x
  \end{equation*}
  holds for some scalar $\lambda \in F$ and some vector
  $x \in V \setminus \{0_V\}$, then $(\lambda, x)$ is called an
  \emph{eigenpair} of $A$, with $\lambda$ and $x$ called an
  \emph{eigenvalue} and an \emph{eigenvector} of $A$, respectively.
\end{definition}

\begin{theorem}
  Let $V$ be a finite-dimensional vector space with an ordered basis $\beta$.
  Let $\lambda \in F$ be a scalar and $x \in V$ be a vector.
  Then $(\lambda, x)$ is an eigenpair of $T$ if and only if
  $(\lambda, [x]_\beta)$ is an eigenpair of $[T]_\beta^\beta$.
\end{theorem}
\begin{proof}
  ($\Rightarrow$)
  Suppose that $T(x) = \lambda x$.
  Then we have
  \begin{equation*}
    [T]_\beta^\beta[x]_\beta
    = [T(x)]_\beta
    = [\lambda x]_\beta
    = \lambda [x]_\beta.
  \end{equation*}
  ($\Leftarrow$)
  Since
  \begin{equation*}
    [T(x)]_\beta
    = [T]_\beta^\beta[x]_\beta
    = \lambda[x]_\beta
    = [\lambda x]_\beta,
  \end{equation*}
  we can conclude that $T(x) = \lambda x$.
\end{proof}

\section{Characteristic Polynomials and Eigenspaces}
\begin{theorem}
  \label{thm:eigenvalue-condition}
  Let $A \in F^{n \times n}$ be a matrix and let $\lambda \in F$ be a scalar.
  Then $\lambda$ is an eigenvalue of $A$ if and only if
  $\det(A - \lambda I_n) = 0_F$.
\end{theorem}
\begin{proof}
  The proof is as follows.
  \begin{align*}
    \text{$\lambda$ is an eigenvalue of $A$}
    &\quad \Leftrightarrow \quad
    \text{$Ax = \lambda x$ for some $x \in F^n \setminus \{0_{F^n}\}$} \\
    &\quad \Leftrightarrow \quad
    \text{$(A - \lambda I_n)x$ for some $x \in F^n \setminus \{0_{F^n}\}$} \\
    &\quad \Leftrightarrow \quad
    \text{$(A - \lambda I_n)$ is not invertible} \\
    &\quad \Leftrightarrow \quad \det(A - \lambda I_n) = 0_F.
    \qedhere
  \end{align*}
\end{proof}

\begin{theorem}
  Let $A, B \in F^{n \times n}$ and $\lambda \in F$.
  If $A \sim B$, then
  \begin{equation*}
    \det(A - \lambda I_n) = \det(B - \lambda I_n).
  \end{equation*}
\end{theorem}
\begin{proof}
  Suppose that $Q \in F^{n \times n}$ is invertible such that $A = Q^{-1}BQ$.
  Then we have
  \begin{align*}
    \det(A - \lambda I_n)
    &= \det(Q^{-1}BQ - \lambda Q^{-1}I_nQ) \\
    &= \det(Q^{-1}(B - \lambda I_n)Q) \\
    &= \det(Q^{-1})\det(B - \lambda I_n)\det(Q) \\
    &= \det(B - \lambda I_n).
    \qedhere
  \end{align*}
\end{proof}

\begin{definition}
  Let $V$ be a finite-dimensional vector space with $\dim(V) = n$.
  \begin{itemize}
    \item For any linear operator $T: V \to V$, the \emph{characteristic
    polynomial} of $T$ is
    \begin{equation*}
      f_T(t) = \det([T]_\beta^\beta - tI_n),
    \end{equation*}
    where $\beta$ is an arbitrary basis of $V$.
    \item For any $A \in F^{n \times n}$, the \emph{characteristic polynomial}
    of $A$ is
    \begin{equation*}
      f_A(t) = \det(A - tI_n).
    \end{equation*}
  \end{itemize}
\end{definition}
\begin{remark}
  The characteristic polynomial of a linear operator $T: V \to V$ is
  well-defined, since $[T]_\beta^\beta \sim [T]_\gamma^\gamma$ holds for any
  bases $\beta$ and $\gamma$ of $V$.
\end{remark}

\begin{theorem}
  Let $V$ be a vector space over a field $F$ and let $T: V \to V$ be linear.
  For any scalar $\lambda \in F$ and for any nonzero vector $x \in V$,
  $(\lambda, x)$ is an eigenpair of $T$ if and only if
  $x \in N(T - \lambda I_V)$.
\end{theorem}
\begin{proof}
  The proof is as follows.
  \begin{align*}
    \text{$(\lambda, x)$ is an eigenpair of $T$}
    &\quad \Leftrightarrow \quad T(x) = \lambda x \\
    &\quad \Leftrightarrow \quad T(x) = (\lambda I_V)(x) \\
    &\quad \Leftrightarrow \quad (T - \lambda I_V)(x) = 0_V \\
    &\quad \Leftrightarrow \quad x \in N(T - \lambda I_V).
    \qedhere
  \end{align*}
\end{proof}

\begin{definition}
  Let $V$ be a vector space over $F$ and let $T: V \to V$ be linear.
  For each scalar $\lambda \in F$, we define
  \begin{equation*}
    E_T(\lambda) = N(T - \lambda I_V).
  \end{equation*}
  If $\lambda$ is an eigenvalue of $T$, then $E_T(\lambda)$ is called the
  \emph{eigenspace} of $T$ with respect to $\lambda$.
\end{definition}

\begin{theorem}
  Let $V$ be a vector space over $F$ and let $T: V \to V$ be linear.
  If $(\lambda_1, x_1), \dots, (\lambda_k, x_k)$ are eigenpairs of $T$ such
  that $\lambda_1, \dots, \lambda_k$ are distinct, then $\{x_1, \dots, x_k\}$
  is linearly independent.
\end{theorem}
\begin{proof}
  The proof is by induction on $k$.
  For $k = 1$, the theorem trivially holds.
  For the inductive step, let $k \geq 2$.
  Suppose that there are scalars $a_1, \dots, a_k \in F$ such that
  \begin{equation*}
    \sum_{i=1}^k a_ix_i = 0_V.
  \end{equation*}
  Applying $T - \lambda_k I_V$ to both sides, we have
  \begin{equation*}
    0_V
    = \sum_{i=1}^k (T - \lambda_k I_V)(a_ix_i)
    = \sum_{i=1}^k a_i(\lambda_i - \lambda_k)x_i
    = \sum_{i=1}^{k-1} a_i(\lambda_i - \lambda_k)x_i.
  \end{equation*}
  Thus, we have $a_i = 0_F$ for each $i \in \{1, \dots, k-1\}$ since
  $\{x_1, \dots, x_{k-1}\}$ is linearly independent by induction hypothesis.
  It follows that $a_k = 0_F$ since
  \begin{equation*}
    a_kx_k = 0_V - \sum_{i=1}^{k-1} a_ix_i = 0_V.
  \end{equation*}
  Thus, $\{x_1, \dots, x_k\}$ is linearly independent, completing the proof.
\end{proof}

\section{Diagonalizability}
\begin{definition}
  Let $V$ be a finite-dimensional vector space over $F$ and let $T: V \to V$ be
  linear.
  For any scalar $\lambda \in F$, the \emph{multiplicity} of $\lambda$ with
  respect to $T$ is the largest nonnegative integer $m$ such that
  \begin{equation*}
    (t-\lambda)^m \mid f_T(t).
  \end{equation*}
\end{definition}

\begin{theorem}
  Let $V$ be a finite-dimensional vector space over $F$ and let $T: V \to V$ be
  linear.
  For any $\lambda \in F$, if $m$ is the multiplicity of $\lambda$ with respect
  to $T$ and $d$ is the dimension of $E_T(\lambda)$, then
  \begin{equation*}
    d \leq m.
  \end{equation*}
\end{theorem}
\begin{proof}
  Let $\{x_1, \dots, x_d\}$ be a basis of $E_T(\lambda)$.
  By replacement theorem, there exists an ordered basis $\beta = \{x_1, \dots,
  x_n\}$ of $V$.
  Note that we have
  \begin{equation*}
    [T]_\beta^\beta =
    \begin{pmatrix}
      \lambda I_d & X \\
      O & Y
    \end{pmatrix},
  \end{equation*}
  where $O$ is an $(n-d) \times d$ zero matrix.
  It follows that
  \begin{equation*}
    f_T(t) = \det
    \begin{pmatrix}
      (\lambda - t)I_d & X \\
      O & Y - tI_{n-d}
    \end{pmatrix}
    = (\lambda - t)^d \det(Y - tI_{n-d}),
  \end{equation*}
  implying
  \begin{equation*}
    (t - \lambda)^d \mid f_T(t).
  \end{equation*}
  Thus, $d \leq m$.
\end{proof}

\begin{theorem}
  Let $V$ be a finite-dimensional vector space with $\dim(V) = n$ and let
  $T: V \to V$ be linear.
  Let $\lambda_1, \dots, \lambda_k$ be the distinct eigenvalues of $T$, and let
  $d_i = \dim(E_T(\lambda_i))$ for $i \in \{1, \dots, k\}$.
  Then $V$ has an eigenbasis of $T$ if and only if
  \begin{equation*}
    \sum_{i=1}^k d_i = n.
  \end{equation*}
\end{theorem}
\begin{proof}
  ($\Leftarrow$)
  For each $i \in \{1, \dots, k\}$ let
  \begin{equation*}
    S_i = \{x_{ij}: 1 \leq j \leq d_i\}
  \end{equation*}
  be a basis of $E_T(\lambda_i)$.
  Suppose that there are scalars $a_{ij} \in F$ for each $i \in
  \{1, \dots, k\}$ and for each $j \in \{1, \dots, d_i\}$ such that
  \begin{equation*}
    \sum_{i=1}^k \sum_{j=1}^{d_i} a_{ij}x_{ij} = 0_V,
  \end{equation*}
  and we define
  \begin{equation*}
    y_i = \sum_{j=1}^{d_i} a_{ij}x_{ij}
  \end{equation*}
  for each $i \in \{1, \dots, k\}$.
  We claim that $y_i = 0_V$ for each $i \in \{1, \dots, k\}$, which is proved
  as follows.

  Let $\pi$ be a permutation over $\{1, \dots, k\}$ such that
  $y_{\pi(1)}, \dots, y_{\pi(\ell)}$ are nonzero and
  $y_{\pi(\ell+1)}, \dots, y_{\pi(k)}$ are zero, where $0 \leq \ell \leq k$.
  Assume for contradiction that $\ell \neq 0$.
  It is obvious that $\{y_{\pi(1)}, y_{\pi(2)}, \dots, y_{\pi(\ell)}\}$ is
  linearly dependent.
  However,
  \begin{equation*}
    (\lambda_{\pi(1)}, y_{\pi(1)}),
    (\lambda_{\pi(2)}, y_{\pi(2)}),
    \dots,
    (\lambda_{\pi(\ell)}, y_{\pi(\ell)})
  \end{equation*}
  are eigenpairs of $T$, implying that $\{y_{\pi(1)}, y_{\pi(2)}, \dots,
  y_{\pi(\ell)}\}$ is linearly independent, contradiction.

  It follows that for each $i \in \{1, \dots, k\}$ we have $y_i = 0_V$, and
  thus $a_{ij} = 0_F$ for each $j \in \{1, \dots, d_i\}$ since $S_i$ is
  linearly independent.
  Therefore,
  \begin{equation*}
    S = \bigcup_{i=1}^k S_i
  \end{equation*}
  is linearly independent, and thus is a basis of $V$.

  ($\Rightarrow$)
  Let $S$ be an eigenbasis of $V$, and let $S_i = S \cap E_T(\lambda_i)$ for
  each $i \in \{1, \dots, k\}$.
  Let $m_i$ is the multiplicity of $\lambda_i$.
  Then we have
  \begin{equation*}
    n
    = \sum_{i=1}^k |S_i|
    \leq \sum_{i=1}^k d_i
    \leq \sum_{i=1}^k m_i
    \leq n,
  \end{equation*}
  implying
  \begin{equation*}
    \sum_{i=1}^k d_i = n.
    \qedhere
  \end{equation*}
\end{proof}

\begin{theorem}
  Let $V$ be a finite-dimensional vector space over $F$ with $\dim(V) = n$ and
  let $T: V \to V$ be linear.
  If $\lambda_1, \lambda_2, \dots, \lambda_k$ are the eigenvalues of $T$, then
  \begin{equation*}
    V = E_T(\lambda_1) \oplus E_T(\lambda_2) \oplus \cdots \oplus
    E_T(\lambda_k)
  \end{equation*}
  if and only if $V$ has an eigenbasis for $T$.
\end{theorem}
\begin{proof}
  ($\Rightarrow$)
  By \Cref{thm:direct-sum}, there is an ordered basis $\beta_i$ of
  $E_T(\lambda_i)$ for each $i \in \{1, \dots, k\}$ such that
  $\beta = \beta_1 \cup \beta_2 \cup \cdots \cup \beta_k$ is an ordered basis
  of $V$.

  ($\Leftarrow$)
  By \Cref{thm:direct-sum}, it suffices to show that there is an ordered basis
  $\beta_i$ of $E_T(\lambda_i)$ for each $i \in \{1, \dots, k\}$ such that
  $\beta_1 \cup \cdots \cup \beta_k$ is an ordered basis of $V$.
  Let $\beta$ be an eigenbasis of $V$ for $T$.
  For each $i \in \{1, \dots, k\}$, let $\beta_i = \beta \cap E_T(\lambda_i)$
  and $d_i = \dim(E_T(\lambda_i))$.
  Note that $|\beta_i| \leq d_i$ holds by the linear independence of $\beta_i$,
  and we have
  \begin{equation*}
    \sum_{i=1}^k d_i = n = \sum_{i=1}^k |\beta_i|.
  \end{equation*}
  It follows that for each $i \in \{1, \dots, k\}$, we have $|\beta_i| = d_i$,
  and thus $\beta_i$ is an ordered basis of $E_T(\lambda_i)$ for each
  $i \in \{1, \dots, k\}$.
\end{proof}

\section{Cayley-Hamilton Theorem}
\begin{definition}
  Let $V$ be a vector space and let $T \in \mathcal{L}(V)$.
  A subspace $W$ of $V$ is a \emph{$T$-invariant subspace} of $V$ if
  \begin{equation*}
    T(W) \subseteq W.
  \end{equation*}
\end{definition}

\begin{theorem}
  Let $V$ be a finite-dimensional vector space and let $T: V \to V$ be linear.
  Let $W$ be a $T$-invariant subspace of $V$ and define $T': W \to W$ as the
  transformation such that $T'(x) = T(x)$ for any $x \in W$.
  Then we have
  \begin{equation*}
    f_{T'}(t) \mid f_T(t).
  \end{equation*}
\end{theorem}
\begin{proof}
  Let $\gamma = (x_1, \dots, x_k)$ be an ordered basis of $W$.
  By replacement theorem (\Cref{thm:replacement}), there is an ordered basis
  $\beta = (x_1, \dots, x_k, x_{k+1}, \dots, x_n)$ of $V$.
  It can be shown that
  \begin{equation*}
    [T]_\beta^\beta =
    \begin{pmatrix}
      [T']_\gamma^\gamma & X \\
      O & Y
    \end{pmatrix}
  \end{equation*}
  for some $X \in F^{k \times (n-k)}$ and $Y \in F^{(n-k) \times (n-k)}$.
  Thus, we have
  \begin{align*}
    f_T(t)
    &= \det([T]_\beta^\beta - tI_n) \\
    &= \det
    \begin{pmatrix}
      [T']_\gamma^\gamma - tI_k & X \\
      O & Y - tI_{n-k}
    \end{pmatrix} \\
    &= \det([T']_\gamma^\gamma - tI_k) \cdot \det(Y - tI_{n-k}) \\
    &= f_{T'}(t) \cdot \det(Y - tI_{n-k}).
    \qedhere
  \end{align*}
\end{proof}

\begin{definition}
  Let $V$ be a vector space and let $T \in \mathcal{L}(V)$.
  The \emph{$T$-cyclic subspace} of $V$ generated by $x \in V$ is defined as
  \begin{equation*}
    C_T(x) = \spn\left(\bigcup_{i=0}^\infty \{T^i(x)\}\right).
  \end{equation*}
\end{definition}

\begin{theorem}
  Let $V$ be a vector space and let $T \in \mathcal{L}(V)$.
  Then the following statements hold for any $x \in V$.
  \begin{enumerate}
    \item $C_T(x)$ is a $T$-invariant subspace of $V$.
    \item If $W$ is a $T$-invariant subspace of $V$ with $x \in W$, then
    $C_T(x) \subseteq W$.
  \end{enumerate}
\end{theorem}
\begin{proof}
  \leavevmode
  \begin{enumerate}
    \item Suppose that $y \in C_T(x)$ with
    \begin{equation*}
      y = \sum_{i=0}^k a_iT^i(x).
    \end{equation*}
    Then we have
    \begin{equation*}
      T(y)
      = T\left(\sum_{i=0}^k a_iT^i(x)\right)
      = \sum_{i=0}^k a_iT^{i+1}(x)
      \in C_T(x).
    \end{equation*}
    It follows that $T(C_T(x)) \subseteq C_T(x)$, and thus $C_T(x)$ is
    $T$-invariant.
    \item Since $x \in U$ and $T(U) \subseteq U$, we can conclude that
    $T^i(x) \in U$ holds for any nonnegative integer $i$.
    Thus, we have
    \begin{equation*}
      \bigcup_{i=0}^\infty \{T^i(x)\} \subseteq U,
    \end{equation*}
    implying
    \begin{equation*}
      C_T(x)
      = \spn\left(\bigcup_{i=0}^\infty \{T^i(x)\}\right)
      \subseteq U.
      \qedhere
    \end{equation*}
  \end{enumerate}
\end{proof}