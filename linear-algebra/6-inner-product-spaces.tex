\chapter{Inner Product Spaces}
\section{Inner Products and Norms}
\begin{definition}
  \label{def:inner-product}
  Let $V$ be a vector space over a field $F \in \{\mathbb{R}, \mathbb{C}\}$.
  A function
  \begin{equation*}
    \inner{\cdot}{\cdot}: V \times V \to F
  \end{equation*}
  is called an \emph{inner product} on $V$ if it satisfies the following
  properties for all $x, x', y \in V$.
  \begin{enumerate}
    \item $\inner{ax + x'}{y} = a\inner{x}{y} + \inner{x'}{y}$.
    \item $\inner{x}{y} = \overline{\inner{y}{x}}$.
    \item $\inner{x}{x} \in \mathbb{R}^+$ for any $x \in V \setminus \{0_V\}$.
  \end{enumerate}
  A vector space equipped with an inner product is called an
  \emph{inner product space}.
\end{definition}

\begin{proposition}
  \label{thm:inner-product-basic}
  Let $V$ be an inner product space over a field
  $F \in \{\mathbb{R}, \mathbb{C}\}$.
  Then the following statements are true for $x, y, y' \in V$ and $a \in F$.
  \begin{enumerate}
    \item $\inner{x}{ay + y'} = \overline{a}\inner{x}{y} + \inner{x}{y'}$.
    \item $\inner{x}{0_V} = 0_F = \inner{0_V}{x}$.
    \item $\inner{x}{x} = 0_F$ if and only if $x = 0_V$.
    \item If $\inner{x}{y} = \inner{x}{y'}$ holds for all $x \in V$, then
    $y = y'$.
  \end{enumerate}
\end{proposition}
\begin{proof}
  \leavevmode
  \begin{enumerate}
    \item It is proved by
    \begin{equation*}
      \inner{x}{ay + y'}
      = \overline{\inner{ay + y'}{x}}
      = \overline{a\inner{y}{x} + \inner{y'}{x}}
      = \overline{a}\inner{x}{y} + \inner{x}{y'}.
    \end{equation*}

    \item By
    \begin{equation*}
      \inner{x}{x}
      = \inner{x}{1_Fx + 0_V}
      = \overline{1_F}\inner{x}{x} + \inner{x}{0_V}
      = \inner{x}{x} + \inner{x}{0_V}
    \end{equation*}
    and
    \begin{equation*}
      \inner{x}{x}
      = \inner{1_Fx + 0_V}{x}
      = 1_F\inner{x}{x} + \inner{0_V}{x}
      = \inner{x}{x} + \inner{0_V}{x},
    \end{equation*}
    we have $\inner{x}{0_V} = 0_F = \inner{0_V}{x}$.

    \item ($\Leftarrow$) If $x = 0_V$, then $\inner{x}{x} = 0_F$ by (b).

    ($\Rightarrow$) If $\inner{x}{x} = 0_F$, then $x = 0_V$ by
    \Cref{def:inner-product} (c).

    \item Note that
    \begin{equation*}
      \inner{x}{y - y'} = \inner{x}{y} + \overline{(-1_F)}\inner{x}{y'} = 0_F
    \end{equation*}
    holds for all $x \in V$.
    Since $\inner{y - y'}{y - y'} = 0_F$, we have $y - y' = 0_V$,
    and thus $y = y'$.
    \qedhere
  \end{enumerate}
\end{proof}

\begin{definition}
  Let $V$ be an inner product space over a field $F$.
  \begin{itemize}
    \item Vectors $x$ and $y$ in $V$ are \emph{orthogonal} if
    \begin{equation*}
      \inner{x}{y} = 0_F.
    \end{equation*}
    \item A subset $S$ of $V$ is \emph{orthogonal} if any two distinct vectors
    in $S$ are orthogonal.
  \end{itemize}
\end{definition}

\begin{theorem}
  \label{thm:orthogonal-decomposition}
  Let $V$ be an inner product space over a field $F$.
  Let $S$ be an orthogonal subset of $V \setminus \{0_V\}$ and let
  $x_1, \dots, x_n$ be distinct vectors in $S$.
  Then for $y \in V$, if
  \begin{equation*}
    y = \sum_{i=1}^n a_ix_i
  \end{equation*}
  for some $a_1, \dots, a_n \in F$, then
  \begin{equation*}
    a_i = \frac{\inner{y}{x_i}}{\inner{x_i}{x_i}}
  \end{equation*}
  for each $i \in \{1, \dots, n\}$.
\end{theorem}
\begin{proof}
  For each $i \in \{1, \dots, n\}$, we have
  \begin{equation*}
    \inner{y}{x_i}
    = \inner{\sum_{j=1}^n a_jx_j}{x_i}
    = \sum_{j=1}^n a_j \inner{x_j}{x_i}
    = a_i \inner{x_i}{x_i},
  \end{equation*}
  implying
  \begin{equation*}
    a_i = \frac{\inner{y}{x_i}}{\inner{x_i}{x_i}}.
    \qedhere
  \end{equation*}
\end{proof}

\begin{corollary}
  \label{thm:orthogonal-linearly-independent}
  Let $V$ be an inner product space over a field $F$.
  If $S$ is an orthogonal subset of $V \setminus \{0_V\}$, then $S$ is linearly
  independent.
\end{corollary}
\begin{proof}
  Suppose that there exist scalars $a_1, \dots, a_n \in F$ and distinct vectors
  $x_1, \dots, x_n \in S$ such that
  \begin{equation*}
    \sum_{i=1}^n a_ix_i = 0_V.
  \end{equation*}
  Then we have
  \begin{equation*}
    a_i = \frac{\inner{0_V}{x_i}}{\inner{x_i}{x_i}} = 0_F
  \end{equation*}
  for each $i \in \{1, \dots, n\}$.
  Thus, $S$ is linearly independent.
\end{proof}

\begin{theorem}[Gram-Schmidt Process]
  Let $V$ be a finite-dimensional inner product space over a field $F$.
  Let $R = \{x_1, \dots, x_n\}$ be a linearly independent subset of $V$.
  Then the set $S = \{y_1, \dots, y_n\}$ with
  \begin{equation*}
    y_i = x_i - \sum_{j=1}^{i-1} \frac{\inner{x_i}{y_j}}{\inner{y_j}{y_j}}y_j
  \end{equation*}
  for $1 \leq i \leq n$ is an orthogonal set of nonzero vectors
  satisfying $\spn(S) = \spn(R)$.
\end{theorem}
\begin{proof}
  The proof is by induction on $n$.
  The theorem holds for $n = 0$.
  To show the induction step, let $n \geq 1$.
  By the induction hypothesis, $\inner{y_j}{y_i} = 0_F$ for distinct
  $i, j \in \{1, \dots, n - 1\}$.
  Then since for $i \in \{1, \dots, n - 1\}$, we have
  \begin{align*}
    \inner{y_n}{y_i}
    &= \inner{x_n - \sum_{j=1}^{n-1}
              \frac{\inner{x_n}{y_j}}{\inner{y_j}{y_j}}y_j}
             {y_i} \\
    &= \inner{x_n}{y_i} - \sum_{j=1}^{n-1}
       \frac{\inner{x_n}{y_j}}{\inner{y_j}{y_j}}\inner{y_j}{y_i} \\
    &= \inner{x_n}{y_i}
       - \frac{\inner{x_n}{y_i}}{\inner{y_i}{y_i}}\inner{y_i}{y_i} \\
    &= 0_F,
  \end{align*}
  we can conclude that $S$ is orthogonal.
  Furthermore, if $y_n = 0_V$, then
  \begin{equation*}
    x_n \in \spn(\{y_1, \dots, y_{n-1}\}) = \spn(\{x_1, \dots, x_{n-1}\})
  \end{equation*}
  because
  \begin{equation*}
    x_n = y_n + \sum_{j=1}^{n-1} \frac{\inner{x_n}{y_j}}{\inner{y_j}{y_j}}y_j,
  \end{equation*}
  contradiction to the fact that $R$ is linearly independent.
  Thus, $y_n \neq 0_V$, implying $0_V \notin S$.
  It follows that $S$ is linearly independent by
  \Cref{thm:orthogonal-linearly-independent}.
  Therefore, since $|S| = \dim(\spn(R))$, we have $\spn(S) = \spn(R)$.
\end{proof}

\begin{definition}
  Let $V$ be an inner product space.
  For each vector $x \in S$, the \emph{norm} of $x$ is a nonnegative real
  number, defined as
  \begin{equation*}
    \norm{x} = \sqrt{\inner{x}{x}}.
  \end{equation*}
\end{definition}

\begin{proposition}
  Let $V$ be an inner produce space over a field
  $F \in \{\mathbb{R}, \mathbb{C}\}$.
  Then the following statements are true for any vectors $x, y \in V$ and any
  scalar $a \in F$.
  \begin{enumerate}
    \item $\norm{ax} = |a| \cdot \norm{x}$.
    \item $\norm{x} = 0_F$ if and only if $x = 0_V$.
  \end{enumerate}
\end{proposition}
\begin{proof}
  \leavevmode
  \begin{enumerate}
    \item We have
    \begin{equation*}
      \norm{ax}
      = \sqrt{\inner{ax}{ax}}
      = \sqrt{a\overline{a}\inner{x}{x}}
      = \sqrt{|a|^2 \inner{x}{x}}
      = |a| \cdot \norm{x}.
    \end{equation*}

    \item We have
    \begin{equation*}
      \norm{x} = 0_F
      \quad \Leftrightarrow \quad
      \inner{x}{x} = 0_F
      \quad \Leftrightarrow \quad
      x = 0_V.
    \end{equation*}
  \end{enumerate}
\end{proof}

\begin{theorem}[Pythagorean Theorem]
  \label{thm:pythagoras}
  Let $V$ be an inner produce space over a field
  $F \in \{\mathbb{R}, \mathbb{C}\}$.
  Then for any vectors $x, y \in V$ with $\inner{x}{y} = 0_F$, we have
  \begin{equation*}
    \norm{x + y}^2 = \norm{x}^2 + \norm{y}^2.
  \end{equation*}
\end{theorem}
\begin{proof}
  We have
  \begin{align*}
    \norm{x + y}^2
    &= \inner{x + y}{x + y} \\
    &= \inner{x}{x + y} + \inner{y}{x + y} \\
    &= \inner{x}{x} + \inner{x}{y} + \inner{y}{x} + \inner{y}{y} \\
    &= \inner{x}{x} + 0_F + 0_F + \inner{y}{y} \\
    &= \norm{x}^2 + \norm{y}^2.
    \qedhere
  \end{align*}
\end{proof}

\begin{definition}
  Let $V$ be an inner produce space over a field
  $F \in \{\mathbb{R}, \mathbb{C}\}$.
  We say that a subset $S$ of $V$ is \emph{orthonormal} if $S$ is orthogonal
  and $\norm{x} = 1_F$ for each $x \in S$.
\end{definition}

\begin{theorem}
  \label{thm:orthonormal-decomposition}
  Let $V$ be an inner produce space over a field
  $F \in \{\mathbb{R}, \mathbb{C}\}$.
  Let $S$ be an orthonormal subset of $V$ and let
  $x_1, \dots, x_n$ be distinct vectors in $S$.
  Then for $y \in V$, if
  \begin{equation*}
    y = \sum_{i=1}^n a_ix_i
  \end{equation*}
  for some $a_1, \dots, a_n \in F$, then
  \begin{equation*}
    a_i = \inner{y}{x_i}
  \end{equation*}
  for each $i \in \{1, \dots, n\}$.
\end{theorem}
\begin{proof}
  Since $S$ is orthonormal, we have $0_V \notin S$.
  It follows that
  \begin{equation*}
    a_i
    = \frac{\inner{y}{x_i}}{\inner{x_i}{x_i}}
    = \frac{\inner{y}{x_i}}{1_F}
    = \inner{y}{x_i}
  \end{equation*}
  for each $i \in \{1, \dots, n\}$ by \Cref{thm:orthogonal-decomposition}.
\end{proof}

\section{Adjoints}
\begin{theorem}
  \label{thm:functional}
  Let $V$ be a finite-dimensional inner produce space over
  $F \in \{\mathbb{R}, \mathbb{C}\}$.
  Let $f: V \to F$ be a linear transformation.
  Then there exists a unique vector $y \in V$ such that
  \begin{equation*}
    f(x) = \inner{x}{y}
  \end{equation*}
  for all $x \in V$.
\end{theorem}
\begin{proof}
  Let $S = \{x_1, x_2, \dots, x_n\}$ be an orthonormal basis
  for $V$.
  Then we have
  \begin{align*}
    f(x)
    &= f\left(\sum_{i=1}^n \inner{x}{x_i} \cdot x_i\right) \\
    &= \sum_{i=1}^n \inner{x}{x_i} \cdot f(x_i) \\
    &= \inner{x}{\sum_{i=1}^n \overline{f(x_i)} \cdot x_i}.
  \end{align*}
  Thus, there exists
  \begin{equation*}
    y = \sum_{i=1}^n \overline{f(x_i)} \cdot x_i
  \end{equation*}
  such that $f(x) = \inner{x}{y}$ for all $x \in V$.

  Furthermore, if there exists $y' \in V$ such that $f(x) = \inner{x}{y'}$
  for all $x \in V$, then we have $y' = y$ by \Cref{thm:inner-product-basic}
  (d), which completes the proof.
\end{proof}

\begin{theorem}
  Let $V$ be a finite-dimensional inner produce space over
  $F \in \{\mathbb{R}, \mathbb{C}\}$.
  For any linear operator $T: V \to V$, there exists a unique operator
  $T': V \to V$ such that
  \begin{equation*}
    \inner{T(x)}{y} = \inner{x}{T'(y)}
  \end{equation*}
  for all $x, y \in V$.
  Also, $T'$ is linear.
\end{theorem}
\begin{proof}
  Suppose that $y \in V$ is an arbitrary vector.
  Let $f: V \to F$ be a function such that $f(x) = \inner{T(x)}{y}$ for each
  $x \in V$.
  Then $f$ is linear since
  \begin{align*}
    f(ax_1 + x_2)
    &= \inner{T(ax_1 + x_2)}{y} \\
    &= \inner{aT(x_1) + T(x_2)}{y} \\
    &= a\inner{T(x_1)}{y} + \inner{T(x_2)}{y} \\
    &= af(x_1) + f(x_2)
  \end{align*}
  holds for each $a \in F$ and for each $x_1, x_2 \in V$.
  Since $f$ is linear, there exists a vector $y' \in V$ such that
  $f(x) = \inner{x}{y'}$ by \Cref{thm:functional}.
  Thus, we can define $T': V \to V$ as the functoin with $T'(y) = y'$,
  implying $\inner{T(x)}{y} = \inner{x}{T'(y)}$ for each $x, y \in V$.

  Now we prove that $T'$ is linear.
  For any $a \in F$ and $x, y_1, y_2 \in V$, we have
  \begin{align*}
    \inner{x}{T'(ay_1 + y_2)}
    &= \inner{T(x)}{ay_1 + y_2} \\
    &= \overline{a}\inner{T(x)}{y_1} + \inner{T(x)}{y_2} \\
    &= \overline{a}\inner{x}{T'(y_1)} + \inner{x}{T'(y_2)} \\
    &= \inner{x}{aT'(y_1) + T'(y_2)}.
  \end{align*}
  Thus, we can conclude that $T'(ay_1 + y_2) = aT'(y_1) + T'(y_2)$ for any
  $a \in F$ and $y_1, y_2 \in V$ by \Cref{thm:inner-product-basic} (d).

  To show that $T'$ is unique, suppose that $T'': V \to V$ is linear and
  satisfies $\inner{T(x)}{y} = \inner{x}{T''(y)}$ for any $x, y \in V$.
  Then we have
  \begin{equation*}
    \inner{x}{T''(y)} = \inner{T(x)}{y} = \inner{x}{T'(y)},
  \end{equation*}
  implying $T''(y) = T(y)$ for any $y \in V$ by \Cref{thm:inner-product-basic}
  (d).
  Thus, $T'' = T'$.
\end{proof}

\begin{definition}
  Let $V$ be a finite-dimensional inner produce space over
  $F \in \{\mathbb{R}, \mathbb{C}\}$ and let $T: V \to V$ be linear.
  The \emph{adjoint} of $T$, denoted $T^*$, is the linear operator satisfying
  \begin{equation*}
    \inner{T(x)}{y} = \inner{x}{T^*(y)}
  \end{equation*}
  for all $x, y \in V$.
\end{definition}