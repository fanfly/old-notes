\chapter{Inner Product Spaces}
\section{Inner Products and Norms}
\begin{definition}
  \label{def:inner-product}
  Let $V$ be a vector space over a field $F \in \{\mathbb{R}, \mathbb{C}\}$.
  A function
  \begin{equation*}
    \inner{\cdot}{\cdot}: V \times V \to F
  \end{equation*}
  is called an \emph{inner product} on $V$ if it satisfies the following
  properties for all $x, x', y \in V$.
  \begin{enumerate}
    \item $\inner{ax + x'}{y} = a\inner{x}{y} + \inner{x'}{y}$.
    \item $\inner{x}{y} = \overline{\inner{y}{x}}$.
    \item $\inner{x}{x} \in \mathbb{R}^+$ for any $x \in V \setminus \{0_V\}$.
  \end{enumerate}
  A vector space equipped with an inner product is called an
  \emph{inner product space}.
\end{definition}

\begin{proposition}
  \label{thm:inner-product-basic}
  Let $V$ be an inner product space over a field
  $F \in \{\mathbb{R}, \mathbb{C}\}$.
  Then the following statements are true for $x, y, y' \in V$ and $a \in F$.
  \begin{enumerate}
    \item $\inner{x}{ay + y'} = \overline{a}\inner{x}{y} + \inner{x}{y'}$.
    \item $\inner{x}{0_V} = 0_F = \inner{0_V}{x}$.
    \item $\inner{x}{x} = 0_F$ if and only if $x = 0_V$.
    \item If $\inner{x}{y} = \inner{x}{y'}$ holds for all $x \in V$, then
    $y = y'$.
  \end{enumerate}
\end{proposition}
\begin{proof}
  \leavevmode
  \begin{enumerate}
    \item It is proved by
    \begin{equation*}
      \inner{x}{ay + y'}
      = \overline{\inner{ay + y'}{x}}
      = \overline{a\inner{y}{x} + \inner{y'}{x}}
      = \overline{a}\inner{x}{y} + \inner{x}{y'}.
    \end{equation*}

    \item By
    \begin{equation*}
      \inner{x}{x}
      = \inner{x}{1_Fx + 0_V}
      = \overline{1_F}\inner{x}{x} + \inner{x}{0_V}
      = \inner{x}{x} + \inner{x}{0_V}
    \end{equation*}
    and
    \begin{equation*}
      \inner{x}{x}
      = \inner{1_Fx + 0_V}{x}
      = 1_F\inner{x}{x} + \inner{0_V}{x}
      = \inner{x}{x} + \inner{0_V}{x},
    \end{equation*}
    we have $\inner{x}{0_V} = 0_F = \inner{0_V}{x}$.

    \item ($\Leftarrow$) If $x = 0_V$, then $\inner{x}{x} = 0_F$ by (b).

    ($\Rightarrow$) If $\inner{x}{x} = 0_F$, then $x = 0_V$ by
    \Cref{def:inner-product} (c).

    \item Note that
    \begin{equation*}
      \inner{x}{y - y'} = \inner{x}{y} + \overline{(-1_F)}\inner{x}{y'} = 0_F
    \end{equation*}
    holds for all $x \in V$.
    Since $\inner{y - y'}{y - y'} = 0_F$, we have $y - y' = 0_V$,
    and thus $y = y'$.
    \qedhere
  \end{enumerate}
\end{proof}

\begin{definition}
  Let $V$ be an inner product space over a field $F$.
  \begin{itemize}
    \item For $x, y \in V$, we say that $x$ and $y$ are \emph{orthogonal} if
    \begin{equation*}
      \inner{x}{y} = 0_F.
    \end{equation*}
    \item A subset $S$ of $V$ is \emph{orthogonal} if any two distinct vectors
    in $S$ are orthogonal.
  \end{itemize}
\end{definition}

\begin{theorem}
  \label{thm:orthogonal-decomposition}
  Let $V$ be an inner product space over a field $F$.
  Let $S$ be an orthogonal subset of $V \setminus \{0_V\}$ and let
  $x_1, \dots, x_n$ be distinct vectors in $S$.
  Then for $y \in V$, if
  \begin{equation*}
    y = \sum_{i=1}^n a_ix_i
  \end{equation*}
  for some $a_1, \dots, a_n \in F$, then
  \begin{equation*}
    a_i = \frac{\inner{y}{x_i}}{\inner{x_i}{x_i}}
  \end{equation*}
  for each $i \in \{1, \dots, n\}$.
\end{theorem}
\begin{proof}
  For each $i \in \{1, \dots, n\}$, we have
  \begin{equation*}
    \inner{y}{x_i}
    = \inner{\sum_{j=1}^n a_jx_j}{x_i}
    = \sum_{j=1}^n a_j \inner{x_j}{x_i}
    = a_i \inner{x_i}{x_i},
  \end{equation*}
  implying
  \begin{equation*}
    a_i = \frac{\inner{y}{x_i}}{\inner{x_i}{x_i}}.
    \qedhere
  \end{equation*}
\end{proof}

\begin{corollary}
  \label{thm:orthogonal-linearly-independent}
  Let $V$ be an inner product space over a field $F$.
  If $S$ is an orthogonal subset of $V \setminus \{0_V\}$, then $S$ is linearly
  independent.
\end{corollary}
\begin{proof}
  Suppose that there exist scalars $a_1, \dots, a_n \in F$ and distinct vectors
  $x_1, \dots, x_n \in S$ such that
  \begin{equation*}
    \sum_{i=1}^n a_ix_i = 0_V.
  \end{equation*}
  Then we have
  \begin{equation*}
    a_i = \frac{\inner{0_V}{x_i}}{\inner{x_i}{x_i}} = 0_F
  \end{equation*}
  for each $i \in \{1, \dots, n\}$.
  Thus, $S$ is linearly independent.
\end{proof}

\begin{theorem}[Gram-Schmidt Process]
  Let $V$ be a finite-dimensional inner product space over a field $F$.
  Let $R = \{x_1, \dots, x_n\}$ be a linearly independent subset of $V$.
  Then the set $S = \{y_1, \dots, y_n\}$ with
  \begin{equation*}
    y_i = x_i - \sum_{j=1}^{i-1} \frac{\inner{x_i}{y_j}}{\inner{y_j}{y_j}}y_j
  \end{equation*}
  for $1 \leq i \leq n$ is an orthogonal set of nonzero vectors
  satisfying $\spn(S) = \spn(R)$.
\end{theorem}
\begin{proof}
  The proof is by induction on $n$.
  The theorem holds for $n = 0$.
  To show the induction step, let $n \geq 1$.
  By the induction hypothesis, $\inner{y_j}{y_i} = 0_F$ for distinct
  $i, j \in \{1, \dots, n - 1\}$.
  Then since for $i \in \{1, \dots, n - 1\}$, we have
  \begin{align*}
    \inner{y_n}{y_i}
    &= \inner{x_n - \sum_{j=1}^{n-1}
              \frac{\inner{x_n}{y_j}}{\inner{y_j}{y_j}}y_j}
             {y_i} \\
    &= \inner{x_n}{y_i} - \sum_{j=1}^{n-1}
       \frac{\inner{x_n}{y_j}}{\inner{y_j}{y_j}}\inner{y_j}{y_i} \\
    &= \inner{x_n}{y_i}
       - \frac{\inner{x_n}{y_i}}{\inner{y_i}{y_i}}\inner{y_i}{y_i} \\
    &= 0_F,
  \end{align*}
  we can conclude that $S$ is orthogonal.
  Furthermore, if $y_n = 0_V$, then
  \begin{equation*}
    x_n \in \spn(\{y_1, \dots, y_{n-1}\}) = \spn(\{x_1, \dots, x_{n-1}\})
  \end{equation*}
  because
  \begin{equation*}
    x_n = y_n + \sum_{j=1}^{n-1} \frac{\inner{x_n}{y_j}}{\inner{y_j}{y_j}}y_j,
  \end{equation*}
  contradiction to the fact that $R$ is linearly independent.
  Thus, $y_n \neq 0_V$, implying $0_V \notin S$.
  It follows that $S$ is linearly independent by
  \Cref{thm:orthogonal-linearly-independent}.
  Therefore, since $|S| = \dim(\spn(R))$, we have $\spn(S) = \spn(R)$.
\end{proof}

\begin{definition}
  Let $V$ be an inner product space.
  For each vector $x \in S$, the \emph{norm} of $x$ is a nonnegative real
  number, defined as
  \begin{equation*}
    \norm{x} = \sqrt{\inner{x}{x}}.
  \end{equation*}
\end{definition}

\begin{proposition}
  Let $V$ be an inner product space over a field
  $F \in \{\mathbb{R}, \mathbb{C}\}$.
  Then the following statements are true for any vectors $x, y \in V$ and any
  scalar $a \in F$.
  \begin{enumerate}
    \item $\norm{ax} = |a| \cdot \norm{x}$.
    \item $\norm{x} = 0_F$ if and only if $x = 0_V$.
  \end{enumerate}
\end{proposition}
\begin{proof}
  \leavevmode
  \begin{enumerate}
    \item We have
    \begin{equation*}
      \norm{ax}
      = \sqrt{\inner{ax}{ax}}
      = \sqrt{a\overline{a}\inner{x}{x}}
      = \sqrt{|a|^2 \inner{x}{x}}
      = |a| \cdot \norm{x}.
    \end{equation*}

    \item We have
    \begin{equation*}
      \norm{x} = 0_F
      \quad \Leftrightarrow \quad
      \inner{x}{x} = 0_F
      \quad \Leftrightarrow \quad
      x = 0_V.
    \end{equation*}
  \end{enumerate}
\end{proof}

\begin{theorem}[Pythagorean Theorem]
  \label{thm:pythagoras}
  Let $V$ be an inner product space over a field
  $F \in \{\mathbb{R}, \mathbb{C}\}$.
  Then for any vectors $x, y \in V$ with $\inner{x}{y} = 0_F$, we have
  \begin{equation*}
    \norm{x + y}^2 = \norm{x}^2 + \norm{y}^2.
  \end{equation*}
\end{theorem}
\begin{proof}
  We have
  \begin{align*}
    \norm{x + y}^2
    &= \inner{x + y}{x + y} \\
    &= \inner{x}{x + y} + \inner{y}{x + y} \\
    &= \inner{x}{x} + \inner{x}{y} + \inner{y}{x} + \inner{y}{y} \\
    &= \inner{x}{x} + 0_F + 0_F + \inner{y}{y} \\
    &= \norm{x}^2 + \norm{y}^2.
    \qedhere
  \end{align*}
\end{proof}

\begin{definition}
  Let $V$ be an inner product space over a field
  $F \in \{\mathbb{R}, \mathbb{C}\}$.
  We say that a subset $S$ of $V$ is \emph{orthonormal} if $S$ is orthogonal
  and $\norm{x} = 1_F$ for each $x \in S$.
\end{definition}

\begin{theorem}
  \label{thm:orthonormal-decomposition}
  Let $V$ be an inner product space over a field
  $F \in \{\mathbb{R}, \mathbb{C}\}$.
  Let $S$ be an orthonormal subset of $V$ and let
  $x_1, \dots, x_n$ be distinct vectors in $S$.
  Then for $y \in V$, if
  \begin{equation*}
    y = \sum_{i=1}^n a_ix_i
  \end{equation*}
  for some $a_1, \dots, a_n \in F$, then
  \begin{equation*}
    a_i = \inner{y}{x_i}
  \end{equation*}
  for each $i \in \{1, \dots, n\}$.
\end{theorem}
\begin{proof}
  Since $S$ is orthonormal, we have $0_V \notin S$.
  It follows that
  \begin{equation*}
    a_i
    = \frac{\inner{y}{x_i}}{\inner{x_i}{x_i}}
    = \frac{\inner{y}{x_i}}{1_F}
    = \inner{y}{x_i}
  \end{equation*}
  for each $i \in \{1, \dots, n\}$ by \Cref{thm:orthogonal-decomposition}.
\end{proof}

\begin{definition}
  Let $V$ be an inner product space over $F$ and let $S$ be a subspace of $V$.
  The \emph{orthogonal complement} of $S$, denoted $S^\perp$, is the set of
  vectors that are orthogonal to every vector in $S$, i.e.,
  \begin{equation*}
    S^\perp = \{x \in V: \text{$\inner{x}{y} = 0_F$ for all $y \in S$}\}.
  \end{equation*}
\end{definition}

\begin{theorem}
  Let $V$ be an inner product space over $F$.
  For any subset $S$ of $V$, $S^\perp$ is a subspace of $V$.
\end{theorem}
\begin{proof}
  We have $0_V \in S^\perp$ since $\inner{0_V}{z} = 0_F$ for any $z \in S$.
  For any $a \in F$ and $x, y \in S^\perp$, we have
  \begin{align*}
    \inner{ax + y}{z}
    &= a\inner{x}{z} + \inner{y}{z} \\
    &= a0_F + 0_F \\
    &= 0_F
  \end{align*}
  for any $z \in S$, implying $ax + y \in S^\perp$.
  Thus, $S^\perp$ is a subspace of $V$ by \Cref{cor:subspace}.
\end{proof}

\begin{theorem}
  Let $V$ be a finite-dimensional inner product space over $F$.
  If $W$ is a subspace of $V$, then $W \oplus W^\perp = V$.
\end{theorem}
\begin{proof}
  Let $R = \{y_1, \dots, y_k\}$ be an orthonormal basis of $W$.
  We have $W + W^\perp \subseteq V$ since $W$ and $W^\perp$ are subspaces of
  $V$.
  To prove $V \subseteq W + W^\perp$, suppose that $x \in V$, and let
  \begin{equation*}
    y = \sum_{i=1}^k \inner{x}{y_i}y_i
  \end{equation*}
  be a vector in $W$.
  Then $x - y \in V^\perp$ since
  \begin{align*}
    \inner{x - y}{y_j}
    &= \inner{x - \sum_{i=1}^k \inner{x}{y_i}y_i}{y_j} \\
    &= \inner{x}{y_j} - \sum_{i=1}^k \inner{x}{y_i} \inner{y_i}{y_j} \\
    &= \inner{x}{y_j} - \inner{x}{y_j} \\
    &= 0_F.
  \end{align*}
  Thus,
  \begin{equation*}
    x = y + (x - y) \in V + V^\perp,
  \end{equation*}
  implying $V \subseteq W + W^\perp$, and thus $W + W^\perp = V$.

  Furthermore, for any $x \in W \cap W^\perp$, we have $\inner{x}{x} = 0_F$,
  implying $x = 0_V$.
  Thus, we have $W \cap W^\perp = \{0_V\}$, which implies
  $W \oplus W^\perp = V$.
\end{proof}

\section{The Adjoint of a Linear Operator}
\begin{theorem}
  \label{thm:functional}
  Let $V$ be a finite-dimensional inner product space over
  $F \in \{\mathbb{R}, \mathbb{C}\}$.
  Let $f: V \to F$ be a linear transformation.
  Then there exists a unique vector $y \in V$ such that
  \begin{equation*}
    f(x) = \inner{x}{y}
  \end{equation*}
  for all $x \in V$.
\end{theorem}
\begin{proof}
  Let $S = \{x_1, x_2, \dots, x_n\}$ be an orthonormal basis
  for $V$.
  Then we have
  \begin{align*}
    f(x)
    &= f\left(\sum_{i=1}^n \inner{x}{x_i} \cdot x_i\right) \\
    &= \sum_{i=1}^n \inner{x}{x_i} \cdot f(x_i) \\
    &= \inner{x}{\sum_{i=1}^n \overline{f(x_i)} \cdot x_i}.
  \end{align*}
  Thus, there exists
  \begin{equation*}
    y = \sum_{i=1}^n \overline{f(x_i)} \cdot x_i
  \end{equation*}
  such that $f(x) = \inner{x}{y}$ for all $x \in V$.

  Furthermore, if there exists $y' \in V$ such that $f(x) = \inner{x}{y'}$
  for all $x \in V$, then we have $y' = y$ by \Cref{thm:inner-product-basic}
  (d), which completes the proof.
\end{proof}

\begin{theorem}
  Let $V$ be a finite-dimensional inner product space over
  $F \in \{\mathbb{R}, \mathbb{C}\}$.
  For any linear operator $T: V \to V$, there exists a unique operator
  $T': V \to V$ such that
  \begin{equation*}
    \inner{T(x)}{y} = \inner{x}{T'(y)}
  \end{equation*}
  for all $x, y \in V$.
  Also, $T'$ is linear.
\end{theorem}
\begin{proof}
  Suppose that $y \in V$ is an arbitrary vector.
  Let $f: V \to F$ be a function such that $f(x) = \inner{T(x)}{y}$ for each
  $x \in V$.
  Then $f$ is linear since
  \begin{align*}
    f(ax_1 + x_2)
    &= \inner{T(ax_1 + x_2)}{y} \\
    &= \inner{aT(x_1) + T(x_2)}{y} \\
    &= a\inner{T(x_1)}{y} + \inner{T(x_2)}{y} \\
    &= af(x_1) + f(x_2)
  \end{align*}
  holds for each $a \in F$ and for each $x_1, x_2 \in V$.
  Since $f$ is linear, there exists a vector $y' \in V$ such that
  $f(x) = \inner{x}{y'}$ by \Cref{thm:functional}.
  Thus, we can define $T': V \to V$ as the functoin with $T'(y) = y'$,
  implying $\inner{T(x)}{y} = \inner{x}{T'(y)}$ for each $x, y \in V$.

  Now we prove that $T'$ is linear.
  For any $a \in F$ and $x, y_1, y_2 \in V$, we have
  \begin{align*}
    \inner{x}{T'(ay_1 + y_2)}
    &= \inner{T(x)}{ay_1 + y_2} \\
    &= \overline{a}\inner{T(x)}{y_1} + \inner{T(x)}{y_2} \\
    &= \overline{a}\inner{x}{T'(y_1)} + \inner{x}{T'(y_2)} \\
    &= \inner{x}{aT'(y_1) + T'(y_2)}.
  \end{align*}
  Thus, we can conclude that $T'(ay_1 + y_2) = aT'(y_1) + T'(y_2)$ for any
  $a \in F$ and $y_1, y_2 \in V$ by \Cref{thm:inner-product-basic} (d).

  To show that $T'$ is unique, suppose that $T'': V \to V$ is linear and
  satisfies $\inner{T(x)}{y} = \inner{x}{T''(y)}$ for any $x, y \in V$.
  Then we have
  \begin{equation*}
    \inner{x}{T''(y)} = \inner{T(x)}{y} = \inner{x}{T'(y)},
  \end{equation*}
  implying $T''(y) = T'(y)$ for any $y \in V$ by \Cref{thm:inner-product-basic}
  (d).
  Thus, $T'' = T'$.
\end{proof}

\begin{definition}
  Let $V$ be a finite-dimensional inner product space over
  $F \in \{\mathbb{R}, \mathbb{C}\}$ and let $T: V \to V$ be linear.
  The \emph{adjoint} of $T$, denoted $T^*$, is the linear operator satisfying
  \begin{equation*}
    \inner{T(x)}{y} = \inner{x}{T^*(y)}
  \end{equation*}
  for all $x, y \in V$.
\end{definition}

\begin{theorem}
  Let $V$ be a finite-dimensional inner product space and let $\beta$ be an
  ordered orthonormal basis of $V$.
  If $T: V \to V$ is linear, then
  \begin{equation*}
    [T^*]_\beta^\beta = \Bigl([T]_\beta^\beta\Bigr)^*.
  \end{equation*}
\end{theorem}
\begin{proof}
  Suppose that $\dim(V) = n$ and $\beta = (x_1, x_2, \dots, x_n)$.
  Let $A = [T^*]_\beta^\beta$ and $B = [T]_\beta^\beta$ be $n \times n$
  matrices.
  Then for any $i, j \in \{1, \dots, n\}$, we have
  \begin{equation*}
    A_{ij}
    = \inner{T^*(x_j)}{x_i}
    = \inner{x_j}{T(x_i)}
    = \overline{\inner{T(x_i)}{x_j}}
    = \overline{B_{ji}},
  \end{equation*}
  and thus $A = B^*$.
\end{proof}

\begin{theorem}
  \label{thm:adjoint-properties}
  Let $V$ be a finite-dimensional inner product space over $F$.
  Then the following statements hold for any $a \in F$ and $T_1, T_2, T \in
  \mathcal{L}(V)$.
  \begin{enumerate}
    \item $(a \cdot T_1 + T_2)^* = \overline{a} \cdot T_1^* + T_2^*$.
    \item $(T_1T_2)^* = T_2^*T_1^*$.
    \item $(T^*)^* = T$.
    \item $I_V^* = I_V$.
  \end{enumerate}
\end{theorem}
\begin{proof}
  To be completed.
\end{proof}

\begin{corollary}
  Let $F \in \{\mathbb{R}, \mathbb{C}\}$ be a field.
  Then the following statements are true for any $c \in F$ and
  $A, B \in F^{n \times n}$.
  \begin{enumerate}
    \item $(cA + B)^* = \overline{c}A^* + B^*$.
    \item $(AB)^* = B^*A^*$.
    \item $(A^*)^* = A$.
    \item $I_n^* = I_n$.
  \end{enumerate}
\end{corollary}
\begin{proof}
  Straightforward from \Cref{thm:adjoint-properties}.
\end{proof}

\begin{theorem}
  Let $V$ be a inner product space over $F$ and let $T: V \to V$ be linear.
  Then $\mathcal{R}(T^*)^\perp = \mathcal{N}(T)$.
\end{theorem}
\begin{proof}
  The theorem is proved by
  \begin{align*}
    x \in \mathcal{R}(T^*)^\perp
    \quad &\Leftrightarrow \quad
    \text{$\inner{x}{T^*(y)} = 0_F$ for all $y \in V$} \\
    \quad &\Leftrightarrow \quad
    \text{$\inner{T(x)}{y} = 0_F$ for all $y \in V$} \\
    \quad &\Leftrightarrow \quad
    T(x) = 0_V \\
    \quad &\Leftrightarrow \quad
    x \in \mathcal{N}(T).
    \qedhere
  \end{align*}
\end{proof}

\begin{theorem}
  \label{thm:adjoint-eigenvalue}
  Let $V$ be a finite-dimensional inner product space over $F$ and let
  $T \in \mathcal{L}(V)$.
  Then $\overline{\lambda}$ is an eigenvalue of $T^*$ if and only if $\lambda$
  is an eigenvalue of $T$.
\end{theorem}
\begin{proof}
  The theorem is proved by
  \begin{align*}
    \mathcal{N}(T^* - \overline{\lambda}I_V) = \{0_V\}
    \quad &\Leftrightarrow \quad
    \mathcal{R}(T^* - \overline{\lambda}I_V) = V \\
    \quad &\Leftrightarrow \quad
    \mathcal{R}(T^* - \overline{\lambda}I_V)^\perp = \{0_V\} \\
    \quad &\Leftrightarrow \quad
    \mathcal{N}(T - \lambda I_V) = \{0_V\}.
    \qedhere
  \end{align*}
\end{proof}

\section{Normal and Self-Adjoint Operators}
\begin{definition}
  A polynomial $f$ in $\mathcal{P}(F)$ \emph{splits} if there are
  scalars $c, a_1, \dots, a_n$ in $F$ such that
  \begin{equation*}
    f(t) = c\prod_{i=1}^n (t - a_i).
  \end{equation*}
\end{definition}

\begin{theorem}[Schur's Theorem]
  \label{thm:schur}
  Let $V$ be a finite-dimensional inner product space over
  $F \in \{\mathbb{R}, \mathbb{C}\}$ and let $T: V \to V$ be linear.
  If $f_T$ splits, then there is an orthonormal ordered basis $\beta$ of $V$
  such that $[T]_\beta^\beta$ is upper triangular.
\end{theorem}
\begin{proof}
  The proof is by induction on $n = \dim(V)$.
  The theorem holds trivially for $n = 1$.
  For $n \geq 2$, since $f_T$ splits, $T$ has an eigenvalue, and thus $T^*$
  has an eigenvalue by \Cref{thm:adjoint-eigenvalue}.

  Suppose that $(\lambda, x)$ is an eigenpair of $T^*$ with
  $\Vert x \Vert = 1_F$.
  Let $W = \{x\}^\perp$.
  Then $\dim(W) = n - 1$, and we can conclude that $W$ is $T$-invariant since
  \begin{equation*}
    \inner{x}{T(y)}
    = \inner{T^*(x)}{y}
    = \inner{\lambda x}{y}
    = \lambda \inner{x}{y}
    = 0_F
  \end{equation*}
  for any $y \in W$.
  Define $T': W \to W$ with $T'(y) = T(y)$ for each $y \in W$.
  It follows that $f_{T'}(t) \mid f_T(t)$, and thus $f_{T'}(t)$ splits.
  By induction hypothesis, there is an orthonormal ordered basis
  \begin{equation*}
    \beta' = (x_1, \dots, x_{n-1})
  \end{equation*}
  of $W$ such that $A = [T']_{\beta'}^{\beta'}$
  is upper triangular.
  We can conclude that
  \begin{equation*}
    \beta = (x_1, \dots, x_{n-1}, x)
  \end{equation*}
  is an orthonormal ordered basis of $V$, and it follows that
  $B = [T]_\beta^\beta$ is upper triangular since $B_{ij} = A_{ij}$ for all
  $i, j \in \{1, \dots, n-1\}$, which completes the proof.
\end{proof}

\begin{definition}
  Let $V$ be an inner product space over $F \in \{\mathbb{R}, \mathbb{C}\}$.
  \begin{itemize}
    \item We say that $T \in \mathcal{L}(V)$ is \emph{normal} if $TT^* = T^*T$.
    \item We say that $A \in F^{n \times n}$ is \emph{normal} if $AA^* = A^*A$.
  \end{itemize}
\end{definition}

\begin{theorem}
  Let $V$ be an inner product space over $F \in \{\mathbb{R}, \mathbb{C}\}$,
  and let $T: V \to V$ be normal.
  Then the following statements hold.
  \begin{enumerate}
    \item $\Vert T(x) \Vert = \Vert T^*(x) \Vert$ for any $x \in V$.
    \item $T - cI_V$ is normal for any $c \in F$.
    \item If $(\lambda, x)$ is an eigenpair of $T$, then
    $(\overline\lambda, x)$ is an eigenpair of $T^*$.
    \item If $\lambda_1$ and $\lambda_2$ are distinct eigenvalues of $T$, then
    for any $x \in E_T(\lambda_1)$ and $y \in E_T(\lambda_2)$ we have
    $\inner{x}{y} = 0_F$.
  \end{enumerate}
\end{theorem}
\begin{proof}
  To be completed.
\end{proof}

\begin{theorem}
  Let $V$ be a finite-dimensional inner product space over $\mathbb{C}$ and let
  $T: V \to V$ be linear.
  Then $T$ is normal if and only if there is an orthonormal eigenbasis of $V$
  for $T$.
\end{theorem}
\begin{proof}
  ($\Rightarrow$)
  It can be shown that $f_T(t)$ splits by fundamental theorem of algebra.
  Thus, by Schur's theorem (\Cref{thm:schur}), there is an orthonormal ordered
  basis $\beta = (x_1, \dots, x_n)$ such that $A = [T]_\beta^\beta$ is upper
  triangular.
  By induction on $j \in \{1, \dots, n\}$, we show that $(A_{jj}, x_j)$ is an
  eigenpair of $T$.
  The induction basis with $j = 1$ holds trivially since $A$ is upper
  triangular, implying $T(x_1) = A_{11}x_1$.
  For $j \geq 2$, we have
  \begin{equation*}
    T(x_j) = \sum_{i=1}^j A_{ij}x_i,
  \end{equation*}
  and since
  \begin{equation*}
    A_{ij}
    = \inner{T(x_j)}{x_i}
    = \inner{x_j}{T^*(x_i)}
    = \inner{x_j}{\overline{A_{ii}}x_i}
    = A_{ii}\inner{x_j}{x_i}
    = 0_F
  \end{equation*}
  holds for any $i \in \{1, \dots, j-1\}$, it follows that
  $T(x_j) = A_{jj}x_j$.
  Thus, $x_1, \dots, x_n$ are eigenvectors of $T$, implying $\beta$ is an
  orthonormal eigenbasis of $V$.

  ($\Leftarrow$)
  Suppose that $\beta$ is an orthonormal eigenbasis of $T$.
  Then $[T]_\beta^\beta$ is diagonal, implying
  \begin{equation*}
    [T^*]_\beta^\beta = \left([T]_\beta^\beta\right)^*
  \end{equation*}
  is diagonal.
  It follows that
  \begin{equation*}
    [TT^*]_\beta^\beta
    = [T]_\beta^\beta[T^*]_\beta^\beta
    = [T^*]_\beta^\beta[T]_\beta^\beta
    = [T^*T]_\beta^\beta,
  \end{equation*}
  implying $TT^* = T^*T$.
\end{proof}

\begin{definition}
  Let $V$ be a finite-dimensional inner product space over
  $F \in \{\mathbb{R}, \mathbb{C}\}$.
  \begin{itemize}
    \item We say that $T \in \mathcal{L}(V)$ is \emph{self-adjoint} if
    $T^* = T$.
    \item We say that $A \in F^{n \times n}$ is \emph{self-adjoint} if
    $A^* = A$.
  \end{itemize}
\end{definition}

\begin{theorem}
  Let $V$ be a finite-dimensional inner product space over $\mathbb{R}$ and let
  $T: V \to V$ be linear.
  Then $T$ is self-adjoint if and only if there is an orthonormal eigenbasis of
  $V$ for $T$.
\end{theorem}
\begin{proof}
  To be completed.
\end{proof}