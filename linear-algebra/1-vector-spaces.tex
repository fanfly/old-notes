\chapter{Vector Spaces}
\section{Groups and Fields}
\begin{definition}\label{def:binary-operation}
  A binary operation on a set $G$ is a mapping from $G \times G$ to $G$.
\end{definition}

\begin{definition}\label{def:associativity}
  A binary operation $\star$ on a set $G$ is called \emph{associative} if
  for all $a, b, c \in G$, $(a \star b) \star c = a \star (b \star c)$ holds.
\end{definition}

\begin{definition}\label{def:identity}
  Let $G$ be a set and $\star$ be a binary operation on $G$. An
  \emph{identity} of $G$ with respect to $\star$ is an element $e \in G$ such
  that $a \star e = a$ and $e \star a = a$ for all $a \in G$.
\end{definition}

\begin{theorem}\label{thm:identity-uniqueness}
  The identity of $G$ with respect to $\star$ is unique if it exists.
\end{theorem}
\begin{proof}
  If $e$ and $e'$ are identity of $G$ with respect to $\star$, then
  $e = e \star e' = e'$.
\end{proof}

\begin{notation}
  The identity of $G$ is denoted by $1_G$.
  However, if the binary operation is written additively, the identity is
  denoted by $0_G$ instead.
\end{notation}

\begin{definition}\label{def:inverse}
  Let $\star$ be a binary operation on $G$ with identity $e$. Let $a$ be an
  element of $G$. An element $b \in G$ is called an \emph{inverse} of $a$ if
  $a \star b = e$ and $b \star a = e$.
\end{definition}

\begin{theorem}\label{thm:inverse-uniqueness}
  For all $a \in G$, the inverse of $a \in G$ is unique if it exists.
\end{theorem}
\begin{proof}
  If both $b$ and $b'$ are inverses of $a$, then
  \begin{align*}
  b
  = b \star 1_G
  = b \star (a \star b')
  = (b \star a) \star b'
  = 1_G \star b'
  = b'. &\qedhere
  \end{align*}
\end{proof}

\begin{notation}
  The inverse of $a$ in $G$ is denoted by $a^{-1}$.
  However, if the binary operation is written additively, the inverse of $a$
  is denoted by $-a$ instead.
\end{notation}

\begin{definition}\label{def:group}
  A set $G$ and a binary operation $\star$ on $G$ form a \emph{group}
  $(G, \star)$ if the following conditions hold.
  \begin{enumerate}[label=(G \arabic*),leftmargin=3.5em]
    \item $\star$ is associative.
    \item The identity of $G$ (with respect to $\star$) exists.
    \item For all $a \in G$, the inverse of $a$ (with respect to $\star$)
      exists.
  \end{enumerate}
\end{definition}

\begin{example}
  Let $S$ denote the set of permutations of $\{1, 2, 3\}$ and let $\circ$
  denote the composition of permutations. That is,
  \begin{equation*}
    S = \big\{
      (1)(2)(3), (1)(2\;3), (2)(3\;1), (3)(1\;2), (1\;2\;3), (3\;2\;1) 
    \big\}.
  \end{equation*}
  Then $(S, \circ)$ is a group.
\end{example}

\begin{definition}\label{def:commutativity}
  A binary operation $\star$ on a set $G$ is called \emph{commutative} if
  for all $a, b, \in G$, $a \star b = b \star a$ holds.
\end{definition}

\begin{definition}\label{def:abelian-group}
  A group $(G, \star)$ is called an \emph{Abelian group} if the following
  condition holds.
  \begin{enumerate}[label=(G \arabic*),leftmargin=3.5em,start=4]
    \item $\star$ is commutative.
  \end{enumerate}
\end{definition}

\begin{example}
  $(\mathbb{Z}, +)$ and $(\mathbb{Q} \setminus \{0\}, \cdot)$ are Abelian
  groups.
\end{example}

\begin{theorem}\label{thm:inverse-inverse}
  Let $(G, \star)$ be a group. Then for all $a \in G$, $(a^{-1})^{-1} = a$.
\end{theorem}
\begin{proof}
  Since $a \star a^{-1} = 1_G$, $a$ is the inverse of $a^{-1}$ in $G$.
  Thus, $(a^{-1})^{-1} = a$.
\end{proof}

\begin{theorem}[Cancellation Law]\label{thm:group-cancel}
  Let $(G, \star)$ be a group. Then the following statements are true.
  \begin{enumerate}
    \item For all $a, b, c \in G$, if $c \star a = c \star b$, then $a = b$.
    \item For all $a, b, c \in G$, if $a \star c = b \star c$, then $a = b$.
  \end{enumerate}
\end{theorem}
\begin{proof} \leavevmode
  \begin{enumerate}
    \item We have
      $$
      a = 1_G \star a = (c^{-1} \star c) \star a = c^{-1} \star (c \star a)
      $$
      and
      $$
      b = 1_G \star b = (c^{-1} \star c) \star b = c^{-1} \star (c \star b).
      $$
      Because $c \star a = c \star b$, we have $a = b$.
    \item The proof is similar to (a). \qedhere
  \end{enumerate}
\end{proof}

\begin{definition}
  Let $F$ be a set. Let $+$ and $\cdot$ be binary operations on $F$.
  \begin{itemize}
    \item The operation $\cdot$ is called \emph{left-distributive} over $+$
      if $a \cdot (b + c) = a \cdot b + a \cdot c$ for all $a, b, c \in F$.
    \item The operation $\cdot$ is called \emph{right-distributive} over $+$
      if $(a + b) \cdot c = a \cdot c + b \cdot c$ for all $a, b, c \in F$.
    \item The operation $\cdot$ is called \emph{distributive} over $+$
      if it is both left-distributive and right-distributive.
  \end{itemize}
\end{definition}

\begin{definition}\label{def:field}
  A set $F$ and two binary operations $+$ and $\cdot$ on $F$ form a
  \emph{field} $(F, +, \cdot)$ if the following conditions hold.
  \begin{enumerate}[label=(F \arabic*),leftmargin=3.5em]
    \item $(F, +)$ is an Abelian group.
    \item $(F \setminus \{0_F\}, \cdot)$ is an Abelian group.
    \item The operation $\cdot$ is distributive over $+$.
  \end{enumerate}
\end{definition}

\begin{example}
  $(\mathbb{Q}, +, \cdot)$, $(\mathbb{R}, +, \cdot)$,
  and $(\mathbb{C}, +, \cdot)$ are fields.
\end{example}

\begin{example}
  $(\mathbb{Q}[\sqrt{2}], +, \cdot)$ is a field, where
  $$
  \mathbb{Q}[\sqrt{2}] = \{a + b\sqrt{2} : a, b \in \mathbb{Q}\}.
  $$
\end{example}

\begin{theorem}\label{thm:field-multiplication}
  Let $(F, +, \cdot)$ be a field. Then the following statements are true.
  \begin{enumerate}
    \item For all $a \in F$, $a \cdot 0_F = 0_F = 0_F \cdot a$.
    \item For all $a, b \in F$, $(-a) \cdot b = -(a \cdot b) = a \cdot (-b)$.
    \item For all $a, b \in F$, $(-a) \cdot (-b) = a \cdot b$.
  \end{enumerate}
\end{theorem}
\begin{proof} \leavevmode
  \begin{enumerate}
    \item We have
      $$
      a \cdot 0_F + a \cdot 0_F
      = a \cdot (0_F + 0_F)
      = a \cdot 0_F
      = a \cdot 0_F + 0_F.
      $$
      Thus, $a \cdot 0_F = 0_F$ by cancelltaion law
      (\Cref{thm:group-cancel}). The proof of $0_F \cdot a = 0_F$ is similar.
    \item By (a), we have
      $$
      a \cdot b + (-a) \cdot b
      = (a + (-a)) \cdot b
      = 0_F \cdot b
      = 0_F.
      $$
      Thus, $(-a) \cdot b = -(a \cdot b)$.
      The proof of $a \cdot (-b) = -(a \cdot b)$ is similar.
    \item We have
      $$
      (-a) \cdot (-b) = -(a \cdot (-b)) = -(-(a \cdot b)) = a \cdot b
      $$
      by applying (b) twice. \qedhere
  \end{enumerate}
\end{proof}

\begin{remark}
  Let $G = F \setminus \{0_F\}$ and $1_G$ be the multiplicative identity of
  $G$.
  By \Cref{thm:field-multiplication} (a), we have
  $1_G \cdot 0_F = 0_F = 0_F \cdot 1_G$.
  Therefore, $1_G$ is also the multiplicative identity of $F$, and thus we
  denote it by $1_F$.
\end{remark}

\begin{remark}
  Subtraction and division are defined in terms of addition and
  multiplication by using additive and multiplicative inverses.
\end{remark}

\section{Vector Spaces}
\begin{definition}\label{def:vector-space}
  Let $F$ be a field and let $V$ be a set on which two operations
  $+: V \times V \to V$ and $\cdot: F \times V \to V$ are defined.
  Then $(V, +, \cdot)$ is a \emph{vector space} over
  $F$ if the following conditions hold.
  \begin{enumerate}[label=(V \arabic*),leftmargin=3.5em]
    \item $(V, +)$ is an Abelian group.
    \item For all $x \in V$, $1_F \cdot x = x$.
    \item For all $a, b \in F$ and for all $x \in V$,
      $(a \cdot b) \cdot x = a \cdot (b \cdot x)$.
    \item For all $a, b \in F$ and for all $x \in V$,
      $(a + b) \cdot x = a \cdot x + b \cdot x$.
    \item For all $a \in F$ and for all $x, y \in V$,
      $a \cdot (x + y) = a \cdot x + a \cdot y$.
  \end{enumerate}
\end{definition}
\begin{remark}
  We also say that $V$ is a vector space over $F$ if both $+$ and $\cdot$
  are ``standard''.
\end{remark}

\begin{example}
  $(\mathbb{C}, +, \cdot)$ is a vector space over $\mathbb{R}$, and
  $(\mathbb{R}, +, \cdot)$ is a vector space over $\mathbb{Q}$.
\end{example}
\begin{example} Let $F$ be a field.
  \begin{itemize}
    \item $(F^n, +, \cdot)$ is a vector space over $F$.
    \item Let $\mathcal{P}(F)$ denote the set of polynomials with coefficients
      in $F$.
      Then $(\mathcal{P}(F), +, \cdot)$ is a vector space over $F$.
    \item Let $\mathcal{F}(S, F)$ denote the set of functions from $S$ to $F$.
      Then $(\mathcal{F}(S, F), +, \cdot)$ is a vector space over $F$.
  \end{itemize}
\end{example}

\begin{theorem}\label{thm:vector-space-multiplication}
  Let $(V, +, \cdot)$ be a vector space over $F$.
  Then the following statements are true.
  \begin{enumerate}
    \item For all $x \in V$, $0_F \cdot x = 0_V$.
    \item For all $a \in F$, $a \cdot 0_V = 0_V$.
    \item For all $a \in F$ and $x \in V$,
      $(-a) \cdot x = -(a \cdot x) = a \cdot (-x)$.
  \end{enumerate}
\end{theorem}
\begin{proof}
  It is similar to the proof of \Cref{thm:field-multiplication}.
  \begin{enumerate}
    \item We have
      $$
      0_F \cdot x + 0_F \cdot x
      = (0_F + 0_F) \cdot x
      = 0_F \cdot x
      = 0_F \cdot x + 0_V.
      $$
      Thus, $0_F \cdot x = 0_V$ by cancelltaion law
      (\Cref{thm:group-cancel}).
    \item It is similar to the proof of (a).
    \item By (a), we have
      $$
      a \cdot x + (-a) \cdot x
      = (a + (-a)) \cdot x
      = 0_F \cdot x
      = 0_V.
      $$
      Thus, $(-a) \cdot x = -(a \cdot x)$.
      By (b), we have
      $$
      a \cdot x + a \cdot (-x)
      = a \cdot (x + (-x))
      = a \cdot 0_V
      = 0_V.
      $$
      Thus, $a \cdot (-x) = -(a \cdot x)$. \qedhere
  \end{enumerate}
\end{proof}

\section{Subspaces}
\begin{definition}\label{def:inheritance}
  Let $(V, +_V, \cdot_V)$ be a vector space over a field $F$.
  Let $W$ be a subset of $V$.
  If $+_W: W \times W \to W$ and $\cdot_W: F \times W \to W$
  satisfy
  \begin{equation*}
    x +_W y = x +_V y
      \quad \text{and}
      \quad a \cdot_W x = a \cdot_V x
  \end{equation*}
  for all $a \in F$ and $x, y \in W$, then we say that $+_W$ and $\cdot_W$
  \emph{inherit} $+_V$ and $\cdot_V$, respectively.
\end{definition}

\begin{definition}\label{def:subspace}
  Let $(V, +_V, \cdot_V)$ be a vector space over $F$.
  A subset $W$ of $V$ is called a \emph{subspace} of $V$ if
  $(W, +_W, \cdot_W)$ is a vector space over $F$, where $+_W$ and $\cdot_W$
  inherit $+_V$ and $\cdot_V$, respectively.
\end{definition}

\begin{theorem}\label{thm:subspace}
  Let $(V, +_V, \cdot_V)$ be a vector space over $F$.
  Let $W$ be a subset of $V$.
  Then $W$ is a subspace of $V$ if the following conditions hold.
  \begin{enumerate}
    \item For all $x, y \in W$, $x +_V y \in W$.
    \item For all $a \in F$ and $x \in W$, $a \cdot_V x \in W$.
    \item $0_V \in W$.
  \end{enumerate}
\end{theorem}
\begin{proof}
  We can define operations $+_W: W \times W \to W$
  and $\cdot_W: F \times W \to W$ such that
  \begin{equation*}
    x +_W y = x +_V y
      \quad \text{and}
      \quad a \cdot_W x = a \cdot_V x
  \end{equation*}
  for all $a \in F$ and $x, y \in W$ due to (a) and (b).
  Then $+_W$ and $\cdot_W$ inherit $+_V$ and $\cdot_V$, respectively.

  Now we prove that $(W, +_W, \cdot_W)$ is a vector space over $F$.
  Since a vector in $W$ is also in $V$, (V 2), (V 3), (V 4) and (V 5) hold
  trivially for $W$.
  Thus, one only needs to prove (V 1), i.e., $(W, +_W)$ is an Abelian group.

  Since $+_W$ inherits $+_V$, $+_V$ is associative implies that
  $+_W$ is associative.
  Furthermore, since
  \begin{equation*}
    \begin{array}{lll}
      0_V \in W
        & \text{and}
        & -x = -(1_F \cdot x) = (-1_F) \cdot x \in W
    \end{array}
  \end{equation*}
  hold for all $x \in W$, we have
  \begin{equation*}
    \begin{array}{lll}
      0_V +_W x = x = x +_W 0_V
        & \text{and}
        & x +_W (-x) = 0_V = (-x) +_W x
    \end{array}
  \end{equation*}
  hold for all $x \in W$.
  Thus, $0_V \in W$ is an additive identity of $W$, and each vector in $W$
  also has an additive inverse in $W$, which complete the proof.
\end{proof}

\begin{example}
  Let $\mathcal{P}_n(F)$ denote the set of polynomials in $\mathcal{P}(F)$
  with degree less than or equal to $n$, where $n \geq -1$ is an integer.
  Then it follows from \Cref{thm:subspace} that $\mathcal{P}_n(F)$ is
  a subspace of $\mathcal{P}(F)$.
\end{example}

\begin{theorem}\label{thm:intersection}
  Let $(V, +_V, \cdot_V)$ be a vector space over $F$.
  Let $I$ be an index set such that $W_i$ is a subspace of $V$
  for all $i \in I$.
  Then the intersection
  \begin{equation*}
    W = \bigcap_{i \in I} W_i
  \end{equation*}
  is a subspace of $V$.
\end{theorem}
\begin{proof}
  For all $a \in F$ and for all $x, y \in W$, since
  \begin{equation*}
    \begin{array}{lllll}
      x +_V y \in W_i
      & \text{and}
      & a \cdot_V x \in W_i
      & \text{and}
      & 0_V \in W_i
    \end{array}
  \end{equation*}
  hold for all indices $i \in I$, we have
  \begin{equation*}
    \begin{array}{lllll}
      x +_V y \in W
      & \text{and}
      & a \cdot_V x \in W
      & \text{and}
      & 0_V \in W.
    \end{array}
  \end{equation*}
  Thus, $W$ is a subspace of $V$.
\end{proof}

\begin{definition}\label{def:sum-of-sets}
  Let $(V, +_V, \cdot_V)$ be a vector space over $F$.
  Let $S_1$ and $S_2$ be subsets of $V$.
  Then the \emph{sum} of $S_1$ and $S_2$, denoted $S_1 + S_2$,
  is defined as
  \begin{equation*}
    S_1 + S_2 \; = \; \{x + y: x \in S_1 \;\text{and}\; y \in S_2\}.
  \end{equation*}
\end{definition}

\begin{theorem}\label{thm:sum}
  Let $(V, +_V, \cdot_V)$ be a vector space over $F$.
  If $W_1$ and $W_2$ be subspaces of $V$, then the following statements are
  true.
  \begin{enumerate}
    \item $W_1 + W_2$ is a subspace of $V$.
    \item If $W$ is a subspace of $V$ with $W_1 \cup W_2 \subseteq W$,
      then $W_1 + W_2 \subseteq W$.
  \end{enumerate}
\end{theorem}
\begin{proof} \leavevmode
  \begin{enumerate}
    \item Suppose that $a \in F$ and $x, y \in W_1 + W_2$.
      Then there exists $x_1, y_1 \in W_1$ and $x_2, y_2 \in W_2$ such that
      \begin{equation*}
        \begin{array}{lll}
        x = x_1 +_V x_2
          & \text{and}
          & y = y_1 +_V y_2.
        \end{array}
      \end{equation*}
      Thus,
      \begin{equation*}
        a \cdot_V x
          = a \cdot_V (x_1 + x_2)
          = a \cdot_V x_1 + a \cdot_V x_2
          \in W_1 + W_2
      \end{equation*}
      and
      \begin{equation*}
        x +_V y
          = (x_1 +_V x_2) + (y_1 +_V y_2)
          = (x_1 +_V y_1) + (x_2 +_V y_2)
          \in W_1 + W_2.
      \end{equation*}
      We also have $0_V = 0_V +_V 0_V \in W_1 + W_2$.
      Hence, $W_1 + W_2$ is a subspace of $V$.
    \item If $x \in W_1 + W_2$, then there exists $x_1 \in W_1$ and
      $x_2 \in W_2$ such that $x = x_1 + x_2$.
      Since $W_1 \subseteq W$ and $W_2 \subseteq W$, we have $x_1 \in W$ and
      $x_2 \in W$, which implies $x \in W$. \qedhere
  \end{enumerate}
\end{proof}

\section{Spanning Sets}
\begin{definition}\label{def:summation}
  Let $(G, +)$ be an Abelian group.
  Then we define
  \begin{equation*}
    \sum_{i=m}^n a_i =
      \left\{
      \begin{array}{ll}
        \displaystyle
          \sum_{i=m}^{n-1} a_i + a_n & \text{if} \; m \leq n\\[1.5em]
        0_G & \text{if} \; m > n,\\
      \end{array}
      \right.
  \end{equation*}
  where $a_i \in G$ for each integer $i$ with $m \leq i \leq n$.
\end{definition}

\begin{definition}\label{def:linear-combination}
  Let $(V, +, \cdot)$ be a vector space over $F$.
  Let $S$ be a subset of $V$.
  Then a vector $x \in V$ is called a \emph{linear combination} of $S$ if
  there exist some nonnegative integer $n$,
  scalars $a_1, \dots, a_n \in F$,
  and vectors $x_1, \dots, x_n \in S$ such that
  \begin{equation*}
    x = \sum_{i=1}^n a_ix_i.
  \end{equation*}
\end{definition}
\begin{remark}
  Since $n$ can be zero, $0_V$ is a linear combination for all $S \subseteq V$.
\end{remark}
\begin{remark}
  Although $S$ can be infinite, the number of terms in the summation must be
  finite.
  For example, in the vector space $\mathbb{R}$ over $\mathbb{Q}$,
  although we have
  \begin{equation*}
    e = \sum_{k=0}^\infty \frac{1}{k!}
      = \frac{1}{1} + \frac{1}{1} + \frac{1}{2} + \frac{1}{6}
        + \frac{1}{24} + \cdots,
  \end{equation*}
  $e$ is still not a linear combination of $\mathbb{Q}$.
\end{remark}

\begin{definition}\label{def:span}
  Let $(V, +, \cdot)$ is a vector space over $F$.
  The \emph{span} of $S$, denoted $\mathrm{span}(S)$, is the set that
  consists of all linear combinations of $S$.
\end{definition}

\begin{theorem}\label{thm:span}
  Let $(V, +, \cdot)$ be a vector space over $F$.
  Let $S \subseteq V$. Then the following statements are true.
  \begin{enumerate}
    \item $\mathrm{span}(S)$ is a subspace of $V$.
    \item If $W$ is a subspace of $V$ such that $S \subseteq W$, then
      $\mathrm{span}(S) \subseteq W$.
  \end{enumerate}
\end{theorem}
\begin{proof} \leavevmode
\begin{enumerate}
  \item If $c \in F$ and $x, y \in \mathrm{span}(S)$, then there exist
    nonnegative integers $m, n$,
    scalars $a_1, \dots, a_m, b_1, \dots, b_n \in F$
    and vectors $x_1, \dots, x_m, y_1, \dots, y_n \in S$ such that
    \begin{equation*}
      \begin{array}{lll}
        \displaystyle x = \sum_{i=1}^m a_ix_i
        & \text{and}
        & \displaystyle y = \sum_{j=1}^n b_jy_j.
      \end{array}
    \end{equation*}
    Thus, we have
    \begin{align*}
      cx
      &= c(a_1x_1 + \cdots + a_mx_m) \\
      &= c(a_1x_1) + \cdots + c(a_mx_m) \\
      &= (ca_1)x_1 + \cdots + (ca_m)x_m \in \mathrm{span}(S)
    \end{align*}
    and
    \begin{equation*}
      x + y
        = a_1x_1 + \cdots a_mx_m + b_1y_1 + \cdots + b_ny_n
        \in \mathrm{span}(S).
    \end{equation*}
    Also, $0_V \in \mathrm{span}(S)$.
    Hence, $\mathrm{span}(S)$ is a subspace of $V$.
  \item If $x \in \mathrm{span}(S)$, then there exists an
    nonnegative integer $n$, scalars $a_1, \dots, a_n \in F$
    and vectors $x_1, \dots, x_n \in S$ such that
    \begin{equation*}
      x = \sum_{i=1}^m a_ix_i.
    \end{equation*}
    Thus, since $x_1, \dots, x_n \in W$, we have
    $x = a_1x_1 + \cdots + a_nx_n \in W$. \qedhere
\end{enumerate}
\end{proof}

\begin{definition}\label{def:spanning-set}
  A subset $S$ of a vector space $(V, +, \cdot)$ \emph{spans} $V$
  if $\mathrm{span}(S) = V$.
  In this case, we also say that $S$ is a \emph{spanning set} of $V$.
\end{definition}

\begin{example}
  $\{(0, 1, 1), (1, 0, 1), (1, 1, 0)\}$ is a spanning set of $\mathbb{R}^3$
  since for all $x, y, z \in \mathbb{R}$,
  \begin{equation*}
    (x, y, z)
      = \frac{-x+y+z}{2} \cdot (0, 1, 1)
      + \frac{x-y+z}{2} \cdot (1, 0, 1)
      + \frac{x+y-z}{2} \cdot (1, 1, 0).
  \end{equation*}
\end{example}

\section{Linearly Independent Sets}
\begin{definition}
  Let $(V, +, \cdot)$ be a vector space over $F$.
  Let $S$ be a subset of $V$.
  For scalars $a_1, \dots, a_n \in F$ and distinct vectors
  $x_1, \dots, x_n \in S$, we say that
  \begin{equation*}
    \sum_{i=1}^n a_ix_i = 0_V
  \end{equation*}
  is a \emph{trivial representation} of $0_V$ as a linear combination of $S$
  if $a_1 = \cdots = a_n = 0_F$.
\end{definition}

\begin{definition}
  Let $(V, +, \cdot)$ be a vector space over $F$.
  \begin{itemize}
    \item A subset $S$ of $V$ is called \emph{linearly dependent} if there
      exists a nontrivial representation of $0_V$ as a linear combination of
      $S$.
    \item A subset $S$ of $V$ is called \emph{linearly independent} if
      it is not linear dependent.
  \end{itemize}
\end{definition}

\begin{theorem}\label{thm:linear-dependence-first-equivalence}
  Let $(V, +, \cdot)$ be a vector space over $F$ and let $S \subseteq V$.
  Then $S$ is linearly independent if and only if there exists $x \in S$
  such that $x \in \mathrm{span}(S \setminus \{x\})$.
\end{theorem}
\begin{proof}
  ($\Rightarrow$) Because $S$ is linearly dependent, it follows that
  there exists a nontrivial representation
  \begin{equation*}
    a_1x_1 + a_2x_2 + \cdots + a_nx_n = 0_V
  \end{equation*}
  as a linear combination of $S$, where $a_1, \dots, a_n \in F$ are scalars
  and $x_1, \dots, x_n \in S$ are distinct vectors.
  Without loss of generality, let $a_1 \neq 0_F$.
  Then we have
  \begin{align*}
    x_1
    &= (-a_1)^{-1}(a_2x_2 + \cdots + a_nx_n) \\
    &= (-a_1)^{-1}a_2x_2 + \cdots + (-a_1)^{-1}a_nx_n \\
    &\in \mathrm{span}(S \setminus \{x_1\}).
  \end{align*}
  
  ($\Leftarrow$) Since $x \in \mathrm{span}(S \setminus \{x\})$, there exists
  scalars $a_1, \dots, a_n \in F$ and distinct vectors
  $x_1, \dots, x_n \in S \setminus \{x\}$ such that
  \begin{equation*}
    a_1x_1 + \cdots + a_nx_n = x.
  \end{equation*}
  Then
  \begin{equation*}
    (-1_F)x + a_1x_1 + \cdots + a_nx_n = 0_V
  \end{equation*}
  is a nontrivial representation of $0_V$ as a linear combination of $S$.
\end{proof}

\begin{theorem}\label{thm:linear-dependence-second-equivalence}
  Let $(V, +, \cdot)$ be a vector space over $F$.
  Let $S$ be a subset of $V$ and let $x$ be an element of $S$.
  Then $x \in \mathrm{span}(S \setminus \{x\})$ if and only if
  $\mathrm{span}(S) = \mathrm{span}(S \setminus \{x\})$.
\end{theorem}
\begin{proof}
  ($\Rightarrow$) Since $x \in \mathrm{span}(S \setminus \{x\})$
  and $S \setminus \{x\} \subseteq \mathrm{span}(S \setminus \{x\})$,
  we have
  \begin{equation*}
    S \subseteq \mathrm{span}(S \setminus \{x\})
    \quad \Rightarrow \quad
    \mathrm{span}(S) \subseteq \mathrm{span}(S \setminus \{x\})
  \end{equation*}
  by
  \Cref{thm:span}.
  Also, $\mathrm{span}(S \setminus \{x\}) \subseteq \mathrm{span}(S)$
  because $S \setminus \{x\} \subseteq S$.
  Thus, we can conclude that
  $\mathrm{span}(S \setminus \{x\}) = \mathrm{span}(S)$.

  ($\Leftarrow$) Since
  $x \in S \subseteq \mathrm{span}(S) = \mathrm{span}(S \setminus \{x\})$,
  we have $x \in \mathrm{span}(S \setminus \{x\})$.
\end{proof}

\begin{example}
  Let $S = \{1, 1+2x, 1+2x+3x^2, 1+2x+3x^2+4x^3\}$ be a subset of
  $\mathcal{P}_3(\mathbb{R})$.
  Then $S$ is linearly independent since the only solution to the
  following system of linear equations
  \begin{equation*}
    \setlength\arraycolsep{0pt}
    \begin{array}{r>{{}}c<{{}}r>{{}}c<{{}}r>{{}}c<{{}}r@{{}={}}r}
      a_1 & &      & &      & &      & 0 \\
      a_1 &+& 2a_2 & &      & &      & 0 \\
      a_1 &+& 2a_2 &+& 3a_3 & &      & 0 \\
      a_1 &+& 2a_2 &+& 3a_3 &+& 4a_4 & 0 \\
    \end{array}
  \end{equation*}
  is $a_1 = a_2 = a_3 = a_4 = 0$.
\end{example}

\begin{theorem}\label{thm:linear-independence-implication}
  Let $(V, +, \cdot)$ be a vector space, and let $R \subseteq S \subseteq V$.
  If $R$ is linearly dependent, then $S$ is linearly dependent.
\end{theorem}
\begin{proof}
  If $R$ is linearly dependent, then there exists $x \in R$ such that
  $x \in \mathrm{span}(R \setminus \{x\})$.
  By $R \subseteq S$, we have
  $R \setminus \{x\} \subseteq S \setminus \{x\}$.
  Since $x \in S$ and $x \in \mathrm{span}(S \setminus \{x\})$, $S$ is
  linearly dependent.
\end{proof}

\begin{corollary}
  Let $(V, +, \cdot)$ be a vector space, and let $R \subseteq S \subseteq V$.
  If $S$ is linearly independent, then $R$ is linearly independent.
\end{corollary}
\begin{proof}
  Suppose that $S$ is linearly independent.
  If $R$ is linearly dependent, then so is $S$ by
  \Cref{thm:linear-independence-implication}, contradiction.
  Thus, $R$ is linearly independent.
\end{proof}

\begin{theorem}\label{thm:linearly-independent-subset}
  Let $(V, +, \cdot)$ be a vector space.
  For each finite set $S \subseteq V$, there exists a linearly independent
  set $Q \subseteq S$ such that $\mathrm{span}(Q) = \mathrm{span}(S)$.
\end{theorem}
\begin{proof}
  The proof is by induction on $n = |S|$.
  The induction begins with $n = 0$, i.e., $S = \varnothing$.
  Since $\varnothing$ is linearly independent, we can choose
  $R = \varnothing$, and thus the theorem holds.

  Now suppose that the theorem is true for some integer $n \geq 0$,
  and we prove that the theorem holds for $n + 1$.
  If $S$ is linearly independent, then we can choose $Q = S$.
  Otherwise, there exists $x \in S$
  with $\mathrm{span}(S \setminus \{x\}) = \mathrm{span}(S)$ because
  $S$ is linearly dependent.
  Let $S' = S \setminus \{x\}$. Then there exists a linearly independent set
  $Q \subseteq S'$ such that $\mathrm{span}(Q) = \mathrm{span}(S')$ by
  induction hypothesis, implying
  $Q \subseteq S$ and $\mathrm{span}(Q) = \mathrm{span}(S)$.
\end{proof}

\section{Bases and Dimension}
\begin{definition}\label{def:basis}
  Let $(V, +, \cdot)$ be a vector space. A subset $S$ of $V$ is a \emph{basis}
  of $V$ if $S$ is not only a spanning set but also a linearly independent set
  of $V$.
\end{definition}

\begin{example}
  Since $\mathrm{span}(\varnothing) = \{0_V\}$ and $\varnothing$ is linearly
  independent, $\varnothing$ is a basis of $\{0_V\}$.
\end{example}
\begin{example}
  Let $S = \{x_1, \dots, x_n\}$ be a subset of $F^n$ with
  $(x_i)_j = [\![i = j]\!]$ for all $i, j \in \{1, \dots, n\}$.
  Then $S$ is called the \emph{standard basis} of $F^n$.
\end{example}
\begin{example}
  The set $S = \{1_F, x, x^2, \dots, x^n\}$ is the called the
  \emph{standard basis} of $\mathcal{P}_n(F)$.
\end{example}

\begin{theorem}\label{thm:finite-basis-existence}
  Let $(V, +, \cdot)$ be a vector space over $F$.
  If there exists a finite set $S$ that spans $V$,
  then $V$ has a finite basis.
\end{theorem}
\begin{proof}
  By \Cref{thm:linearly-independent-subset}, there exists
  a linearly independent set $Q \subseteq S$
  such that $\mathrm{span}(Q) = \mathrm{span}(S) = V$.
  Thus, $Q$ is a finite basis of $V$.
\end{proof}

\begin{theorem}[Replacement Theorem]\label{thm:replacement}
  Let $(V, +, \cdot)$ be a vector space over $F$.
  Let $S$ be a finite set that spans $V$,
  and let $Q \subseteq V$ be a finite linearly independent set.
  Then $|Q| \leq |S|$, and there exists $R \subseteq S \setminus Q$ such that
  both $|Q \cup R| = |S|$ and $\mathrm{span}(Q \cup R) = V$ hold.
\end{theorem}
\begin{proof}
  The proof is based on induction on $|Q|$.
  The induction begins with $|Q| = 0$, i.e., $Q = \varnothing$.
  Choosing $R = S$, we have $Q \cup R = S$, and thus
  both $|Q \cup R| = S$ and $\mathrm{span}(Q \cup R) = V$ hold.

  Now suppose that the theorem is true for $|Q| = m$ with $m \geq 0$,
  and we prove that the theorem holds for $|Q|= m + 1$.
  Let $Q = \{x_1, \dots, x_{m+1}\}$ and let $Q' = Q \setminus \{x_{m+1}\}$.
  By induction hypothesis, there exists
  $R' = \{y_1, \dots, y_k\} \subseteq S \setminus Q'$
  such that $m + k = |S|$ and $\mathrm{span}(Q' \cup R') = V$.
  Since $Q' \cup R'$ spans $V$, there exists
  $a_1, \dots, a_m, b_1, \dots, b_k \in F$ such that
  \begin{equation*}
    x_{m+1} = \sum_{i=1}^m a_ix_i + \sum_{j=1}^k b_jy_j.
  \end{equation*}
  If $b_j = 0_F$ for all $j \in \{1, \dots, k\}$, then $x_{m+1}$ is a linear
  combination of $Q$, implying that $Q$ is linearly dependent, contradiction.
  Thus, there must exist some $j \in \{1, \dots, k\}$
  such that $b_j \neq 0_F$.
  Without loss of generality let $b_k \neq 0_F$.
  Also, let $R = \{y_1, \dots, y_{k-1}\}$.
  Then $|Q \cup R| = (m+1) + (k-1) = |S|$.
  Since $k \geq 1$, we have $|Q| \leq |S|$.
  Note that $(Q' \cup R') \setminus (Q \cup R) = \{y_k\}$.
  By
  \begin{equation*}
    y_k = (-b_k)^{-1}\left(
       \sum_{i=1}^m a_ix_i + (-1_F)x_{m+1} + \sum_{j=1}^{k-1} b_jy_j
    \right) \in \mathrm{span}(Q \cup R),
  \end{equation*}
  we have
  \begin{equation*}
    Q' \cup R'
      \subseteq Q \cup R \cup \{y_k\}
      \subseteq \mathrm{span}(Q \cup R).
  \end{equation*}
  Thus, by \Cref{thm:span} we have
  \begin{equation*}
    V = \mathrm{span}(Q' \cup R')
      \subseteq \mathrm{span}(Q \cup R)
      \subseteq V,
  \end{equation*}
  implying $\mathrm{span}(Q \cup R) = V$.
\end{proof}

\begin{corollary}
  Let $(V, +, \cdot)$ be a vector space over $F$ that is spanned by a finite
  set. Then every linearly independent subset of $V$ is finite.
\end{corollary}
\begin{proof}
  Suppose that $S$ is a finite spanning set of $V$ and that $Q$ is linearly
  independent.
  If $Q$ is infinite, then there exists $Q' \subseteq Q$ with
  $|Q'| = |S| + 1$.
  It follows that $Q'$ is linearly independent by
  \Cref{thm:linear-independence-implication}, and thus $|Q'| \leq |S|$ by
  \Cref{thm:replacement}, contradiction to $|Q'| = |S| + 1$.
  Therefore, $Q$ is finite.
\end{proof}