\chapter{Vector Spaces}
\section{Fields}
\begin{definition}\label{def:field}
  A \emph{field} is a set $F$ with two operations, called \emph{addition}
  (denoted by $+$) and \emph{multiplication} (denoted by $\cdot$), which
  satisfy the following axioms.
  \begin{enumerate}[leftmargin=3.5em]
    \item[(A 1)] If $a \in F$ and $b \in F$, then $a + b \in F$.
    \item[(M 1)] If $a \in F$ and $b \in F$, then $a \cdot b \in F$.
    \item[(A 2)] $a + b = b + a$ for all $a, b \in F$.
    \item[(M 2)] $a \cdot b = b \cdot a$ for all $a, b \in F$.
    \item[(A 3)] $(a + b) + c = a + (b + c)$ for all $a, b, c \in F$.
    \item[(M 3)] $(a \cdot b) + c = a + (b \cdot c)$ for all $a, b, c \in F$.
    \item[(A 4)] There is an element $0_F$ in $F$ such that $0_F + a = a$ for
      all $a \in F$.
    \item[(M 4)] There is an element $1_F$ in $F \setminus \{0_F\}$ such that
      $1_F \cdot a = a$ for all $a \in F$.
    \item[(A 5)] For each $a \in F$ there is an element $-a$ in $F$ such that
      $a + (-a) = 0_F$.
    \item[(M 5)] For each $a \in F \setminus \{0_F\}$ there is an element
      $a^{-1}$ in $F$ such that $a \cdot a^{-1} = 1_F$.
    \item[(D)] $a \cdot (b + c) = a \cdot b + a \cdot c$ for all
      $a, b, c \in F$.
  \end{enumerate}
\end{definition}
\begin{remark} \leavevmode
  \begin{itemize}
    \item For simplification, we usually write $ab$ instead of $a \cdot b$.
    \item The axioms labeled with ``A'' and ``M'' are usually called the
      \emph{axioms of addition} and the \emph{axioms of multiplication},
      respectively. The axiom labeld with ``D'' is the \emph{distributive law}.
    \item The elements $0_F$ and $1_F$ are usually called the
      \emph{additive identity} and the \emph{multiplicative identity} of $F$,
      respectively.
      Also, $-a$ and $a^{-1}$ are called the \emph{additive inverse} and the
      \emph{multiplicative inverse} of $a$, respectively.
    \item \emph{Subtraction} and \emph{division} can be defined using additive
      and multiplicative inverses.
  \end{itemize}
\end{remark}

\begin{example}
  $\mathbb{Q}$, $\mathbb{R}$ and $\mathbb{C}$ are fields.
\end{example}

\begin{example}
  Let $\mathbb{B} = \{0, 1\}$ and the operations $\oplus$ and $\odot$ are
  defined as follows.
  \begin{align*}
    \begin{array}{c|cc}
      \oplus & 0 & 1 \\
      \hline
      0      & 0 & 1 \\
      1      & 1 & 0 \\
    \end{array}
    \quad
    \begin{array}{c|cc}
      \odot & 0 & 1 \\
      \hline
      0     & 0 & 0 \\
      1     & 0 & 1 \\
    \end{array}
  \end{align*}
  Then $\mathbb{B}$ is a field with $\oplus$ and $\odot$ as addition and
  multiplication, respectively.
\end{example}

\begin{remark}
  In a field $F$, if one can add up finite $1_F$'s such that the sum equals to
  $0_F$, then the smallest number of summands is called the
  \emph{characteristic} of $F$.
  Thus, the characteristic of $\mathbb{B}$ is $2$ since $1 \oplus 1 = 0$ in
  $\mathbb{B}$.
\end{remark}

\begin{proposition}\label{prop:cancelltaion-of-addition}
  Let $F$ be a field with $a, b, c \in F$.
  \begin{enumerate}
    \item If $a + b = a + c$, then $b = c$.
    \item If $a + b = a$, then $b = 0_F$.
    \item If $a + b = 0_F$, then $b = -a$.
    \item $-(-a) = a$.
  \end{enumerate}
\end{proposition}
\begin{proof} \leavevmode
  \begin{enumerate}
    \item It can be proved by
      \begin{align*}
        b
        &= 0_F + b      \\
        &= (-a + a) + b \\
        &= -a + (a + b) \\
        &= -a + (a + c) \\
        &= (-a + a) + c \\
        &= 0_F + c      \\
        &= c.
      \end{align*}
    \item By applying (a), it follows from $a + b = a + 0_F$ that $b = 0_F$.
    \item By applying (a), it follows from $a + b = a + (-a)$ that $b = -a$.
    \item Since $-a + a = 0_F$, we have $a = -(-a)$ by (c). \qedhere
  \end{enumerate}
\end{proof}

\begin{proposition}\label{prop:cancelltaion-of-multiplication}
  Let $F$ be a field with $a, b, c \in F$ and $a \neq 0_F$.
  \begin{enumerate}
    \item If $a \cdot b = a \cdot c$, then $b = c$.
    \item If $a \cdot b = a$, then $b = 1_F$.
    \item If $a \cdot b = 1_F$, then $b = a^{-1}$.
    \item $(a^{-1})^{-1} = a$.
  \end{enumerate}
\end{proposition}
\begin{proof}
  The proof is similar to \Cref{prop:cancelltaion-of-addition}.
\end{proof}

\begin{proposition}\label{prop:multiplication-of-inverses}
  Let $F$ be a field with $a, b \in F$.
  \begin{enumerate}
    \item $0_F \cdot a = 0_F$.
    \item $(-a) \cdot b = -(a \cdot b) = a \cdot (-b)$.
    \item $(-a) \cdot (-b) = a \cdot b$.
  \end{enumerate}
\end{proposition}
\begin{proof} \leavevmode
  \begin{enumerate}
    \item Since
      \begin{equation*}
        0_F \cdot a + 0_F \cdot a
        = (0_F + 0_F) \cdot a
        = 0_F \cdot a,
      \end{equation*}
      we have $0_F \cdot a = 0_F$ by \Cref{prop:cancelltaion-of-addition} (b).
    \item Since
      \begin{equation*}
        (-a) \cdot b + a \cdot b
        = (-a + a) \cdot b
        = 0_F \cdot b
        = 0_F,
      \end{equation*}
      we have $(-a) \cdot b = -(a \cdot b)$ by
      \Cref{prop:cancelltaion-of-addition} (c).
      The other half can be proved similarly.
    \item By applying (b) twice, we have
      \begin{equation*}
        (-a) \cdot (-b)
        = -(a \cdot (-b))
        = -(-(a \cdot b))
        = a \cdot b. \qedhere
      \end{equation*}
  \end{enumerate}
\end{proof}

\section{Vector Spaces}
\begin{definition}\label{def:vector-space}
  Let $F$ be a field and let $V$ be a set on which two operations
  $+: V \times V \to V$ and $\cdot: F \times V \to V$ are defined.
  Then $(V, +, \cdot)$ is a \emph{vector space} over
  $F$ if the following conditions hold.
  \begin{enumerate}[label=(V \arabic*),leftmargin=3.5em]
    \item $(V, +)$ is an Abelian group.
    \item For all $x \in V$, $1_F \cdot x = x$.
    \item For all $a, b \in F$ and for all $x \in V$,
      $(a \cdot b) \cdot x = a \cdot (b \cdot x)$.
    \item For all $a, b \in F$ and for all $x \in V$,
      $(a + b) \cdot x = a \cdot x + b \cdot x$.
    \item For all $a \in F$ and for all $x, y \in V$,
      $a \cdot (x + y) = a \cdot x + a \cdot y$.
  \end{enumerate}
\end{definition}
\begin{remark}
  We also say that $V$ is a vector space over $F$ if both $+$ and $\cdot$
  are ``standard''.
\end{remark}

\begin{example}
  $(\mathbb{C}, +, \cdot)$ is a vector space over $\mathbb{R}$, and
  $(\mathbb{R}, +, \cdot)$ is a vector space over $\mathbb{Q}$.
\end{example}
\begin{example} Let $F$ be a field.
  \begin{itemize}
    \item $(F^n, +, \cdot)$ is a vector space over $F$.
    \item Let $\mathcal{P}(F)$ denote the set of polynomials with coefficients
      in $F$.
      Then $(\mathcal{P}(F), +, \cdot)$ is a vector space over $F$.
    \item Let $\mathcal{F}(S, F)$ denote the set of functions from $S$ to $F$.
      Then $(\mathcal{F}(S, F), +, \cdot)$ is a vector space over $F$.
  \end{itemize}
\end{example}

\begin{theorem}\label{thm:vector-space-multiplication}
  Let $(V, +, \cdot)$ be a vector space over $F$.
  Then the following statements are true.
  \begin{enumerate}
    \item For all $x \in V$, $0_F \cdot x = 0_V$.
    \item For all $a \in F$, $a \cdot 0_V = 0_V$.
    \item For all $a \in F$ and $x \in V$,
      $(-a) \cdot x = -(a \cdot x) = a \cdot (-x)$.
  \end{enumerate}
\end{theorem}
\begin{proof}
  It is similar to the proof of \Cref{prop:cancelltaion-of-multiplication}.
  \begin{enumerate}
    \item We have
      $$
      0_F \cdot x + 0_F \cdot x
      = (0_F + 0_F) \cdot x
      = 0_F \cdot x
      = 0_F \cdot x + 0_V.
      $$
      Thus, $0_F \cdot x = 0_V$ by \Cref{prop:cancelltaion-of-addition}.
    \item It is similar to the proof of (a).
    \item By (a), we have
      $$
      a \cdot x + (-a) \cdot x
      = (a + (-a)) \cdot x
      = 0_F \cdot x
      = 0_V.
      $$
      Thus, $(-a) \cdot x = -(a \cdot x)$.
      By (b), we have
      $$
      a \cdot x + a \cdot (-x)
      = a \cdot (x + (-x))
      = a \cdot 0_V
      = 0_V.
      $$
      Thus, $a \cdot (-x) = -(a \cdot x)$. \qedhere
  \end{enumerate}
\end{proof}

\section{Subspaces}
\begin{definition}\label{def:inheritance}
  Let $(V, +_V, \cdot_V)$ be a vector space over a field $F$.
  Let $W$ be a subset of $V$.
  If $+_W: W \times W \to W$ and $\cdot_W: F \times W \to W$
  satisfy
  \begin{equation*}
    x +_W y = x +_V y
      \quad \text{and}
      \quad a \cdot_W x = a \cdot_V x
  \end{equation*}
  for all $a \in F$ and $x, y \in W$, then we say that $+_W$ and $\cdot_W$
  \emph{inherit} $+_V$ and $\cdot_V$, respectively.
\end{definition}

\begin{definition}\label{def:subspace}
  Let $(V, +_V, \cdot_V)$ be a vector space over $F$.
  A subset $W$ of $V$ is called a \emph{subspace} of $V$ if
  $(W, +_W, \cdot_W)$ is a vector space over $F$, where $+_W$ and $\cdot_W$
  inherit $+_V$ and $\cdot_V$, respectively.
\end{definition}

\begin{theorem}\label{thm:subspace}
  Let $(V, +_V, \cdot_V)$ be a vector space over $F$.
  Let $W$ be a subset of $V$.
  Then $W$ is a subspace of $V$ if the following conditions hold.
  \begin{enumerate}
    \item For all $x, y \in W$, $x +_V y \in W$.
    \item For all $a \in F$ and $x \in W$, $a \cdot_V x \in W$.
    \item $0_V \in W$.
  \end{enumerate}
\end{theorem}
\begin{proof}
  We can define operations $+_W: W \times W \to W$
  and $\cdot_W: F \times W \to W$ such that
  \begin{equation*}
    x +_W y = x +_V y
      \quad \text{and}
      \quad a \cdot_W x = a \cdot_V x
  \end{equation*}
  for all $a \in F$ and $x, y \in W$ due to (a) and (b).
  Then $+_W$ and $\cdot_W$ inherit $+_V$ and $\cdot_V$, respectively.

  Now we prove that $(W, +_W, \cdot_W)$ is a vector space over $F$.
  Since a vector in $W$ is also in $V$, (V 2), (V 3), (V 4) and (V 5) hold
  trivially for $W$.
  Thus, one only needs to prove (V 1), i.e., $(W, +_W)$ is an Abelian group.

  Since $+_W$ inherits $+_V$, $+_V$ is associative implies that
  $+_W$ is associative.
  Furthermore, since
  \begin{equation*}
    \begin{array}{lll}
      0_V \in W
        & \text{and}
        & -x = -(1_F \cdot x) = (-1_F) \cdot x \in W
    \end{array}
  \end{equation*}
  hold for all $x \in W$, we have
  \begin{equation*}
    \begin{array}{lll}
      0_V +_W x = x = x +_W 0_V
        & \text{and}
        & x +_W (-x) = 0_V = (-x) +_W x
    \end{array}
  \end{equation*}
  hold for all $x \in W$.
  Thus, $0_V \in W$ is an additive identity of $W$, and each vector in $W$
  also has an additive inverse in $W$, which complete the proof.
\end{proof}

\begin{example}
  Let $\mathcal{P}_n(F)$ denote the set of polynomials in $\mathcal{P}(F)$
  with degree less than or equal to $n$, where $n \geq -1$ is an integer.
  Then it follows from \Cref{thm:subspace} that $\mathcal{P}_n(F)$ is
  a subspace of $\mathcal{P}(F)$.
\end{example}

\begin{theorem}\label{thm:intersection}
  Let $(V, +_V, \cdot_V)$ be a vector space over $F$.
  Let $I$ be an index set such that $W_i$ is a subspace of $V$
  for all $i \in I$.
  Then the intersection
  \begin{equation*}
    W = \bigcap_{i \in I} W_i
  \end{equation*}
  is a subspace of $V$.
\end{theorem}
\begin{proof}
  For all $a \in F$ and for all $x, y \in W$, since
  \begin{equation*}
    \begin{array}{lllll}
      x +_V y \in W_i
      & \text{and}
      & a \cdot_V x \in W_i
      & \text{and}
      & 0_V \in W_i
    \end{array}
  \end{equation*}
  hold for all indices $i \in I$, we have
  \begin{equation*}
    \begin{array}{lllll}
      x +_V y \in W
      & \text{and}
      & a \cdot_V x \in W
      & \text{and}
      & 0_V \in W.
    \end{array}
  \end{equation*}
  Thus, $W$ is a subspace of $V$.
\end{proof}

\begin{definition}\label{def:sum-of-sets}
  Let $(V, +_V, \cdot_V)$ be a vector space over $F$.
  Let $S_1$ and $S_2$ be subsets of $V$.
  Then the \emph{sum} of $S_1$ and $S_2$, denoted $S_1 + S_2$,
  is defined as
  \begin{equation*}
    S_1 + S_2 \; = \; \{x + y: x \in S_1 \;\text{and}\; y \in S_2\}.
  \end{equation*}
\end{definition}

\begin{theorem}\label{thm:sum}
  Let $(V, +_V, \cdot_V)$ be a vector space over $F$.
  If $W_1$ and $W_2$ be subspaces of $V$, then the following statements are
  true.
  \begin{enumerate}
    \item $W_1 + W_2$ is a subspace of $V$.
    \item If $W$ is a subspace of $V$ with $W_1 \cup W_2 \subseteq W$,
      then $W_1 + W_2 \subseteq W$.
  \end{enumerate}
\end{theorem}
\begin{proof} \leavevmode
  \begin{enumerate}
    \item Suppose that $a \in F$ and $x, y \in W_1 + W_2$.
      Then there exists $x_1, y_1 \in W_1$ and $x_2, y_2 \in W_2$ such that
      \begin{equation*}
        \begin{array}{lll}
        x = x_1 +_V x_2
          & \text{and}
          & y = y_1 +_V y_2.
        \end{array}
      \end{equation*}
      Thus,
      \begin{equation*}
        a \cdot_V x
          = a \cdot_V (x_1 + x_2)
          = a \cdot_V x_1 + a \cdot_V x_2
          \in W_1 + W_2
      \end{equation*}
      and
      \begin{equation*}
        x +_V y
          = (x_1 +_V x_2) + (y_1 +_V y_2)
          = (x_1 +_V y_1) + (x_2 +_V y_2)
          \in W_1 + W_2.
      \end{equation*}
      We also have $0_V = 0_V +_V 0_V \in W_1 + W_2$.
      Hence, $W_1 + W_2$ is a subspace of $V$.
    \item If $x \in W_1 + W_2$, then there exists $x_1 \in W_1$ and
      $x_2 \in W_2$ such that $x = x_1 + x_2$.
      Since $W_1 \subseteq W$ and $W_2 \subseteq W$, we have $x_1 \in W$ and
      $x_2 \in W$, which implies $x \in W$. \qedhere
  \end{enumerate}
\end{proof}

\section{Spanning Sets}
\begin{definition}\label{def:summation}
  Let $(G, +)$ be an Abelian group.
  Then we define
  \begin{equation*}
    \sum_{i=m}^n a_i =
      \left\{
      \begin{array}{ll}
        \displaystyle
          \sum_{i=m}^{n-1} a_i + a_n & \text{if} \; m \leq n\\[1.5em]
        0_G & \text{if} \; m > n,\\
      \end{array}
      \right.
  \end{equation*}
  where $a_i \in G$ for each integer $i$ with $m \leq i \leq n$.
\end{definition}

\begin{definition}\label{def:linear-combination}
  Let $(V, +, \cdot)$ be a vector space over $F$.
  Let $S$ be a subset of $V$.
  Then a vector $x \in V$ is called a \emph{linear combination} of $S$ if
  there exist some nonnegative integer $n$,
  scalars $a_1, \dots, a_n \in F$,
  and vectors $x_1, \dots, x_n \in S$ such that
  \begin{equation*}
    x = \sum_{i=1}^n a_ix_i.
  \end{equation*}
\end{definition}
\begin{remark}
  Since $n$ can be zero, $0_V$ is a linear combination for all $S \subseteq V$.
\end{remark}
\begin{remark}
  Although $S$ can be infinite, the number of terms in the summation must be
  finite.
  For example, in the vector space $\mathbb{R}$ over $\mathbb{Q}$,
  although we have
  \begin{equation*}
    e = \sum_{k=0}^\infty \frac{1}{k!}
      = \frac{1}{1} + \frac{1}{1} + \frac{1}{2} + \frac{1}{6}
        + \frac{1}{24} + \cdots,
  \end{equation*}
  $e$ is still not a linear combination of $\mathbb{Q}$.
\end{remark}

\begin{definition}\label{def:span}
  Let $(V, +, \cdot)$ is a vector space over $F$.
  The \emph{span} of $S$, denoted $\mathrm{span}(S)$, is the set that
  consists of all linear combinations of $S$.
\end{definition}

\begin{theorem}\label{thm:span}
  Let $(V, +, \cdot)$ be a vector space over $F$.
  Let $S \subseteq V$. Then the following statements are true.
  \begin{enumerate}
    \item $\mathrm{span}(S)$ is a subspace of $V$.
    \item If $W$ is a subspace of $V$ such that $S \subseteq W$, then
      $\mathrm{span}(S) \subseteq W$.
  \end{enumerate}
\end{theorem}
\begin{proof} \leavevmode
\begin{enumerate}
  \item If $c \in F$ and $x, y \in \mathrm{span}(S)$, then there exist
    nonnegative integers $m, n$,
    scalars $a_1, \dots, a_m, b_1, \dots, b_n \in F$
    and vectors $x_1, \dots, x_m, y_1, \dots, y_n \in S$ such that
    \begin{equation*}
      \begin{array}{lll}
        \displaystyle x = \sum_{i=1}^m a_ix_i
        & \text{and}
        & \displaystyle y = \sum_{j=1}^n b_jy_j.
      \end{array}
    \end{equation*}
    Thus, we have
    \begin{align*}
      cx
      &= c(a_1x_1 + \cdots + a_mx_m) \\
      &= c(a_1x_1) + \cdots + c(a_mx_m) \\
      &= (ca_1)x_1 + \cdots + (ca_m)x_m \in \mathrm{span}(S)
    \end{align*}
    and
    \begin{equation*}
      x + y
        = a_1x_1 + \cdots a_mx_m + b_1y_1 + \cdots + b_ny_n
        \in \mathrm{span}(S).
    \end{equation*}
    Also, $0_V \in \mathrm{span}(S)$.
    Hence, $\mathrm{span}(S)$ is a subspace of $V$.
  \item If $x \in \mathrm{span}(S)$, then there exists an
    nonnegative integer $n$, scalars $a_1, \dots, a_n \in F$
    and vectors $x_1, \dots, x_n \in S$ such that
    \begin{equation*}
      x = \sum_{i=1}^m a_ix_i.
    \end{equation*}
    Thus, since $x_1, \dots, x_n \in W$, we have
    $x = a_1x_1 + \cdots + a_nx_n \in W$. \qedhere
\end{enumerate}
\end{proof}

\begin{definition}\label{def:spanning-set}
  A subset $S$ of a vector space $(V, +, \cdot)$ \emph{spans} $V$
  if $\mathrm{span}(S) = V$.
  In this case, we also say that $S$ is a \emph{spanning set} of $V$.
\end{definition}

\begin{example}
  $\{(0, 1, 1), (1, 0, 1), (1, 1, 0)\}$ is a spanning set of $\mathbb{R}^3$
  since for all $x, y, z \in \mathbb{R}$,
  \begin{equation*}
    (x, y, z)
      = \frac{-x+y+z}{2} \cdot (0, 1, 1)
      + \frac{x-y+z}{2} \cdot (1, 0, 1)
      + \frac{x+y-z}{2} \cdot (1, 1, 0).
  \end{equation*}
\end{example}

\section{Linearly Independent Sets}
\begin{definition}
  Let $(V, +, \cdot)$ be a vector space over $F$.
  Let $S$ be a subset of $V$.
  For scalars $a_1, \dots, a_n \in F$ and distinct vectors
  $x_1, \dots, x_n \in S$, we say that
  \begin{equation*}
    \sum_{i=1}^n a_ix_i = 0_V
  \end{equation*}
  is a \emph{trivial representation} of $0_V$ as a linear combination of $S$
  if $a_1 = \cdots = a_n = 0_F$.
\end{definition}

\begin{definition}
  Let $(V, +, \cdot)$ be a vector space over $F$.
  \begin{itemize}
    \item A subset $S$ of $V$ is called \emph{linearly dependent} if there
      exists a nontrivial representation of $0_V$ as a linear combination of
      $S$.
    \item A subset $S$ of $V$ is called \emph{linearly independent} if
      it is not linear dependent.
  \end{itemize}
\end{definition}

\begin{theorem}\label{thm:linear-dependence-first-equivalence}
  Let $(V, +, \cdot)$ be a vector space over $F$ and let $S \subseteq V$.
  Then $S$ is linearly independent if and only if there exists $x \in S$
  such that $x \in \mathrm{span}(S \setminus \{x\})$.
\end{theorem}
\begin{proof}
  ($\Rightarrow$) Because $S$ is linearly dependent, it follows that
  there exists a nontrivial representation
  \begin{equation*}
    a_1x_1 + a_2x_2 + \cdots + a_nx_n = 0_V
  \end{equation*}
  as a linear combination of $S$, where $a_1, \dots, a_n \in F$ are scalars
  and $x_1, \dots, x_n \in S$ are distinct vectors.
  Without loss of generality, let $a_1 \neq 0_F$.
  Then we have
  \begin{align*}
    x_1
    &= (-a_1)^{-1}(a_2x_2 + \cdots + a_nx_n) \\
    &= (-a_1)^{-1}a_2x_2 + \cdots + (-a_1)^{-1}a_nx_n \\
    &\in \mathrm{span}(S \setminus \{x_1\}).
  \end{align*}
  
  ($\Leftarrow$) Since $x \in \mathrm{span}(S \setminus \{x\})$, there exists
  scalars $a_1, \dots, a_n \in F$ and distinct vectors
  $x_1, \dots, x_n \in S \setminus \{x\}$ such that
  \begin{equation*}
    a_1x_1 + \cdots + a_nx_n = x.
  \end{equation*}
  Then
  \begin{equation*}
    (-1_F)x + a_1x_1 + \cdots + a_nx_n = 0_V
  \end{equation*}
  is a nontrivial representation of $0_V$ as a linear combination of $S$.
\end{proof}

\begin{theorem}\label{thm:linear-dependence-second-equivalence}
  Let $(V, +, \cdot)$ be a vector space over $F$.
  Let $S$ be a subset of $V$ and let $x$ be an element of $S$.
  Then $x \in \mathrm{span}(S \setminus \{x\})$ if and only if
  $\mathrm{span}(S) = \mathrm{span}(S \setminus \{x\})$.
\end{theorem}
\begin{proof}
  ($\Rightarrow$) Since $x \in \mathrm{span}(S \setminus \{x\})$
  and $S \setminus \{x\} \subseteq \mathrm{span}(S \setminus \{x\})$,
  we have
  \begin{equation*}
    S \subseteq \mathrm{span}(S \setminus \{x\})
    \quad \Rightarrow \quad
    \mathrm{span}(S) \subseteq \mathrm{span}(S \setminus \{x\})
  \end{equation*}
  by
  \Cref{thm:span}.
  Also, $\mathrm{span}(S \setminus \{x\}) \subseteq \mathrm{span}(S)$
  because $S \setminus \{x\} \subseteq S$.
  Thus, we can conclude that
  $\mathrm{span}(S \setminus \{x\}) = \mathrm{span}(S)$.

  ($\Leftarrow$) Since
  $x \in S \subseteq \mathrm{span}(S) = \mathrm{span}(S \setminus \{x\})$,
  we have $x \in \mathrm{span}(S \setminus \{x\})$.
\end{proof}

\begin{example}
  Let $S = \{1, 1+2x, 1+2x+3x^2, 1+2x+3x^2+4x^3\}$ be a subset of
  $\mathcal{P}_3(\mathbb{R})$.
  Then $S$ is linearly independent since the only solution to the
  following system of linear equations
  \begin{equation*}
    \setlength\arraycolsep{0pt}
    \begin{array}{r>{{}}c<{{}}r>{{}}c<{{}}r>{{}}c<{{}}r@{{}={}}r}
      a_1 & &      & &      & &      & 0 \\
      a_1 &+& 2a_2 & &      & &      & 0 \\
      a_1 &+& 2a_2 &+& 3a_3 & &      & 0 \\
      a_1 &+& 2a_2 &+& 3a_3 &+& 4a_4 & 0 \\
    \end{array}
  \end{equation*}
  is $a_1 = a_2 = a_3 = a_4 = 0$.
\end{example}

\begin{theorem}\label{thm:linear-independence-implication}
  Let $(V, +, \cdot)$ be a vector space, and let $R \subseteq S \subseteq V$.
  If $R$ is linearly dependent, then $S$ is linearly dependent.
\end{theorem}
\begin{proof}
  If $R$ is linearly dependent, then there exists $x \in R$ such that
  $x \in \mathrm{span}(R \setminus \{x\})$.
  By $R \subseteq S$, we have
  $R \setminus \{x\} \subseteq S \setminus \{x\}$.
  Since $x \in S$ and $x \in \mathrm{span}(S \setminus \{x\})$, $S$ is
  linearly dependent.
\end{proof}

\begin{corollary}
  Let $(V, +, \cdot)$ be a vector space, and let $R \subseteq S \subseteq V$.
  If $S$ is linearly independent, then $R$ is linearly independent.
\end{corollary}
\begin{proof}
  Suppose that $S$ is linearly independent.
  If $R$ is linearly dependent, then so is $S$ by
  \Cref{thm:linear-independence-implication}, contradiction.
  Thus, $R$ is linearly independent.
\end{proof}

\begin{theorem}\label{thm:linearly-independent-subset}
  Let $(V, +, \cdot)$ be a vector space.
  For each finite set $S \subseteq V$, there exists a linearly independent
  set $Q \subseteq S$ such that $\mathrm{span}(Q) = \mathrm{span}(S)$.
\end{theorem}
\begin{proof}
  The proof is by induction on $n = |S|$.
  The induction begins with $n = 0$, i.e., $S = \varnothing$.
  Since $\varnothing$ is linearly independent, we can choose
  $R = \varnothing$, and thus the theorem holds.

  Now suppose that the theorem is true for some integer $n \geq 0$,
  and we prove that the theorem holds for $n + 1$.
  If $S$ is linearly independent, then we can choose $Q = S$.
  Otherwise, there exists $x \in S$
  with $\mathrm{span}(S \setminus \{x\}) = \mathrm{span}(S)$ because
  $S$ is linearly dependent.
  Let $S' = S \setminus \{x\}$. Then there exists a linearly independent set
  $Q \subseteq S'$ such that $\mathrm{span}(Q) = \mathrm{span}(S')$ by
  induction hypothesis, implying
  $Q \subseteq S$ and $\mathrm{span}(Q) = \mathrm{span}(S)$.
\end{proof}

\section{Bases and Dimension}
\begin{definition}\label{def:basis}
  Let $(V, +, \cdot)$ be a vector space. A subset $S$ of $V$ is a \emph{basis}
  of $V$ if $S$ is not only a spanning set but also a linearly independent set
  of $V$.
\end{definition}

\begin{example}
  Following are some examples of bases.
  \begin{itemize}
    \item Since $\mathrm{span}(\varnothing) = \{0_V\}$ and $\varnothing$ is
      linearly independent, $\varnothing$ is a basis of $\{0_V\}$.
    \item Let $S = \{x_1, \dots, x_n\}$ be a subset of $F^n$ with
      $(x_i)_j = [\![i = j]\!]$ for all $i, j \in \{1, \dots, n\}$.
      Then $S$ is called the \emph{standard basis} of $F^n$.
    \item The set $S = \{1_F, x, x^2, \dots, x^n\}$ is the called the
      \emph{standard basis} of $\mathcal{P}_n(F)$.
  \end{itemize}
\end{example}

\begin{theorem}\label{thm:finite-basis-existence}
  Let $(V, +, \cdot)$ be a vector space over $F$.
  If there exists a finite set $S$ that spans $V$, then there is a subset
  $Q$ of $S$ that is a finite basis of $V$.
\end{theorem}
\begin{proof}
  By \Cref{thm:linearly-independent-subset}, there exists
  a linearly independent set $Q \subseteq S$
  such that $\mathrm{span}(Q) = \mathrm{span}(S) = V$.
  Thus, $Q$ is a finite basis of $V$.
\end{proof}

\begin{theorem}[Replacement Theorem]\label{thm:replacement}
  Let $(V, +, \cdot)$ be a vector space over $F$.
  Let $S$ be a finite set that spans $V$,
  and let $Q \subseteq V$ be a finite linearly independent set.
  Then $|Q| \leq |S|$, and there exists $R \subseteq S \setminus Q$ such that
  both $|Q \cup R| = |S|$ and $\mathrm{span}(Q \cup R) = V$ hold.
\end{theorem}
\begin{proof}
  The proof is based on induction on $|Q|$.
  The induction begins with $|Q| = 0$, i.e., $Q = \varnothing$.
  Choosing $R = S$, we have $Q \cup R = S$, and thus
  both $|Q \cup R| = S$ and $\mathrm{span}(Q \cup R) = V$ hold.

  Now suppose that the theorem is true for $|Q| = m$ with $m \geq 0$,
  and we prove that the theorem holds for $|Q|= m + 1$.
  Let $Q = \{x_1, \dots, x_{m+1}\}$ and let $Q' = Q \setminus \{x_{m+1}\}$.
  By induction hypothesis, there exists
  $R' = \{y_1, \dots, y_k\} \subseteq S \setminus Q'$
  such that $m + k = |S|$ and $\mathrm{span}(Q' \cup R') = V$.
  Since $Q' \cup R'$ spans $V$, there exists
  $a_1, \dots, a_m, b_1, \dots, b_k \in F$ such that
  \begin{equation*}
    x_{m+1} = \sum_{i=1}^m a_ix_i + \sum_{j=1}^k b_jy_j.
  \end{equation*}
  If $b_j = 0_F$ for all $j \in \{1, \dots, k\}$, then $x_{m+1}$ is a linear
  combination of $Q$, implying that $Q$ is linearly dependent, contradiction.
  Thus, there must exist some $j \in \{1, \dots, k\}$
  such that $b_j \neq 0_F$.
  Without loss of generality let $b_k \neq 0_F$.
  Also, let $R = \{y_1, \dots, y_{k-1}\}$.
  Then $|Q \cup R| = (m+1) + (k-1) = |S|$.
  Since $k \geq 1$, we have $|Q| \leq |S|$.
  Note that $(Q' \cup R') \setminus (Q \cup R) = \{y_k\}$.
  By
  \begin{equation*}
    y_k = (-b_k)^{-1}\left(
       \sum_{i=1}^m a_ix_i + (-1_F)x_{m+1} + \sum_{j=1}^{k-1} b_jy_j
    \right) \in \mathrm{span}(Q \cup R),
  \end{equation*}
  we have
  \begin{equation*}
    Q' \cup R'
      \subseteq Q \cup R \cup \{y_k\}
      \subseteq \mathrm{span}(Q \cup R).
  \end{equation*}
  Thus, by \Cref{thm:span} we have
  \begin{equation*}
    V = \mathrm{span}(Q' \cup R')
      \subseteq \mathrm{span}(Q \cup R)
      \subseteq V,
  \end{equation*}
  implying $\mathrm{span}(Q \cup R) = V$.
\end{proof}

\begin{corollary}
  Let $(V, +, \cdot)$ be a vector space over $F$ that is spanned by a finite
  set. Then every linearly independent subset of $V$ is finite.
\end{corollary}
\begin{proof}
  Suppose that $S$ is a finite spanning set of $V$ and that $Q$ is linearly
  independent.
  If $Q$ is infinite, then there exists $Q' \subseteq Q$ with
  $|Q'| = |S| + 1$.
  It follows that $Q'$ is linearly independent by
  \Cref{thm:linear-independence-implication}, and thus $|Q'| \leq |S|$ by
  \Cref{thm:replacement}, contradiction to $|Q'| = |S| + 1$.
  Therefore, $Q$ is finite.
\end{proof}

\begin{theorem}\label{thm:dimension}
  Let $(V, +, \cdot)$ be a vector space over $F$.
  If $V$ has a finite basis, then all bases of $V$ have the same size.
\end{theorem}
\begin{proof}
  Let $S$ be a finite basis of $V$ and let $Q$ be an arbitrary basis of $V$.
  Since $V = \mathrm{span}(S)$ and $Q$ is linearly independent, it
  follows that $Q$ is finite, and thus $|Q| \leq |S|$ by
  replacement theorem (\Cref{thm:replacement}).

  Also, since $V = \mathrm{span}(Q)$ and $S$ is linearly independent,
  we have $|S| \leq |Q|$ by replacement theorem (\Cref{thm:replacement}).
  Thus, $|Q| = |S|$.
\end{proof}

\begin{definition}
  A vector space $(V, +, \cdot)$ over $F$ is called \emph{finite-dimensional}
  if it has a finite basis.
  A vector space that is not finite-dimensional is called
  \emph{infinite-dimensional}.
\end{definition}

\begin{definition}
  The number of vectors in each basis of a finite-dimensional vector space $V$
  is called the \emph{dimension} of $V$ and is denoted by $\mathrm{dim}(V)$.
\end{definition}

\begin{example}
  We have $\mathrm{dim}(\{0_V\}) = 0$, $\mathrm{dim}(F^n) = n$, and
  $\mathrm{dim}(\mathcal{P}_n(F)) = n + 1$.
\end{example}

\begin{example}
  The dimension of a vector space depends on its field of scalars.
  \begin{itemize}
    \item If $V = \mathbb{C}$ is a vector space over $\mathbb{R}$, then
      $\mathrm{dim}(V) = 2$ since $\{1, i\}$ is a basis of $V$.
    \item If $W = \mathbb{C}$ is a vector space over $\mathbb{C}$, then
      $\mathrm{dim}(W) = 1$ since $\{1\}$ is a basis of $W$.
  \end{itemize}
\end{example}

\begin{theorem}\label{thm:basis-equivalence}
  Let $(V, +, \cdot)$ be a vector space over $F$.
  Then a subset of $V$ of $n = \mathrm{dim}(V)$ vectors is linearly
  independent if and only if it is a spanning set of $V$.
\end{theorem}
\begin{proof}
  ($\Rightarrow$) Suppose that $Q$ is linearly independent with $|Q| = n$.
  By replacement theorem (\Cref{thm:replacement}), there exists
  $R \subseteq S \setminus Q$ such that $|Q \cup R| = |S|$ and
  $\mathrm{span}(Q \cup R) = V$.
  Since $|Q| = |S|$, we have $|R| = 0$, i.e., $R = \varnothing$.
  Thus, $\mathrm{span}(Q) = V$.

  ($\Leftarrow$) Suppose that $S$ spans $V$ with $|S| = n$.
  By \Cref{thm:finite-basis-existence}, there is a subset $Q$ of $S$
  that is a basis of $V$.
  Then we have $|Q| = n$, implying $Q = S$.
  Thus, $S$ is a basis of $V$.
\end{proof}

\begin{theorem}\label{thm:subspace-dimension}
  Let $(V, +, \cdot)$ be a finite-dimensional vector space over $F$,
  and let $V'$ be a subspace of $V$. Then the following statements hold.
  \begin{enumerate}
    \item $\mathrm{dim}(V') \leq \mathrm{dim}(V)$.
    \item If $\mathrm{dim}(V') = \mathrm{dim}(V)$, then $V' = V$.
  \end{enumerate}
\end{theorem}
\begin{proof}
  Let $S$ be a basis of $V$ and let $S'$ be a basis of $V'$.
  \begin{enumerate}
    \item Since $S'$ is linearly independent and $V = \mathrm{span}(S)$,
      we have $|S'| \leq |S|$ by replacement theorem (\Cref{thm:replacement}).
      Thus, $\mathrm{dim}(V') \leq \mathrm{dim}(V)$.
    \item Since $S'$ is linearly independent and $|S'| = \mathrm{dim}(V)$,
      we have $\mathrm{span}(S') = V$ by \Cref{thm:basis-equivalence}.
      Thus, $V' = \mathrm{span}(S') = V$. \qedhere
  \end{enumerate}
\end{proof}