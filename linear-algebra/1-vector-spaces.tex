\chapter{Vector Spaces}
\section{Abelian Groups and Fields}
\begin{definition}
  \label{def:abelian-group}
  Let $G$ be a set, and let $\star$ be a binary operation defined on $G$.
  We say that $(G, \star)$ is an \emph{Abelian group} if it satisfies the
  following properties.
  \begin{enumerate}[1.]
    \item Closeness: If $a \in G$ and $b \in G$, then $a \star b \in G$.
    \item Commutativity: We have $a \star b = b \star a$ for any $a, b \in G$.
    \item Associativity: We have $(a \star b) \star c = a \star (b \star c)$
    for any $a, b, c \in G$.
    \item Existence of identity: There exists an element $e \in G$, called the
    \emph{identity}, such that $a \star e = a$ for any $a \in G$.
    \item Existence of inverses: For all $a \in G$, there exists an element
    $b \in G$, called the \emph{inverse} of $a$, such that $a \star b = e$.
  \end{enumerate}
\end{definition}

\begin{definition}
  \label{def:field}
  Let $F$ be a field, and let $+$ and $\cdot$ be binary operations defined on
  $F$.
  We say that $(F, +, \cdot)$ is a \emph{field} if it satisfies the following
  conditions.
  \begin{enumerate}[1.]
    \item Additive Abelian group: $(F, +)$ is an Abelian group with identity
    $0_F$.
    \item Multiplicative Abelian group: $(F \setminus \{0_F\}, \cdot)$ is an
    Abelian group with identity $1_F$.
    \item Distributivity: $a \cdot (b + c) = a \cdot b + a \cdot c$ for any
    $a, b, c \in F$.
  \end{enumerate}
  Let $-a$ and $a^{-1}$ denote the additive inverse and the multiplicative
  inverse of $a$, respectively.
  Subtraction and division are defined as follows.
  \begin{itemize}
    \item For $a \in F$ and $b \in F$, we define $a - b = a + (-b)$.
    \item For $a \in F$ and $b \in F \setminus \{0_F\}$, we define
    $a / b = a \cdot b^{-1}$.
  \end{itemize}
\end{definition}

\begin{remark}
  We may use the underlying set $F$ to represent the entire field if addition
  and multiplication are self-explanatory.
\end{remark}

\begin{examples}
  The set $\mathbb{Q}$ of rational numbers, the set $\mathbb{R}$ of real
  numbers, and the set $\mathbb{C}$ of complex numbers are all fields.
\end{examples}

\begin{theorem}[Cancellation Laws]
  \label{thm:cancellation-law}
  Let $F$ be a field with $a, b, c \in F$.
  \begin{enumerate}
    \item If $a + c = b + c$, then $a = b$.
    \item If $a \cdot c = b \cdot c$ and $c \neq 0_F$, then $a = b$.
  \end{enumerate}
\end{theorem}
\begin{proof}
  The proof of (a) follows from the definition of fields.
  We have
  \begin{align*}
    a
    &= a + 0_F \\
    &= a + (c + (-c)) \\
    &= (a + c) + (-c) \\
    &= (b + c) + (-c) \\
    &= b + (c + (-c)) \\
    &= b + 0_F \\
    &= b.      
  \end{align*}
  The proof of (b) is similar to the proof of (a).
\end{proof}

\begin{corollary}
  The identity and inverse elements in a field are unique.
  That is, the following statements are true for any field $F$ with
  $a, b \in F$.
  \begin{enumerate}
    \item If $a + b = a$, then $b = 0_F$.
    If $a + b = 0_F$, then $b = -a$.
    \item If $a \cdot b = a$ and $a \neq 0_F$, then $b = 1_F$.
    If $a \cdot b = 1_F$ and $a \neq 0_F$, then $b = a^{-1}$.
  \end{enumerate}
\end{corollary}

\begin{theorem}
  Let $F$ be a field and let $a \in F$.
  Then we have $-(-a) = a$.
  Also, if $a \neq 0_F$, we have $(a^{-1})^{-1} = a$.
\end{theorem}
\begin{proof}
  Since $-(-a) + (-a) = 0_F = a + (-a)$, we have $-(-a) = a$.
  If $a \neq 0_F$, then we have $(a^{-1})^{-1} \cdot a^{-1} = 1_F = a \cdot
  a^{-1}$, and thus $(a^{-1})^{-1} = a$.
\end{proof}

\begin{theorem}
  The following statements are true for any field $F$ with $a, b \in F$.
  \begin{enumerate}
    \item $a \cdot 0_F = 0_F = 0_F \cdot a$.
    \item $(-a) \cdot b = -(a \cdot b) = a \cdot (-b)$.
    \item $(-a) \cdot (-b) = a \cdot b$.
  \end{enumerate}
\end{theorem}
\begin{proof}
  \leavevmode
  \begin{enumerate}
    \item It suffices to prove the first equality.
    Since
    \begin{equation*}
      0_F + a \cdot 0_F
      = a \cdot 0_F
      = a \cdot (0_F + 0_F)
      = a \cdot 0_F + a \cdot 0_F,
    \end{equation*}
    it follows from cancellation law (\Cref{thm:cancellation-law}) that
    $a \cdot 0_F = 0_F$.
    \item It suffices to prove the first equality.
    We have
    \begin{equation*}
      a \cdot b + (-a) \cdot b
      = (a + (-a)) \cdot b
      = 0_F \cdot b
      = 0_F,
    \end{equation*}
    where the last equality follows from (a).
    Thus, $(-a) \cdot b = -(a \cdot b)$ due to the uniqueness of additive
    inverses.
    \item We have $(-a) \cdot (-b) = -(a \cdot (-b)) = -(-(a \cdot b))
    = a \cdot b$ by applying (b) twice.
    \qedhere
  \end{enumerate}
\end{proof}

\section{Vector Spaces}
\begin{definition}
  \label{def:vector-space}
  A \emph{vector space} over a field $F$ is a set $V$ with two operations,
  called \emph{addition} (denoted by $+$) and \emph{scalar multiplication}
  (denoted by $\cdot$), which satisfy the following axioms.
  \begin{itemize}
    \item Closeness: If $a \in F$ and $x, y \in V$, then $x + y \in V$ and
    $a \cdot x \in V$.
    \item Commutativity: $x + y = y + x$ holds for any $x, y \in V$.
    \item Associativity: $(x + y) + z = x + (y + z)$ and
    $(a \cdot b) \cdot x = a \cdot (b \cdot x)$ hold for any
    $a, b \in F$ and $x, y, z \in V$.
    \item Identity elements: There is an element $0_V$ in $V$, called the
    \emph{additive identity} of $V$, such that $x + 0_V = x$ for any $x \in V$.
    Also, $1_F \cdot x = x$ for any $x \in V$.
    \item Inverse elements: For each $x \in V$ there is an element $-x$ in $V$,
    called the \emph{additive inverse} of $x$, such that $x + (-x) = 0_V$.
    \item Distributivity: $a \cdot (x + y) = a \cdot x + a \cdot y$ and
    $(a + b) \cdot x = a \cdot x + b \cdot x$ hold for any $a, b \in F$ and
    $x, y \in V$.
  \end{itemize}
  The elements of $F$ and the elements of $V$ are usually called \emph{scalars}
  and \emph{vectors}, respectively.
  Subtraction of vectors is defined by $x - y = x + (-y)$ for any $x, y \in V$.
\end{definition}
\begin{remark}
  For simplification, we usually write $ax$ instead of $a \cdot x$ in this
  note.
\end{remark}

\begin{examples}
  Let $F$ be a field.
  \begin{itemize}
    \item $F$ is a vector space over $F$.
    \item The set of \emph{$n$-tuples} with entries from $F$, denoted $F^n$,
    is a vector space over $F$.
    \item The set of all $m \times n$ \emph{matrices} with entries from $F$,
    denoted $F^{m \times n}$, is a vector space over $F$.
    \item The set of \emph{polynomials} with coefficients from $F$, denoted
    $\mathcal{P}(F)$, is a vector space over $F$.
  \end{itemize}
\end{examples}

\section{Subspaces}
\begin{definition}
  \label{def:subspace}
  Let $V$ be a vector space over $F$.
  A \emph{subspace} of $V$ is a subset $W$ of $V$ such that $W$ is a vector
  space over $F$, where addition and scalar multiplication are the same as
  those defined on $V$.
\end{definition}

\begin{theorem}
  \label{thm:subspace}
  Let $V$ be a vector space over $F$ and let $W \subseteq V$.
  Then $W$ is a subspace of $V$ if and only if $0_V \in W$ and $ax + y \in W$
  for any $a \in F$ and $x, y \in W$.
\end{theorem}
\begin{proof}
  ($\Rightarrow$)
  Straightforward.
  ($\Leftarrow$)
  It suffices to prove the closeness of addition and scalar multiplication,
  and the existence of additive inverses.
  For any $a \in F$ and $x, y \in W$, we have
  \begin{equation*}
    a \cdot x = a \cdot x + 0_W \in W
    \quad \text{and} \quad
    x + y = 1_F \cdot x + y \in W.
  \end{equation*}
  Furthermore, we have
  \begin{equation*}
    x + (-1_F) \cdot x
    = 1_F \cdot x + (-1_F) \cdot x
    = (1_F + (-1_F)) \cdot x
    = 0_F \cdot x
    = 0_V,
  \end{equation*}
  for any $x \in W$, which completes the proof.
\end{proof}

\begin{example}
  For any vector space $V$, $V$ and $\{0_V\}$ are subspaces of $V$.
\end{example}

\begin{example}
  The set $\mathcal{P}_n(F)$ of polynomials in $\mathcal{P}(F)$ with
  degree less than or equal to $n$ is a subspace of $\mathcal{P}(F)$.
\end{example}

\begin{definition}
  Let $V$ be a vector space and let $S_1$ and $S_2$ be subsets of $V$.
  Then the \emph{sum} of $S_1$ and $S_2$ is defined by
  \begin{align*}
    S_1 + S_2 &= \{x + y: \text{$x \in S_1$ and $y \in S_2$}\}.
  \end{align*}
\end{definition}

\begin{theorem}
  \label{thm:subspace-sum}
  Let $V$ be a vector space and let $W_1$ and $W_2$ be subspaces of $V$.
  Then $W_1 + W_2$ is the minimal subspace of $V$ that contains $W_1 \cup W_2$.
\end{theorem}
\begin{proof}
  First we prove that $W_1 + W_2$ is a subspace of $V$.
  We have $0_V = 0_V + 0_V \in W_1 + W_2$, and for any $a \in F$,
  $x_1, y_1 \in W_1$ and $x_2, y_2 \in W_2$, we have
  \begin{align*}
    a(x_1 + x_2) + (y_1 + y_2)
    &= ax_1 + ax_2 + y_1 + y_2 \\
    &= (ax_1 + y_1) + (ax_2 + y_2) \\
    &\in W_1 + W_2.
  \end{align*}
  Thus, it follows from \Cref{thm:subspace} that $W_1 + W_2$ is a subspace of
  $V$.

  Now we prove the minimality.
  Suppose that $W$ is a subspace of $V$ that contains $W_1 \cup W_2$.
  For any $x_1 \in W_1$ and $x_2 \in W_2$, we have $x_1 + x_2 \in W$.
  Thus, $W_1 + W_2 \subseteq W$, completing the proof.
\end{proof}

\section{Spanning Sets}
\begin{definition}
  \label{def:linear-combination}
  Let $V$ be a vector space over $F$ and let $S \subseteq V$.
  A vector $x \in V$ is called a \emph{linear combination} of $S$ if $x = 0_V$
  or there exist scalars $a_1, \dots, a_n \in F$ and vectors
  $x_1, \dots, x_n \in S$ such that
  \begin{equation*}
    x = \sum_{i=1}^n a_ix_i.
  \end{equation*}
  The set of all linear combinations of $S$ is called the \emph{span} of $S$,
  denoted by $\spn(S)$.
  If $W = \spn(S)$, then we say that $W$ is \emph{spanned} by $S$, or $S$ is a
  \emph{spanning set} of $W$.
\end{definition}

\begin{theorem}
  \label{thm:span}
  Let $V$ be a vector space over $F$ and let $S \subseteq V$.
  Then $\spn(S)$ is the minimal subspace of $V$ that contains $S$.
\end{theorem}
\begin{proof}
  First we prove that $\spn(S)$ is a subspace of $V$.
  Obviously $0_V \in \spn(S)$.
  For any $a_1, a_2, \dots, a_n, b_1, b_2, \dots, b_m, c \in F$ and
  for any $x_1, x_2, \dots, x_n, y_1, y_2, \dots, y_m \in S$, we have
  \begin{equation*}
    c\sum_{i=1}^n a_ix_i + \sum_{j=1}^m b_jy_j
    = \sum_{i=1}^n ca_ix_i + \sum_{j=1}^m b_jy_j
    \in \spn(S).
  \end{equation*}
  Thus, $\spn(S)$ is a subspace of $V$.

  Now we prove the minimality.
  Let $W$ be a subspace of $V$ such that $S \subseteq W$.
  Then each element of $\spn(S)$ belongs to $W$ due to the closeness of $W$.
  Thus, $\spn(S) \subseteq W$, which completes the proof.
\end{proof}

\section{Linearly Independent Sets}
\begin{definition}
  \label{def:linear-independence}
  Let $V$ be a vector space over a field $F$ and let $S \subseteq V$.
  We say that $S$ is \emph{linearly dependent} if there exist nonzero scalars
  $a_1, \dots, a_n \in F$ and distinct vectors $x_1, \dots, x_n \in S$ such
  that
  \begin{equation*}
    \sum_{i=1}^n a_ix_i = 0_V.
  \end{equation*}
  We say that $S$ is \emph{linearly independent} is it is not linearly
  dependent.
  The empty set $\varnothing$ is considered to be linearly independent.
\end{definition}

\begin{lemma}
  \label{lem:linear-dependence-equivalence-first}
  Let $V$ be a vector space over $F$ and let $S \subseteq V$.
  Then $S$ is linearly dependent if and only if $x \in \spn(S \setminus \{x\})$
  for some $x \in S$.
\end{lemma}
\begin{proof}
  ($\Rightarrow$)
  Suppose that there exist nonzero scalars $a_1, \dots, a_n \in F$ and distinct
  vectors $x_1, \dots, x_n \in S$ such that
  \begin{equation*}
    \sum_{i=1}^n a_ix_i = 0_V.
  \end{equation*}
  Then we have
  \begin{equation*}
    x_1
    = (-a_1)^{-1}\sum_{i=2}^n a_ix_i
    = \sum_{i=2}^n (-a_1)^{-1}a_ix_i
    \in \spn(S \setminus \{x_1\}).
  \end{equation*}

  ($\Leftarrow$)
  Suppose that $x \in \spn(S \setminus \{x\})$.
  Then there exist nonzero scalars $a_1, \dots, a_n \in F$ and distinct
  vectors $x_1, \dots, x_n \in S \setminus \{x\}$ such that
  \begin{equation*}
    x = \sum_{i=1}^n a_ix_i,
  \end{equation*}
  implying
  \begin{equation*}
    (-1_F)x + \sum_{i=1}^n a_ix_i = 0_V.
    \qedhere
  \end{equation*}
\end{proof}

\begin{lemma}
  \label{lem:linear-dependence-equivalence-second}
  Let $V$ be a vector space over $F$ and let $S \subseteq V$.
  For any $x \in S$, we have $x \in \spn(S \setminus \{x\})$ if and only if
  $\spn(S) = \spn(S \setminus \{x\})$.
\end{lemma}
\begin{proof}
  ($\Leftarrow$)
  Straightforward since $x \in S \subseteq \spn(S) = \spn(S \setminus \{x\})$.
  ($\Rightarrow$)
  Note that we have
  \begin{equation*}
    x \in \spn(S \setminus \{x\})
    \quad \text{and} \quad
    S \setminus \{x\} \subseteq \spn(S \setminus \{x\}).
  \end{equation*}
  Thus, $S \subseteq \spn(S \setminus \{x\})$, and it follows that
  $\spn(S) \subseteq \spn(S \setminus \{x\})$.
  Obviously we have $\spn(S \setminus \{x\}) \subseteq \spn(S)$, and we
  conclude that $\spn(S) = \spn(S \setminus \{x\})$.
\end{proof}

\begin{theorem}
  \label{thm:linear-dependence-equivalence}
  Let $V$ be a vector space over a field $F$ and let $S \subseteq V$.
  Then the following statements are equivalent.
  \begin{enumerate}
    \item $S$ is linearly dependent.
    \item There exists $x \in S$ with $x \in \spn(S \setminus \{x\})$.
    \item There exists $x \in S$ with $\spn(S) = \spn(S \setminus \{x\})$.
  \end{enumerate}
\end{theorem}
\begin{proof}
  Immediately from \Cref{lem:linear-dependence-equivalence-first} and
  \Cref{lem:linear-dependence-equivalence-second}.
\end{proof}

\begin{theorem}
  \label{thm:linear-independence-subset}
  Let $V$ be a vector space.
  Let $R$ and $S$ be subsets of $V$ with $R \subseteq S$.
  If $S$ is linearly independent, then $R$ is linearly independent.
\end{theorem}
\begin{proof}
  Suppose that $R$ is linearly dependent, i.e., there exists a vector $x \in R$
  such that $x \in \spn(R \setminus \{x\})$.
  It follows that $x \in \spn(S \setminus \{x\})$, implying that $S$ is
  linearly dependent, contradiction.
  Thus, $R$ is linearly independent.
\end{proof}

\section{Bases and Dimension}
\begin{definition}
  \label{def:basis}
  A \emph{basis} of a vector space $V$ is a linearly independent subset of $V$
  that spans $V$.
\end{definition}

\begin{example}
  $\varnothing$ is a basis of the zero vector space.
\end{example}

\begin{example}
  Let
  \begin{equation*}
    e_j = \Bigl( \llbracket i = j \rrbracket \Bigr)_{1 \leq i \leq n}
  \end{equation*}
  for each $j \in \{1, \dots, n\}$.
  Then the set $\{e_1, e_2, \dots, e_n\}$ is a basis of $F^n$.
\end{example}

\begin{example}
  $\{1, t, t^2, \dots\}$ is a basis of $\mathcal{P}(F)$.
\end{example}

\begin{proposition}\label{prop:finite-basis-existence}
  Let $V$ be a vector space.
  If there exists a finite set $S$ that spans $V$, then there is a subset
  $Q$ of $S$ that is a finite basis of $V$.
\end{proposition}
\begin{proof}
  The proof is by induction on $|S|$.
  For the induction basis, suppose that $|S| = 0$, i.e., $S = \varnothing$.
  Then the proposition holds since one can choose $Q = \varnothing$ as a basis
  for $V$.

  Now assume the induction hypothesis that the proposition holds for $|S| = n$
  with $n \geq 0$.
  If $S$ is linearly independent, then we can choose $Q = S$ as a basis for
  $V$.
  Otherwise, there exists $x \in S$ with $\spn(S \setminus \{x\}) = \spn(S)$,
  i.e., $S \setminus \{x\}$ spans $V$.
  Thus, by induction hypothesis there is a subset $Q$ of $S \setminus \{x\}$
  that is a basis for $V$, which completes the proof.
\end{proof}

\begin{theorem}[Replacement Theorem]\label{thm:replacement}
  Let $V$ be a vector space over a field $F$.
  Let $S$ be a finite set that spans $V$,
  and let $Q \subseteq V$ be a finite linearly independent set.
  Then $|Q| \leq |S|$, and there exists $R \subseteq S \setminus Q$ such that
  both $|Q \cup R| = |S|$ and $\mathrm{span}(Q \cup R) = V$ hold.
\end{theorem}
\begin{proof}
  The proof is based on induction on $|Q|$.
  The theorem holds for $|Q| = 0$, i.e., $Q = \varnothing$, since we have
  $|\varnothing| \leq |S|$, $|\varnothing \cup S| = |S|$ and
  $\spn(\varnothing \cup S) = V$.

  Now suppose that the theorem is true for $|Q| = m$ with $m \geq 0$,
  and we prove that the theorem holds for $|Q|= m + 1$.
  Let $Q = \{x_1, \dots, x_{m+1}\}$ and let $Q' = \{x_1, \dots, x_m\}$.
  By induction hypothesis, there exists
  $R' = \{y_1, \dots, y_k\} \subseteq S \setminus Q'$
  such that $|Q'| + |R'| = |S|$ and $\mathrm{span}(Q' \cup R') = V$.
  Since $Q' \cup R'$ spans $V$, there exists
  $a_1, \dots, a_m, b_1, \dots, b_k \in F$ such that
  \begin{equation*}
    x_{m+1} = \sum_{i=1}^m a_ix_i + \sum_{j=1}^k b_jy_j.
  \end{equation*}
  If $b_j = 0_F$ for all $j \in \{1, \dots, k\}$, then
  $x_{m+1} \in \spn(Q') = \spn(Q \setminus\{x_{m+1}\})$,
  implying that $Q$ is linearly dependent, contradiction.
  Thus, there must exist some $j \in \{1, \dots, k\}$
  such that $b_j \neq 0_F$.s
  Without loss of generality, suppose that $b_k \neq 0_F$ with $k \geq 1$.
  Also, let $R = \{y_1, \dots, y_{k-1}\}$.
  Then $|Q \cup R| = (m+1) + (k-1) = |S|$, and we have $|Q| \leq |S|$.
  It follows that
  \begin{equation*}
    Q' \cup R'
    \quad \subseteq \quad Q \cup R \cup \{y_k\}
    \quad \subseteq \quad \mathrm{span}(Q \cup R),
  \end{equation*}
  where the second inclusion holds because
  \begin{equation*}
    y_k = (-b_k)^{-1}\left(
       \sum_{i=1}^m a_ix_i + (-1_F)x_{m+1} + \sum_{j=1}^{k-1} b_jy_j
    \right) \in \mathrm{span}(Q \cup R).
  \end{equation*}
  Then, we have
  \begin{equation*}
    V = \mathrm{span}(Q' \cup R')
      \subseteq \mathrm{span}(Q \cup R)
      \subseteq V.
  \end{equation*}
  by \Cref{thm:span}.
  Thus, $\mathrm{span}(Q \cup R) = V$, which completes the proof.
\end{proof}

\begin{corollary}\label{cor:infinite-dimensional}
  Let $V$ be a vector space and $Q$ be a linearly independent subset of $V$
  that is infinite.
  Then each spanning set of $V$ is infinite.
\end{corollary}
\begin{proof}
  Suppose that there is a finite set $S$ that spans $V$.
  Let $Q'$ be a subset of $Q$ with $|Q'| = |S| + 1$.
  By \Cref{prop:linear-independence-subset}, we can conclude that $Q'$ is also
  linearly independent.
  Thus, we have $|Q'| \leq |S|$ by replacement theorem
  (\Cref{thm:replacement}), contradiction.
\end{proof}

\begin{corollary}\label{cor:dimension}
  Let $V$ be a vector space.
  If $V$ has a finite basis, then each basis for $V$ has the same size.
\end{corollary}
\begin{proof}
  Let $S$ be a finite basis for $V$ and $Q$ an arbitrary basis for $V$.
  Since $V = \spn(S)$ and $Q$ is linearly independent, it follows that $Q$ is
  finite by \Cref{cor:infinite-dimensional}, and thus we have $|Q| \leq |S|$.
  Also, since $V = \spn(Q)$ and $S$ is linearly independent, we have
  $|S| \leq |Q|$.
  Thus, $|Q| = |S|$.
\end{proof}

\begin{definition}\label{def:dimension}
  Let $V$ be a vector space.
  \begin{itemize}
    \item $V$ is \emph{finite-dimensional} if it has a finite basis.
      In this case, the number of vectors in each basis for $V$ is called the
      \emph{dimension} of $V$, denoted by $\dim(V)$.
    \item $V$ is \emph{infinite-dimensional} if it is not finite-dimensional.
  \end{itemize}
\end{definition}

\begin{remark}
  \leavevmode
  \begin{itemize}
    \item If a vector space has a linearly independent subset that is infinite,
      we can conclude that it is infinite-dimensional by
      \Cref{cor:infinite-dimensional}.
  \end{itemize}
\end{remark}

\begin{examples}
  One can find the dimension of a vector space by any basis it admits.
  \begin{itemize}
    \item $\dim(\{0_V\}) = 0$.
    \item $\dim(F^n) = n$.
    \item $\dim(F^{m \times n}) = mn$.
    \item $\dim(\mathcal{P}_n(F)) = n + 1$.
    \item $\mathcal{P}(F)$ is infinite-dimensional.
  \end{itemize}
\end{examples}

\begin{examples}
  Note that the dimension of a vector space depends on its field of scalars.
  \begin{itemize}
    \item Let $V = \mathbb{C}$ be a vector space over $\mathbb{R}$.
      Then we have $\dim(V) = 2$ since $\{1, i\}$ is a basis for $V$.
    \item Let $W = \mathbb{C}$ be a vector space over $\mathbb{C}$.
      Then we have $\dim(W) = 1$ since $\{1\}$ is a basis for $V$.
  \end{itemize}
\end{examples}

\begin{proposition}\label{prop:basis-equivalence}
  Let $V$ be a vector space.
  Then a subset of $V$ of $n = \dim(V)$ vectors is linearly
  independent if and only if it is a spanning set of $V$.
\end{proposition}
\begin{proof}
  ($\Rightarrow$) Suppose that $Q$ is linearly independent with $|Q| = n$.
  By replacement theorem (\Cref{thm:replacement}), there exists
  $R \subseteq S \setminus Q$ such that $|Q \cup R| = |S|$ and
  $\mathrm{span}(Q \cup R) = V$.
  Since $|Q| = |S|$, we have $|R| = 0$, i.e., $R = \varnothing$.
  Thus, $\mathrm{span}(Q) = V$.

  ($\Leftarrow$) Suppose that $S$ spans $V$ with $|S| = n$.
  By \Cref{prop:finite-basis-existence}, there is a subset $Q$ of $S$
  that is a basis of $V$.
  Then we have $|Q| = n$, implying $Q = S$.
  Thus, $S$ is a basis for $V$.
\end{proof}

\begin{proposition}\label{prop:unique-coordinate}
  Let $V$ be a finite-dimensional vector space.
  Let $S = \{x_1, \dots, x_n\}$ be a basis for $V$.
  Then for each $x \in V$, there exist a unique $n$-tuple
  $(a_1, \dots, a_n) \in F^n$ with
  \begin{equation*}
    x = a_1x_1 + \cdots + a_nx_n.
  \end{equation*}
\end{proposition}
\begin{proof}
  Since $x \in \spn(S)$, there exist scalars $a_1, \dots, a_n \in F$ such that
  \begin{equation*}
    x = a_1x_1 + \cdots + a_nx_n.
  \end{equation*}
  Now we prove the uniqueness.
  Let $b_1, \dots, b_n \in F$ be scalars with
  \begin{equation*}
    x = b_1x_1 + \cdots + b_nx_n.
  \end{equation*}
  Then we have
  \begin{equation*}
    0_V = (a_1 - b_1)x_1 + \cdots + (a_n - b_n)x_n,
  \end{equation*}
  and it follows that $(a_1 - b_1, a_2 - b_2, \dots, a_n - b_n) = 0_{F^n}$
  since $S$ is linearly independent.
  Thus, $(a_1, \dots, a_n) = (b_1, \dots, b_n)$.
\end{proof}

\begin{proposition}\label{prop:subspace-dimension}
  Let $V$ be a finite-dimensional vector space.
  Let $V'$ be a subspace of $V$.
  Then the following statements are true.
  \begin{enumerate}
    \item $\mathrm{dim}(V') \leq \mathrm{dim}(V)$.
    \item If $\mathrm{dim}(V') = \mathrm{dim}(V)$, then $V' = V$.
  \end{enumerate}
\end{proposition}
\begin{proof}
  Let $S$ and $S'$ be bases for $V$ and $V'$, respectively.
  \begin{enumerate}
    \item Since $S'$ is linearly independent and $V = \mathrm{span}(S)$,
      we have $|S'| \leq |S|$ by replacement theorem (\Cref{thm:replacement}).
      Thus, $\mathrm{dim}(V') \leq \mathrm{dim}(V)$.
    \item Since $S'$ is linearly independent and $|S'| = \mathrm{dim}(V)$,
      we have $\mathrm{span}(S') = V$ by \Cref{prop:basis-equivalence}.
      Thus, $V' = \mathrm{span}(S') = V$. \qedhere
  \end{enumerate}
\end{proof}

\begin{example}
  Let $W$ be the set of $n \times n$ diagonal matrices, which is a subspace of
  $F^{n \times n}$.
  Then one can verify that $\{E_{ii}: 1 \leq i \leq n\}$ is a basis for $W$,
  where $E_{ij}$ is the matrix whose $(i, j)$-entry is $1_F$ and the other
  entries are $0_F$.
  Thus, $\dim(W) = n$.
\end{example}