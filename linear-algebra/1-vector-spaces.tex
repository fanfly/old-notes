\chapter{Vector Spaces}
\section{Fields}
\begin{definition}\label{def:field}
  A \emph{field} is a set $F$ with two operations, called \emph{addition}
  (denoted by $+$) and \emph{multiplication} (denoted by $\cdot$), which
  satisfy the following axioms.
  \begin{enumerate}[leftmargin=3.5em]
    \item[(A 1)] If $a \in F$ and $b \in F$, then $a + b \in F$.
    \item[(A 2)] $a + b = b + a$ for all $a, b \in F$.
    \item[(A 3)] $(a + b) + c = a + (b + c)$ for all $a, b, c \in F$.
    \item[(A 4)] There is an element $0_F$ in $F$ such that $0_F + a = a$ for
      all $a \in F$. 
    \item[(A 5)] For each $a \in F$ there is an element $-a$ in $F$ such that
      $a + (-a) = 0_F$.
    \item[(M 1)] If $a \in F$ and $b \in F$, then $a \cdot b \in F$.
    \item[(M 2)] $a \cdot b = b \cdot a$ for all $a, b \in F$.
    \item[(M 3)] $(a \cdot b) + c = a + (b \cdot c)$ for all $a, b, c \in F$.
    \item[(M 4)] There is an element $1_F$ in $F \setminus \{0_F\}$ such that
      $1_F \cdot a = a$ for all $a \in F$.
    \item[(M 5)] For each $a \in F \setminus \{0_F\}$ there is an element
      $a^{-1}$ in $F$ such that $a \cdot a^{-1} = 1_F$.
    \item[(D)] $a \cdot (b + c) = a \cdot b + a \cdot c$ for all
      $a, b, c \in F$.
  \end{enumerate}
\end{definition}
\begin{remark} \leavevmode
  \begin{itemize}
    \item For simplification, we usually write $ab$ instead of $a \cdot b$.
    \item The axioms labeled with ``A'' and ``M'' are usually called the
      \emph{axioms of addition} and the \emph{axioms of multiplication},
      respectively. The axiom labeld with ``D'' is the \emph{distributive law}.
    \item The elements $0_F$ and $1_F$ are usually called the
      \emph{additive identity} and the \emph{multiplicative identity} of $F$,
      respectively.
      Also, $-a$ and $a^{-1}$ are called the \emph{additive inverse} and the
      \emph{multiplicative inverse} of $a$, respectively.
    \item \emph{Subtraction} and \emph{division} can be defined using additive
      and multiplicative inverses.
  \end{itemize}
\end{remark}

\begin{example}
  $\mathbb{Q}$, $\mathbb{R}$ and $\mathbb{C}$ are fields.
\end{example}

\begin{example}
  Let $\mathbb{B} = \{0, 1\}$ and the operations $\oplus$ and $\odot$ are
  defined as follows.
  \begin{align*}
    \begin{array}{c|cc}
      \oplus & 0 & 1 \\
      \hline
      0      & 0 & 1 \\
      1      & 1 & 0 \\
    \end{array}
    \quad
    \begin{array}{c|cc}
      \odot & 0 & 1 \\
      \hline
      0     & 0 & 0 \\
      1     & 0 & 1 \\
    \end{array}
  \end{align*}
  Then $\mathbb{B}$ is a field with $\oplus$ and $\odot$ as addition and
  multiplication, respectively.
\end{example}

\begin{proposition}\label{prop:field-addition}
  Let $F$ be a field with $a, b, c \in F$.
  \begin{enumerate}
    \item If $a + b = a + c$, then $b = c$.
    \item If $a + b = a$, then $b = 0_F$.
    \item If $a + b = 0_F$, then $b = -a$.
    \item $-(-a) = a$.
  \end{enumerate}
\end{proposition}
\begin{proof} \leavevmode
  \begin{enumerate}
    \item It can be proved by
      \begin{align*}
        b
        &= 0_F + b      \\
        &= (-a + a) + b \\
        &= -a + (a + b) \\
        &= -a + (a + c) \\
        &= (-a + a) + c \\
        &= 0_F + c      \\
        &= c.
      \end{align*}
    \item By applying (a), it follows from $a + b = a + 0_F$ that $b = 0_F$.
    \item By applying (a), it follows from $a + b = a + (-a)$ that $b = -a$.
    \item Since $-a + a = 0_F$, we have $a = -(-a)$ by (c). \qedhere
  \end{enumerate}
\end{proof}

\begin{proposition}\label{prop:field-multiplication}
  Let $F$ be a field with $a, b, c \in F$ and $a \neq 0_F$.
  \begin{enumerate}
    \item If $a \cdot b = a \cdot c$, then $b = c$.
    \item If $a \cdot b = a$, then $b = 1_F$.
    \item If $a \cdot b = 1_F$, then $b = a^{-1}$.
    \item $(a^{-1})^{-1} = a$.
  \end{enumerate}
\end{proposition}
\begin{proof}
  The proof is omitted since it is similar to that of
  \Cref{prop:field-addition}.
\end{proof}

\begin{proposition}\label{prop:field-operation}
  Let $F$ be a field with $a, b \in F$.
  \begin{enumerate}
    \item $0_F \cdot a = 0_F$.
    \item $(-a) \cdot b = -(a \cdot b) = a \cdot (-b)$.
    \item $(-a) \cdot (-b) = a \cdot b$.
  \end{enumerate}
\end{proposition}
\begin{proof} \leavevmode
  \begin{enumerate}
    \item Since
      \begin{equation*}
        0_F \cdot a + 0_F \cdot a
        = (0_F + 0_F) \cdot a
        = 0_F \cdot a,
      \end{equation*}
      we have $0_F \cdot a = 0_F$ by \Cref{prop:field-addition} (b).
    \item Since
      \begin{equation*}
        (-a) \cdot b + a \cdot b
        = (-a + a) \cdot b
        = 0_F \cdot b
        = 0_F,
      \end{equation*}
      we have $(-a) \cdot b = -(a \cdot b)$ by \Cref{prop:field-addition} (c).
      The other half can be proved similarly.
    \item By applying (b) twice, we have
      \begin{equation*}
        (-a) \cdot (-b)
        = -(a \cdot (-b))
        = -(-(a \cdot b))
        = a \cdot b. \qedhere
      \end{equation*}
  \end{enumerate}
\end{proof}

\section{Vector Spaces}
\begin{definition}\label{def:vector-space}
  A \emph{vector space} over a field $F$ is a set $V$ with two operations,
  called \emph{addition} (denoted by $+$) and \emph{scalar multiplication}
  (denoted by $\cdot$), which satisfy the following axioms.
  \begin{enumerate}[leftmargin=3.5em]
    \item[(V 1)] If $x \in V$ and $y \in V$, then $x + y \in V$.
    \item[(V 2)] $x + y = y + x$ for all $x, y \in V$. 
    \item[(V 3)] $(x + y) + z = x + (y + z)$ for all $x, y, z \in V$.
    \item[(V 4)] There is an element $0_V$ in $V$ such that $0_V + x = x$ for
      all $x \in V$.
    \item[(V 5)] For each $x \in V$ there is an element $-x$ such that
      $x + (-x) = 0_V$.
    \item[(V 6)] If $a \in F$ and $x \in V$, then $a \cdot x \in V$. 
    \item[(V 7)] $(a \cdot b) \cdot x = a \cdot (b \cdot x)$ for all
      $a, b \in F$ and $x \in V$. 
    \item[(V 8)] $1_F \cdot x = x$ for all $x \in V$.
    \item[(V 9)] $a \cdot (x + y) = a \cdot x + a \cdot y$ for all $a \in F$
      and $x, y \in V$.
    \item[(V 10)] $(a + b) \cdot x = a \cdot x + b \cdot x$ for all
      $a, b \in F$ and $x \in V$.
  \end{enumerate}
\end{definition}

\begin{remark} \leavevmode
  \begin{itemize}
    \item For simplification, we usually write $ax$ instead of $a \cdot x$.
    \item The elements $0_V$ is usually called the \emph{additive identity} of
      $V$, and $-x$ is called the \emph{additive inverse} of $x$ in $V$.
    \item \emph{Subtraction} can be defined using additive inverses.
  \end{itemize}
\end{remark}

\begin{example}
  A field is a vector space over itself.
\end{example}

\begin{example}
  $\mathbb{C}$ is a vector space over $\mathbb{R}$.
\end{example}

\begin{example}
  $\mathbb{R}$ is a vector space over $\mathbb{Q}$.
\end{example}

\begin{example}
  The set of \emph{$n$-tuples} with elements from a field $F$ is denoted by
  $F^n$.
  For $x = (x_1, \dots, x_n) \in F^n$, $y = (y_1, \dots, y_n) \in F^n$, and
  $c \in F$, we define the operations of addition and scalar multiplication by
  \begin{equation*}
    x + y = (x_1 + y_1, \dots, x_n + y_n)
    \quad \text{and} \quad
    c \cdot x = (c \cdot x_1, \dots, c \cdot x_n).
  \end{equation*}
  Then $F^n$ is a vector space over $F$.
\end{example}

\begin{example}
  The set of all $m \times n$ \emph{matrices} with elements from a field $F$ is
  denoted by $F^{m \times n}$.
  For $A, B \in F^{m \times n}$ and $c \in F$, we define the operations of
  addition and scalar multiplication by
  \begin{equation*}
    (A + B)_{ij} = A_{ij} + B_{ij}
    \quad \text{and} \quad
    (c \cdot A)_{ij} = c \cdot A_{ij}
  \end{equation*}
  for $i \in \{1, \dots, m\}$ and $j \in \{1, \dots, n\}$.
  Then $F^{m \times n}$ is a vector space over $F$.
\end{example}

\begin{example}
  The set of \emph{functions} from a nonempty set $S$ to a field $F$ is denoted
  by $\mathcal{F}(S, F)$.
  For $f, g \in \\mathcal{F}(S, F)$ and $c \in F$, we define the operations of
  addition and scalar multiplication by
  \begin{equation*}
    (f + g)(s) = f(s) + g(s)
    \quad \text{and} \quad
    (c \cdot f)(s) = c \cdot f(s)
  \end{equation*}
  for all $s \in S$.
  Then $\\mathcal{F}(S, F)$ is a vector space over $F$.
\end{example}

\begin{example}
  The set of \emph{polynomials} with coefficients from a field $F$ is denoted
  by $\mathcal{P}(F)$.
  For $f, g \in \mathcal{P}(F)$ and $c \in F$ with
  \begin{equation*}
    f(t) = \sum_{i=0}^n a_it^i
    \quad \text{and} \quad
    g(t) = \sum_{i=0}^n b_it^i,
  \end{equation*}
  we define the operations of addition and scalar multiplication by
  \begin{equation*}
    (f + g)(t) = \sum_{i=0}^n (a_i + b_i)t^i
    \quad \text{and} \quad
    (c \cdot f)(t) = \sum_{i=0}^n (c \cdot a_i)t^i.
  \end{equation*}
  Then $\mathcal{P}(F)$ is a vector space over $F$.
\end{example}

\begin{proposition}\label{prop:vector-space-addition}
  Let $V$ be a vector space with $x, y, z \in F$.
  \begin{enumerate}
    \item If $x + y = x + z$, then $y = z$.
    \item If $x + y = x$, then $y = 0_V$.
    \item If $x + y = 0_V$, then $y = -x$.
    \item $-(-x) = x$.
  \end{enumerate}
\end{proposition}
\begin{proof}
  The proof is omitted since it is similar to that of
  \Cref{prop:field-addition}.
\end{proof}

\begin{proposition}\label{prop:vector-space-operation}
  Let $V$ be a vector space over a field $F$ with $x \in V$ and $a \in F$.
  \begin{enumerate}
    \item $0_F \cdot x = 0_V$.
    \item $a \cdot 0_V = 0_V$.
    \item $(-a) \cdot x = -(a \cdot x) = a \cdot (-x)$.
  \end{enumerate}
\end{proposition}
\begin{proof}
  The proof is omitted since it is similar to that of
  \Cref{prop:field-operation}.
\end{proof}

\section{Subspaces}
\begin{definition}\label{def:subspace}
  Let $V$ be a vector space over a field $F$.
  Then a subset $W$ of $V$ is called a \emph{subspace} of $V$ if $W$ is a
  vector space over $F$ with the operations of addition and scalar
  multiplication defined on $V$.
\end{definition}

\begin{theorem}\label{thm:subspace}
  Let $V$ be a vector space over a field $F$ and $W \subseteq V$.
  Then $W$ is a subspace of $V$ if the following conditions hold.
  \begin{enumerate}
    \item $0_V \in W$.
    \item $x + y \in W$ for all $x, y \in W$.
    \item $ax \in W$ for all $x \in W$ and $a \in F$.
  \end{enumerate}
\end{theorem}
\begin{proof}
  Since a vector in $W$ is also in $V$, (V 2), (V 3), (V 7), (V 8), (V 9) and
  (V 10) in \Cref{def:vector-space} hold trivially.
  Furthermore, (a) implies (V 4), (b) implies (V 1), (c) implies (V 6), and
  (V 5) is also true since
  \begin{equation*}
    -x = -(1_Fx) = (-1_F)x \in W
  \end{equation*}
  holds for all $x \in W$.
  Thus, $W$ is a vector space over $F$.
\end{proof}

\begin{corollary}\label{cor:subspace}
  Let $V$ be a vector space over a field $F$ and $W \subseteq V$.
  Then $W$ is a subspace of $V$ if and only if the following conditions hold.
  \begin{enumerate}
    \item $0_V \in W$.
    \item $ax + y \in W$ for all $x, y \in W$ and $a \in F$.
  \end{enumerate}
\end{corollary}
\begin{proof}
  $(\Rightarrow)$ Straightforward.
  $(\Leftarrow)$ For all $x, y \in W$ and $a \in F$, we have
  \begin{equation*}
    x + y = 1_F x + y \in W
    \quad \text{and} \quad
    ax = ax + 0_V \in W.
  \end{equation*}
  Thus, $W$ is a subspace of $V$ by \Cref{thm:subspace}.
\end{proof}

\begin{example}
  The set of polynomials in $\mathcal{P}(F)$ with degree not greater than
  $n$ is denoted by $\mathcal{P}_n(F)$, where the \emph{degree} of a nonzero
  polynomial
  \begin{equation*}
    f(t) = a_0 + a_1t + a_2t^2 + \cdots + a_mt^m
  \end{equation*}
  is defined to be the largest integer $n$ such that $a_n \neq 0_F$, and the
  degree of zero polynomial is defined to be $-1$.
  Then one can verify that $\mathcal{P}_n(F)$ is a subspace of
  $\mathcal{P}(F)$.
\end{example}

\begin{example}
  An $n \times n$ matrix $A$ is called \emph{diagonal} if $i \neq j$ impies
  $A_{ij} = 0_F$ for all $i, j \in \{1, \dots, n\}$.
  Then one can verify that the set of $n \times n$ diagonal matrices is a
  subspace of $F^{n \times n}$.
\end{example}

\begin{example}
  The \emph{trace} of an $n \times n$ matrix $A$, denoted by $\tr(A)$, is
  defined by
  \begin{equation*}
    \tr(A) = \sum_{i=1}^n A_{ii}.
  \end{equation*}
  Then one can verify that the set of $n \times n$ matrices that have trace
  equal to $0_F$ is a subspace of $F^{n \times n}$.
\end{example}

\begin{proposition}\label{prop:subspace-intersection}
  Let $V$ be a vector space and let $W_1$ and $W_2$ be subspaces of $V$.
  Then $W_1 \cap W_2$ is a subspace of $V$.
\end{proposition}
\begin{proof}
  Since $W_1$ and $W_2$ are subspaces of $V$, we have $0_V \in W_1 \cap W_2$.
  Furthermore, for each $x, y \in W_1 \cap W_2$ and for each $a \in F$, we have
  $ax + y \in W_1 \cap W_2$ by \Cref{cor:subspace}.
  Thus, $W_1 \cap W_2$ is a subspace of $V$.
\end{proof}

\begin{example}
  Let $W_1$ be the set of $n \times n$ diagonal matrices.
  Let $W_2$ be the set of $n \times n$ matrices that have trace equal to $0_F$.
  Then since both $W_1$ and $W_2$ are subspaces of $F^{n \times n}$,
  we can conclude that $W_1 \cap W_2$ is also a subspace of $F^{n \times n}$.
\end{example}

\begin{definition}\label{def:sum}
  Let $V$ be a vector space and let $S_1, S_2 \subseteq V$.
  Then the \emph{sum} of $S_1$ and $S_2$, denoted by $S_1 + S_2$, is the set
  \begin{equation*}
    \{x + y: x \in S_1\ \text{and}\ y \in S_2\}.
  \end{equation*}
\end{definition}

\begin{proposition}\label{prop:subspace-sum}
  Let $V$ be a vector space and let $W_1$ and $W_2$ be subspaces of $V$.
  Then the following statements are true.
  \begin{enumerate}
    \item $W_1 + W_2$ is a subspace of $V$.
    \item If $U$ is a subspace of $V$ with $W_1 \cup W_2 \subseteq U$,
      then $W_1 + W_2 \subseteq U$.
  \end{enumerate}
\end{proposition}
\begin{proof} \leavevmode
  \begin{enumerate}
    \item We have $0_V = 0_V + 0_V \in W_1 + W_2$.
      For each $x, y \in W_1 + W_2$ and for each $a \in F$, by \Cref{def:sum}
      there exist $x_1, y_1 \in W_1$ and $x_2, y_2 \in W_2$ such that
      $x = x_1 + x_2$ and $y = y_1 + y_2$.
      Thus,
      \begin{align*}
        ax + y
        &= a(x_1 + x_2) + (y_1 + y_2) \\
        &= (ax_1 + ax_2) + (y_1 + y_2) \\
        &= (ax_1 + y_1) + (ax_2 + y_2) \\
        &\in W_1 + W_2.
      \end{align*}
    \item Let $x$ be a vector in $W_1 + W_2$.
      Then by \Cref{def:sum} there exists $x_1 \in W_1$ and $x_2 \in W_2$ such
      that $x = x_1 + x_2$.
      We have $x_1 \in U$ since $W_1 \subseteq U$.
      Also, we have $x_2 \in U$ since $W_2 \subseteq U$.
      It follows that $x = x_1 + x_2 \in U$, and thus $W_1 + W_2 \subseteq U$.
      \qedhere
  \end{enumerate}
\end{proof}

\section{Spanning Sets}
\begin{definition}\label{def:linear-combination}
  Let $V$ be a vector space over a field $F$ and let $S \subseteq V$.
  Then a vector $x \in V$ is called a \emph{linear combination} of $S$ if
  there exist scalars $a_1, \dots, a_n \in F$ and vectors
  $x_1, \dots, x_n \in S$ for some nonnegative integer $n$ such that
  \begin{equation*}
    x = \sum_{i=1}^n a_ix_i.
  \end{equation*}
\end{definition}
\begin{remark}
  \leavevmode
  \begin{itemize}
    \item If $n = 0$, then the sum in the right hand side is $0_V$ since
      nothing are added up.
      Thus, $0_V$ is a linear combination of any subset of $V$.
    \item Note that $n$ should be finite. Thus, in the vector space
      $\mathbb{R}$ over the field $\mathbb{Q}$, $e$ is not a linear combination
      of $\mathbb{Q}$ even if we have
      \begin{equation*}
        e = \sum_{i=0}^\infty \frac{1}{i!}.
      \end{equation*}
  \end{itemize}
\end{remark}

\begin{definition}\label{def:span}
  Let $V$ be a vector space over a field $F$ and let $S \subseteq V$.
  Then the \emph{span} of $S$, denoted $\spn(S)$, is defined as the
  set of all linear combinations of $S$.
\end{definition}

\begin{theorem}\label{thm:span}
  Let $V$ be a vector space over $F$ and let $S \subseteq V$.
  Then the following statements are true.
  \begin{enumerate}
    \item $\spn(S)$ is a subspace of $V$.
    \item If $U$ is a subspace of $V$ such that $S \subseteq U$, then
      $\spn(S) \subseteq U$.
  \end{enumerate}
\end{theorem}
\begin{proof}
  \leavevmode
  \begin{enumerate}
    \item Let $c \in F$ and $x, y \in \spn(S)$.
      Then there exist scalars $a_1, \dots, a_n \in F$
      and vectors $x_1, \dots, x_n \in S$ such that
      \begin{equation*}
        x = a_1x_1 + \cdots + a_nx_n.
      \end{equation*}
      Also, there exist scalars $b_1, \dots, b_n \in F$
      and vectors $y_1, \dots, y_m \in S$ such that
      \begin{equation*}
        y = b_1y_1 + \cdots + b_ny_m.
      \end{equation*}
      Thus, we have
      \begin{align*}
        cx + y
        &= c(x_1 + \cdots + x_n) + (y_1 + \cdots + y_m) \\
        &= cx_1 + \cdots + cx_n + y_1 + \cdots + y_m \\
        &\in \spn(S).
      \end{align*}
      Furthermore, $0_V \in \spn(S)$.
      Hence, $\spn(S)$ is a subspace of $V$ by \Cref{cor:subspace}.
    \item Let $x \in \spn(S)$.
      Then there exist scalars $a_1, \dots, a_n \in F$
      and vectors $x_1, \dots, x_n \in S$ such that
      \begin{equation*}
        x = a_1x_1 + \cdots + a_nx_n.
      \end{equation*}
      Since $S \subseteq U$, we have $x_1, \dots, x_n \in U$, and it follows that
      $x = a_1x_1 + \cdots + a_nx_n \in U$ due to the closeness of $U$.
      Thus, $\spn(S) \subseteq U$. \qedhere
  \end{enumerate}
\end{proof}

\begin{definition}\label{def:spanning-set}
  Let $V$ be a vector space and let $S \subseteq V$.
  If $\spn(S) = V$, then $S$ is called a \emph{spanning set} of $V$, and we
  also say $S$ \emph{spans} $V$.
\end{definition}

\begin{example}
  $\{(0, 1, 1), (1, 0, 1), (1, 1, 0)\}$ is a spanning set of $\mathbb{R}^3$
  since for any $x, y, z \in \mathbb{R}$,
  \begin{equation*}
    (x, y, z)
      = \frac{-x+y+z}{2} \cdot (0, 1, 1)
      + \frac{x-y+z}{2} \cdot (1, 0, 1)
      + \frac{x+y-z}{2} \cdot (1, 1, 0).
  \end{equation*}
\end{example}

\begin{proposition}\label{prop:span}
  Let $V$ be a vector space and let $R, S \subseteq V$.
  \begin{enumerate}
    \item $S \subseteq \spn(S)$.
    \item If $R \subseteq S$, then $\spn(R) \subseteq \spn(S)$.
    \item $S = \spn(S)$ if and only if $S$ is a subspace of $V$.
    \item $\spn(R \cup S) = \spn(R) + \spn(S)$.
  \end{enumerate}
\end{proposition}
\begin{proof}
  \leavevmode
  \begin{enumerate}
    \item Straightforward.
    \item It is true since a linear combination of a subset of $S$ is also a
      linear combination of $S$.
    \item $(\Rightarrow)$ Straightforward from \Cref{thm:span} (a).

      $(\Leftarrow)$ Note that any linear combination of $S$ is in $S$ due to
      closeness of addition and scalar multiplication in $S$.
      Thus, $\spn(S) \subseteq S$, and it follows that $S = \spn(S)$.
    \item Since $R \subseteq \spn(R)$ and $S \subseteq \spn(S)$, we have
      $R \cup S \subseteq \spn(R) + \spn(S)$. Thus, by \Cref{thm:span}, we have
      $\spn(R \cup S) \subseteq \spn(R) + \spn(S)$.
      On the other side, since
      \begin{equation*}
        \spn(R) \subseteq \spn(R \cup S)
        \quad \text{and} \quad
        \spn(S) \subseteq \spn(R \cup S),
      \end{equation*}
      we can conclude that $\spn(R) \cup \spn(S) \subseteq \spn(R \cup S)$.
      Thus, $\spn(R) + \spn(S) \subseteq \spn(R \cup S)$
      by \Cref{prop:subspace-sum}. \qedhere
  \end{enumerate}
\end{proof}

\section{Linearly Independent Sets}
\begin{definition}\label{def:linear-independence}
  Let $V$ be a vector space over a field $F$ and let $S \subseteq V$.
  \begin{itemize}
    \item $S$ is \emph{linearly dependent} if there exist scalars
      $a_1, a_2, \dots, a_n \in F \setminus \{0_F\}$ and distinct vectors
      $x_1, x_2, \dots, x_n \in S$ for some positive integer $n$ such that
      \begin{equation*}
        a_1x_1 + a_2x_2 + \cdots + a_nx_n = 0_V.
      \end{equation*}
    \item $S$ is \emph{linearly independent} if it is not linearly dependent.
  \end{itemize}
\end{definition}

\begin{theorem}\label{thm:linear-dependence-equivalence}
  Let $V$ be a vector space over a field $F$ and let $S \subseteq V$.
  Then the following statements are equivalent.
  \begin{enumerate}
    \item $S$ is linearly dependent.
    \item There exists $x \in S$ with $x \in \spn(S \setminus \{x\})$.
    \item There exists $x \in S$ with $\spn(S) = \spn(S \setminus \{x\})$.
  \end{enumerate}
\end{theorem}
\begin{proof}
  \leavevmode
  \begin{enumerate}[(i)]
    \item First we assume (a) and prove (b). Suppose that
      \begin{equation*}
        a_0x_0 + a_1x_1 + \cdots + a_nx_n = 0_V,
      \end{equation*}
      where $a_0, a_1, \dots, a_n$ are nonzero scalars and
      $x_0, x_1, \dots, x_n$ are distinct vectors. Then
      \begin{align*}
        x_0
        &= (-a_0)^{-1}(a_1x_1 + \cdots + a_nx_n) \\
        &= ((-a_0)^{-1}a_1)x_1 + \cdots + ((-a_0)^{-1}a_n)x_n \\
        &\in \spn(S \setminus \{x_0\}).
      \end{align*}
    \item Then we assume (b) and prove (c). Since
      \begin{equation*}
        x \in \spn(S \setminus \{x\})
        \quad \text{and} \quad
        S \setminus \{x\} \subseteq \spn(S \setminus \{x\}),
      \end{equation*}
      we have $S \subseteq \spn(S \setminus \{x\})$.
      Thus, $\spn(S) \subseteq \spn(S \setminus \{x\})$ by \Cref{thm:span},
      and we can conclude that $\spn(S) = \spn(S \setminus \{x\})$.
    \item Then we assume (c) and prove (b). It is straightforward
      since $x \in S \subseteq \spn(S) = \spn(S \setminus \{x\})$.
    \item Finally we assume (b) and prove (a).
      Without loss of generality, let $a_1, \dots, a_n \in F$ be nonzero
      scalars and $x_1, \dots, x_n \in S \setminus \{x\}$ be distinct vectors
      such that $x = a_1x_1 + \cdots + a_nx_n$. Then we have
      \begin{equation*}
        (-1_F)x + a_1x_1 + \cdots + a_nx_n = 0_V,
      \end{equation*}
      which completes the proof. \qedhere
  \end{enumerate}
\end{proof}

\begin{example}
  Let $S = \{(0, 1, 1), (1, 0, 1), (1, 1, 0)\}$ be a subset of $\mathbb{R}^3$.
  Suppose that $a_1, a_2, a_3 \in \mathbb{R}$ are scalars such that
  \begin{equation*}
    a_1(0, 1, 1) + a_2(1, 0, 1) + a_3(1, 1, 0) = (0, 0, 0).
  \end{equation*}
  Then we have the following system of equations.
  \begin{equation*}
    \setlength\arraycolsep{0pt}
    \begin{array}{r>{{}}c<{{}}r>{{}}c<{{}}r@{{}={}}r}
          & & a_2 &+& a_3 & 0 \\
      a_1 & &     &+& a_3 & 0 \\
      a_1 &+& a_2 & &     & 0 \\
    \end{array}
  \end{equation*}
  Since the only solution to this system of equations is $a_1 = a_2 = a_3 = 0$,
  we can conclude that $S$ is linearly independent by
  \Cref{def:linear-independence}.
\end{example}

\begin{example}
  Let $S = \{(1, 1, 1), (0, 1, 1), (1, 0, 1), (1, 1, 0)\}$ be a subset of
  $\mathbb{R}^3$.
  We can conclude that $S$ is linearly dependent since
  \begin{equation*}
    (1, 1, 1)
    = \frac{1}{2} \cdot (0, 1, 1) + \frac{1}{2} \cdot (1, 0, 1)
      + \frac{1}{2} \cdot (1, 1, 0).
  \end{equation*}
\end{example}

\begin{proposition}\label{prop:linear-independence-subset}
  Let $V$ be a vector space and let $R, S$ be subsets of $V$ with
  $R \subseteq S$.
  \begin{enumerate}
    \item If $R$ is linearly dependent, then so is $S$.
    \item If $S$ is linearly independent, then so is $R$.
  \end{enumerate}
\end{proposition}
\begin{proof}
  \leavevmode
  \begin{enumerate}
    \item Suppose that $R$ is linearly dependent. Then by
      \Cref{def:linear-independence} there exists $x \in R$
      such that $x \in \spn(R \setminus \{x\})$. Also, we have
      $R \setminus \{x\} \subseteq S \setminus \{x\}$ since $R \subseteq S$.
      Thus, $x \in \spn{S \setminus \{x\}}$, and it follows that $S$ is
      linearly dependent.
    \item Straightforward from (a). \qedhere
  \end{enumerate}
\end{proof}

\begin{theorem}\label{thm:linearly-independent-subset}
  Let $V$ be a vector space and let $S$ be a finite subset of $V$.
  Then there exists a linearly independent subset $Q$ of $S$ such that
  $\spn(Q) = \spn(S)$.
\end{theorem}
\begin{proof}
  The proof is by induction on $|S|$.
  The induction begins with $|S| = 0$, i.e., $S = \varnothing$.
  Since $\varnothing$ is linearly independent, we can choose
  $Q = \varnothing$, and thus the theorem holds.

  Now suppose that the theorem is true for $|S| = n$, where $n$ is a
  nonnegative integer, and we prove that the theorem holds for $|S| = n + 1$.
  If $S$ is linearly independent, then we can choose $Q = S$.
  Otherwise, there exists $x \in S$ with $\spn(S) = \spn(S \setminus \{x\})$.
  Then there is a linearly independent set $Q \subseteq S \setminus \{x\}$
  such that $\spn(Q) = \spn(S \setminus \{x\})$ by induction hypothesis,
  implying $Q \subseteq S$ and $\spn(Q) = \spn(S)$, which completes the proof.
\end{proof}

\section{Bases and Dimension}
\begin{definition}\label{def:basis}
  Let $(V, +, \cdot)$ be a vector space. A subset $S$ of $V$ is a \emph{basis}
  of $V$ if $S$ is not only a spanning set but also a linearly independent set
  of $V$.
\end{definition}

\begin{example}
  Following are some examples of bases.
  \begin{itemize}
    \item Since $\mathrm{span}(\varnothing) = \{0_V\}$ and $\varnothing$ is
      linearly independent, $\varnothing$ is a basis of $\{0_V\}$.
    \item Let $S = \{x_1, \dots, x_n\}$ be a subset of $F^n$ with
      $(x_i)_j = [\![i = j]\!]$ for all $i, j \in \{1, \dots, n\}$.
      Then $S$ is called the \emph{standard basis} of $F^n$.
    \item The set $S = \{1_F, x, x^2, \dots, x^n\}$ is the called the
      \emph{standard basis} of $\mathcal{P}_n(F)$.
  \end{itemize}
\end{example}

\begin{theorem}\label{thm:finite-basis-existence}
  Let $(V, +, \cdot)$ be a vector space over $F$.
  If there exists a finite set $S$ that spans $V$, then there is a subset
  $Q$ of $S$ that is a finite basis of $V$.
\end{theorem}
\begin{proof}
  By \Cref{thm:linearly-independent-subset}, there exists
  a linearly independent set $Q \subseteq S$
  such that $\mathrm{span}(Q) = \mathrm{span}(S) = V$.
  Thus, $Q$ is a finite basis of $V$.
\end{proof}

\begin{theorem}[Replacement Theorem]\label{thm:replacement}
  Let $(V, +, \cdot)$ be a vector space over $F$.
  Let $S$ be a finite set that spans $V$,
  and let $Q \subseteq V$ be a finite linearly independent set.
  Then $|Q| \leq |S|$, and there exists $R \subseteq S \setminus Q$ such that
  both $|Q \cup R| = |S|$ and $\mathrm{span}(Q \cup R) = V$ hold.
\end{theorem}
\begin{proof}
  The proof is based on induction on $|Q|$.
  The induction begins with $|Q| = 0$, i.e., $Q = \varnothing$.
  Choosing $R = S$, we have $Q \cup R = S$, and thus
  both $|Q \cup R| = S$ and $\mathrm{span}(Q \cup R) = V$ hold.

  Now suppose that the theorem is true for $|Q| = m$ with $m \geq 0$,
  and we prove that the theorem holds for $|Q|= m + 1$.
  Let $Q = \{x_1, \dots, x_{m+1}\}$ and let $Q' = Q \setminus \{x_{m+1}\}$.
  By induction hypothesis, there exists
  $R' = \{y_1, \dots, y_k\} \subseteq S \setminus Q'$
  such that $m + k = |S|$ and $\mathrm{span}(Q' \cup R') = V$.
  Since $Q' \cup R'$ spans $V$, there exists
  $a_1, \dots, a_m, b_1, \dots, b_k \in F$ such that
  \begin{equation*}
    x_{m+1} = \sum_{i=1}^m a_ix_i + \sum_{j=1}^k b_jy_j.
  \end{equation*}
  If $b_j = 0_F$ for all $j \in \{1, \dots, k\}$, then $x_{m+1}$ is a linear
  combination of $Q$, implying that $Q$ is linearly dependent, contradiction.
  Thus, there must exist some $j \in \{1, \dots, k\}$
  such that $b_j \neq 0_F$.
  Without loss of generality let $b_k \neq 0_F$.
  Also, let $R = \{y_1, \dots, y_{k-1}\}$.
  Then $|Q \cup R| = (m+1) + (k-1) = |S|$.
  Since $k \geq 1$, we have $|Q| \leq |S|$.
  Note that $(Q' \cup R') \setminus (Q \cup R) = \{y_k\}$.
  By
  \begin{equation*}
    y_k = (-b_k)^{-1}\left(
       \sum_{i=1}^m a_ix_i + (-1_F)x_{m+1} + \sum_{j=1}^{k-1} b_jy_j
    \right) \in \mathrm{span}(Q \cup R),
  \end{equation*}
  we have
  \begin{equation*}
    Q' \cup R'
      \subseteq Q \cup R \cup \{y_k\}
      \subseteq \mathrm{span}(Q \cup R).
  \end{equation*}
  Thus, by \Cref{thm:span} we have
  \begin{equation*}
    V = \mathrm{span}(Q' \cup R')
      \subseteq \mathrm{span}(Q \cup R)
      \subseteq V,
  \end{equation*}
  implying $\mathrm{span}(Q \cup R) = V$.
\end{proof}

\begin{corollary}
  Let $(V, +, \cdot)$ be a vector space over $F$ that is spanned by a finite
  set. Then every linearly independent subset of $V$ is finite.
\end{corollary}
\begin{proof}
  Suppose that $S$ is a finite spanning set of $V$ and that $Q$ is linearly
  independent.
  If $Q$ is infinite, then there exists $Q' \subseteq Q$ with
  $|Q'| = |S| + 1$.
  It follows that $Q'$ is linearly independent by
  \Cref{thm:linear-independence-implication}, and thus $|Q'| \leq |S|$ by
  \Cref{thm:replacement}, contradiction to $|Q'| = |S| + 1$.
  Therefore, $Q$ is finite.
\end{proof}

\begin{theorem}\label{thm:dimension}
  Let $(V, +, \cdot)$ be a vector space over $F$.
  If $V$ has a finite basis, then all bases of $V$ have the same size.
\end{theorem}
\begin{proof}
  Let $S$ be a finite basis of $V$ and let $Q$ be an arbitrary basis of $V$.
  Since $V = \mathrm{span}(S)$ and $Q$ is linearly independent, it
  follows that $Q$ is finite, and thus $|Q| \leq |S|$ by
  replacement theorem (\Cref{thm:replacement}).

  Also, since $V = \mathrm{span}(Q)$ and $S$ is linearly independent,
  we have $|S| \leq |Q|$ by replacement theorem (\Cref{thm:replacement}).
  Thus, $|Q| = |S|$.
\end{proof}

\begin{definition}
  A vector space $(V, +, \cdot)$ over $F$ is called \emph{finite-dimensional}
  if it has a finite basis.
  A vector space that is not finite-dimensional is called
  \emph{infinite-dimensional}.
\end{definition}

\begin{definition}
  The number of vectors in each basis of a finite-dimensional vector space $V$
  is called the \emph{dimension} of $V$ and is denoted by $\mathrm{dim}(V)$.
\end{definition}

\begin{example}
  We have $\mathrm{dim}(\{0_V\}) = 0$, $\mathrm{dim}(F^n) = n$, and
  $\mathrm{dim}(\mathcal{P}_n(F)) = n + 1$.
\end{example}

\begin{example}
  The dimension of a vector space depends on its field of scalars.
  \begin{itemize}
    \item If $V = \mathbb{C}$ is a vector space over $\mathbb{R}$, then
      $\mathrm{dim}(V) = 2$ since $\{1, i\}$ is a basis of $V$.
    \item If $W = \mathbb{C}$ is a vector space over $\mathbb{C}$, then
      $\mathrm{dim}(W) = 1$ since $\{1\}$ is a basis of $W$.
  \end{itemize}
\end{example}

\begin{theorem}\label{thm:basis-equivalence}
  Let $(V, +, \cdot)$ be a vector space over $F$.
  Then a subset of $V$ of $n = \mathrm{dim}(V)$ vectors is linearly
  independent if and only if it is a spanning set of $V$.
\end{theorem}
\begin{proof}
  ($\Rightarrow$) Suppose that $Q$ is linearly independent with $|Q| = n$.
  By replacement theorem (\Cref{thm:replacement}), there exists
  $R \subseteq S \setminus Q$ such that $|Q \cup R| = |S|$ and
  $\mathrm{span}(Q \cup R) = V$.
  Since $|Q| = |S|$, we have $|R| = 0$, i.e., $R = \varnothing$.
  Thus, $\mathrm{span}(Q) = V$.

  ($\Leftarrow$) Suppose that $S$ spans $V$ with $|S| = n$.
  By \Cref{thm:finite-basis-existence}, there is a subset $Q$ of $S$
  that is a basis of $V$.
  Then we have $|Q| = n$, implying $Q = S$.
  Thus, $S$ is a basis of $V$.
\end{proof}

\begin{theorem}\label{thm:subspace-dimension}
  Let $(V, +, \cdot)$ be a finite-dimensional vector space over $F$,
  and let $V'$ be a subspace of $V$. Then the following statements hold.
  \begin{enumerate}
    \item $\mathrm{dim}(V') \leq \mathrm{dim}(V)$.
    \item If $\mathrm{dim}(V') = \mathrm{dim}(V)$, then $V' = V$.
  \end{enumerate}
\end{theorem}
\begin{proof}
  Let $S$ be a basis of $V$ and let $S'$ be a basis of $V'$.
  \begin{enumerate}
    \item Since $S'$ is linearly independent and $V = \mathrm{span}(S)$,
      we have $|S'| \leq |S|$ by replacement theorem (\Cref{thm:replacement}).
      Thus, $\mathrm{dim}(V') \leq \mathrm{dim}(V)$.
    \item Since $S'$ is linearly independent and $|S'| = \mathrm{dim}(V)$,
      we have $\mathrm{span}(S') = V$ by \Cref{thm:basis-equivalence}.
      Thus, $V' = \mathrm{span}(S') = V$. \qedhere
  \end{enumerate}
\end{proof}