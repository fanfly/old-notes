\chapter{Vector Spaces}
\section{Fields}
\begin{definition}\label{def:field}
  A \emph{field} is a set $F$ with two operations, called \emph{addition}
  (denoted by $+$) and \emph{multiplication} (denoted by $\cdot$), which
  satisfy the following axioms.
  \begin{enumerate}[leftmargin=3.5em]
    \item[(A 1)] If $a \in F$ and $b \in F$, then $a + b \in F$.
    \item[(A 2)] $a + b = b + a$ for all $a, b \in F$.
    \item[(A 3)] $(a + b) + c = a + (b + c)$ for all $a, b, c \in F$.
    \item[(A 4)] There is an element $0_F$ in $F$ such that $0_F + a = a$ for
      all $a \in F$. 
    \item[(A 5)] For each $a \in F$ there is an element $-a$ in $F$ such that
      $a + (-a) = 0_F$.
    \item[(M 1)] If $a \in F$ and $b \in F$, then $a \cdot b \in F$.
    \item[(M 2)] $a \cdot b = b \cdot a$ for all $a, b \in F$.
    \item[(M 3)] $(a \cdot b) + c = a + (b \cdot c)$ for all $a, b, c \in F$.
    \item[(M 4)] There is an element $1_F$ in $F \setminus \{0_F\}$ such that
      $1_F \cdot a = a$ for all $a \in F$.
    \item[(M 5)] For each $a \in F \setminus \{0_F\}$ there is an element
      $a^{-1}$ in $F$ such that $a \cdot a^{-1} = 1_F$.
    \item[(D)] $a \cdot (b + c) = a \cdot b + a \cdot c$ for all
      $a, b, c \in F$.
  \end{enumerate}
\end{definition}
\begin{remark} \leavevmode
  \begin{itemize}
    \item For simplification, we usually write $ab$ instead of $a \cdot b$.
    \item The axioms labeled with ``A'' and ``M'' are usually called the
      \emph{axioms of addition} and the \emph{axioms of multiplication},
      respectively. The axiom labeld with ``D'' is the \emph{distributive law}.
    \item The elements $0_F$ and $1_F$ are usually called the
      \emph{additive identity} and the \emph{multiplicative identity} of $F$,
      respectively.
      Also, $-a$ and $a^{-1}$ are called the \emph{additive inverse} and the
      \emph{multiplicative inverse} of $a$, respectively.
    \item \emph{Subtraction} and \emph{division} can be defined using additive
      and multiplicative inverses.
  \end{itemize}
\end{remark}

\begin{example}
  $\mathbb{Q}$, $\mathbb{R}$ and $\mathbb{C}$ are fields.
\end{example}

\begin{example}
  Let $\mathbb{B} = \{0, 1\}$ and the operations $\oplus$ and $\odot$ are
  defined as follows.
  \begin{align*}
    \begin{array}{c|cc}
      \oplus & 0 & 1 \\
      \hline
      0      & 0 & 1 \\
      1      & 1 & 0 \\
    \end{array}
    \quad
    \begin{array}{c|cc}
      \odot & 0 & 1 \\
      \hline
      0     & 0 & 0 \\
      1     & 0 & 1 \\
    \end{array}
  \end{align*}
  Then $\mathbb{B}$ is a field with $\oplus$ and $\odot$ as addition and
  multiplication, respectively.
\end{example}

\begin{proposition}\label{prop:field-addition}
  Let $F$ be a field with $a, b, c \in F$.
  \begin{enumerate}
    \item If $a + b = a + c$, then $b = c$.
    \item If $a + b = a$, then $b = 0_F$.
    \item If $a + b = 0_F$, then $b = -a$.
    \item $-(-a) = a$.
  \end{enumerate}
\end{proposition}
\begin{proof} \leavevmode
  \begin{enumerate}
    \item It can be proved by
      \begin{align*}
        b
        &= 0_F + b      \\
        &= (-a + a) + b \\
        &= -a + (a + b) \\
        &= -a + (a + c) \\
        &= (-a + a) + c \\
        &= 0_F + c      \\
        &= c.
      \end{align*}
    \item By applying (a), it follows from $a + b = a + 0_F$ that $b = 0_F$.
    \item By applying (a), it follows from $a + b = a + (-a)$ that $b = -a$.
    \item Since $-a + a = 0_F$, we have $a = -(-a)$ by (c). \qedhere
  \end{enumerate}
\end{proof}

\begin{proposition}\label{prop:field-multiplication}
  Let $F$ be a field with $a, b, c \in F$ and $a \neq 0_F$.
  \begin{enumerate}
    \item If $a \cdot b = a \cdot c$, then $b = c$.
    \item If $a \cdot b = a$, then $b = 1_F$.
    \item If $a \cdot b = 1_F$, then $b = a^{-1}$.
    \item $(a^{-1})^{-1} = a$.
  \end{enumerate}
\end{proposition}
\begin{proof}
  The proof is omitted since it is similar to that of
  \Cref{prop:field-addition}.
\end{proof}

\begin{proposition}\label{prop:field-operation}
  Let $F$ be a field with $a, b \in F$.
  \begin{enumerate}
    \item $0_F \cdot a = 0_F$.
    \item $(-a) \cdot b = -(a \cdot b) = a \cdot (-b)$.
    \item $(-a) \cdot (-b) = a \cdot b$.
  \end{enumerate}
\end{proposition}
\begin{proof} \leavevmode
  \begin{enumerate}
    \item Since
      \begin{equation*}
        0_F \cdot a + 0_F \cdot a
        = (0_F + 0_F) \cdot a
        = 0_F \cdot a,
      \end{equation*}
      we have $0_F \cdot a = 0_F$ by \Cref{prop:field-addition} (b).
    \item Since
      \begin{equation*}
        (-a) \cdot b + a \cdot b
        = (-a + a) \cdot b
        = 0_F \cdot b
        = 0_F,
      \end{equation*}
      we have $(-a) \cdot b = -(a \cdot b)$ by \Cref{prop:field-addition} (c).
      The other half can be proved similarly.
    \item By applying (b) twice, we have
      \begin{equation*}
        (-a) \cdot (-b)
        = -(a \cdot (-b))
        = -(-(a \cdot b))
        = a \cdot b. \qedhere
      \end{equation*}
  \end{enumerate}
\end{proof}

\section{Vector Spaces}
\begin{definition}\label{def:vector-space}
  A \emph{vector space} over a field $F$ is a set $V$ with two operations,
  called \emph{addition} (denoted by $+$) and \emph{scalar multiplication}
  (denoted by $\cdot$), which satisfy the following axioms.
  \begin{enumerate}[leftmargin=3.5em]
    \item[(V 1)] If $x \in V$ and $y \in V$, then $x + y \in V$.
    \item[(V 2)] $x + y = y + x$ for all $x, y \in V$. 
    \item[(V 3)] $(x + y) + z = x + (y + z)$ for all $x, y, z \in V$.
    \item[(V 4)] There is an element $0_V$ in $V$ such that $0_V + x = x$ for
      all $x \in V$.
    \item[(V 5)] For each $x \in V$ there is an element $-x$ such that
      $x + (-x) = 0_V$.
    \item[(V 6)] If $a \in F$ and $x \in V$, then $a \cdot x \in V$. 
    \item[(V 7)] $(a \cdot b) \cdot x = a \cdot (b \cdot x)$ for all
      $a, b \in F$ and $x \in V$. 
    \item[(V 8)] $1_F \cdot x = x$ for all $x \in V$.
    \item[(V 9)] $a \cdot (x + y) = a \cdot x + a \cdot y$ for all $a \in F$
      and $x, y \in V$.
    \item[(V 10)] $(a + b) \cdot x = a \cdot x + b \cdot x$ for all
      $a, b \in F$ and $x \in V$.
  \end{enumerate}
\end{definition}

\begin{remark} \leavevmode
  \begin{itemize}
    \item For simplification, we usually write $ax$ instead of $a \cdot x$.
    \item The elements $0_V$ is usually called the \emph{additive identity} of
      $V$, and $-x$ is called the \emph{additive inverse} of $x$ in $V$.
    \item \emph{Subtraction} can be defined using additive inverses.
  \end{itemize}
\end{remark}

\begin{example}
  A field is a vector space over itself.
\end{example}

\begin{example}
  $\mathbb{C}$ is a vector space over $\mathbb{R}$.
\end{example}

\begin{example}
  $\mathbb{R}$ is a vector space over $\mathbb{Q}$.
\end{example}

\begin{example}
  The set of \emph{$n$-tuples} with elements from a field $F$ is denoted by
  $F^n$.
  For $x = (x_1, \dots, x_n) \in F^n$, $y = (y_1, \dots, y_n) \in F^n$, and
  $c \in F$, we define the operations of addition and scalar multiplication by
  \begin{equation*}
    x + y = (x_1 + y_1, \dots, x_n + y_n)
    \quad \text{and} \quad
    c \cdot x = (c \cdot x_1, \dots, c \cdot x_n).
  \end{equation*}
  Then $F^n$ is a vector space over $F$.
\end{example}

\begin{example}
  The set of all $m \times n$ \emph{matrices} with elements from a field $F$ is
  denoted by $F^{m \times n}$.
  For $A, B \in F^{m \times n}$ and $c \in F$, we define the operations of
  addition and scalar multiplication by
  \begin{equation*}
    (A + B)_{ij} = A_{ij} + B_{ij}
    \quad \text{and} \quad
    (c \cdot A)_{ij} = c \cdot A_{ij}
  \end{equation*}
  for $i \in \{1, \dots, m\}$ and $j \in \{1, \dots, n\}$.
  Then $F^{m \times n}$ is a vector space over $F$.
\end{example}

\begin{example}
  The set of \emph{functions} from a nonempty set $S$ to a field $F$ is denoted
  by $\mathcal{F}(S, F)$.
  For $f, g \in \\mathcal{F}(S, F)$ and $c \in F$, we define the operations of
  addition and scalar multiplication by
  \begin{equation*}
    (f + g)(s) = f(s) + g(s)
    \quad \text{and} \quad
    (c \cdot f)(s) = c \cdot f(s)
  \end{equation*}
  for all $s \in S$.
  Then $\\mathcal{F}(S, F)$ is a vector space over $F$.
\end{example}

\begin{example}
  The set of \emph{polynomials} with coefficients from a field $F$ is denoted
  by $\mathcal{P}(F)$.
  For $f, g \in \mathcal{P}(F)$ and $c \in F$ with
  \begin{equation*}
    f(t) = \sum_{i=0}^n a_it^i
    \quad \text{and} \quad
    g(t) = \sum_{i=0}^n b_it^i,
  \end{equation*}
  we define the operations of addition and scalar multiplication by
  \begin{equation*}
    (f + g)(t) = \sum_{i=0}^n (a_i + b_i)t^i
    \quad \text{and} \quad
    (c \cdot f)(t) = \sum_{i=0}^n (c \cdot a_i)t^i.
  \end{equation*}
  Then $\mathcal{P}(F)$ is a vector space over $F$.
\end{example}

\begin{proposition}\label{prop:vector-space-addition}
  Let $V$ be a vector space with $x, y, z \in F$.
  \begin{enumerate}
    \item If $x + y = x + z$, then $y = z$.
    \item If $x + y = x$, then $y = 0_V$.
    \item If $x + y = 0_V$, then $y = -x$.
    \item $-(-x) = x$.
  \end{enumerate}
\end{proposition}
\begin{proof}
  The proof is omitted since it is similar to that of
  \Cref{prop:field-addition}.
\end{proof}

\begin{proposition}\label{prop:vector-space-operation}
  Let $V$ be a vector space over a field $F$ with $x \in V$ and $a \in F$.
  \begin{enumerate}
    \item $0_F \cdot x = 0_V$.
    \item $a \cdot 0_V = 0_V$.
    \item $(-a) \cdot x = -(a \cdot x) = a \cdot (-x)$.
  \end{enumerate}
\end{proposition}
\begin{proof}
  The proof is omitted since it is similar to that of
  \Cref{prop:field-operation}.
\end{proof}

\section{Subspaces}
\begin{definition}\label{def:subspace}
  Let $V$ be a vector space over a field $F$.
  Then a subset $W$ of $V$ is called a \emph{subspace} of $V$ if $W$ is a
  vector space over $F$ with the operations of addition and scalar
  multiplication defined on $V$.
\end{definition}

\begin{theorem}\label{thm:subspace}
  Let $V$ be a vector space over a field $F$ and $W \subseteq V$.
  Then $W$ is a subspace of $V$ if the following conditions hold.
  \begin{enumerate}
    \item $0_V \in W$.
    \item $x + y \in W$ for all $x, y \in W$.
    \item $ax \in W$ for all $x \in W$ and $a \in F$.
  \end{enumerate}
\end{theorem}
\begin{proof}
  Since a vector in $W$ is also in $V$, (V 2), (V 3), (V 7), (V 8), (V 9) and
  (V 10) in \Cref{def:vector-space} hold trivially.
  Furthermore, (a) implies (V 4), (b) implies (V 1), (c) implies (V 6), and
  (V 5) is also true since
  \begin{equation*}
    -x = -(1_Fx) = (-1_F)x \in W
  \end{equation*}
  holds for all $x \in W$.
  Thus, $W$ is a vector space over $F$.
\end{proof}

\begin{corollary}\label{cor:subspace}
  Let $V$ be a vector space over a field $F$ and $W \subseteq V$.
  Then $W$ is a subspace of $V$ if and only if the following conditions hold.
  \begin{enumerate}
    \item $0_V \in W$.
    \item $ax + y \in W$ for all $x, y \in W$ and $a \in F$.
  \end{enumerate}
\end{corollary}
\begin{proof}
  $(\Rightarrow)$ Straightforward.
  $(\Leftarrow)$ For all $x, y \in W$ and $a \in F$, we have
  \begin{equation*}
    x + y = 1_F x + y \in W
    \quad \text{and} \quad
    ax = ax + 0_V \in W.
  \end{equation*}
  Thus, $W$ is a subspace of $V$ by \Cref{thm:subspace}.
\end{proof}

\begin{example}
  The set of polynomials in $\mathcal{P}(F)$ with degree not greater than
  $n$ is denoted by $\mathcal{P}_n(F)$, where the \emph{degree} of a nonzero
  polynomial
  \begin{equation*}
    f(t) = a_0 + a_1t + a_2t^2 + \cdots + a_mt^m
  \end{equation*}
  is defined to be the largest integer $n$ such that $a_n \neq 0_F$, and the
  degree of zero polynomial is defined to be $-1$.
  Then one can verify that $\mathcal{P}_n(F)$ is a subspace of
  $\mathcal{P}(F)$.
\end{example}

\begin{example}
  An $n \times n$ matrix $A$ is called \emph{diagonal} if $i \neq j$ impies
  $A_{ij} = 0_F$ for all $i, j \in \{1, \dots, n\}$.
  Then one can verify that the set of $n \times n$ diagonal matrices is a
  subspace of $F^{n \times n}$.
\end{example}

\begin{example}
  The \emph{trace} of an $n \times n$ matrix $A$, denoted by $\tr(A)$, is
  defined by
  \begin{equation*}
    \tr(A) = \sum_{i=1}^n A_{ii}.
  \end{equation*}
  Then one can verify that the set of $n \times n$ matrices that have trace
  equal to $0_F$ is a subspace of $F^{n \times n}$.
\end{example}

\begin{proposition}\label{prop:subspace-intersection}
  Let $V$ be a vector space and let $W_1$ and $W_2$ be subspaces of $V$.
  Then $W_1 \cap W_2$ is a subspace of $V$.
\end{proposition}
\begin{proof}
  Since $W_1$ and $W_2$ are subspaces of $V$, we have $0_V \in W_1 \cap W_2$.
  Furthermore, for each $x, y \in W_1 \cap W_2$ and for each $a \in F$, we have
  $ax + y \in W_1 \cap W_2$ by \Cref{cor:subspace}.
  Thus, $W_1 \cap W_2$ is a subspace of $V$.
\end{proof}

\begin{example}
  Let $W_1$ be the set of $n \times n$ diagonal matrices.
  Let $W_2$ be the set of $n \times n$ matrices that have trace equal to $0_F$.
  Then since both $W_1$ and $W_2$ are subspaces of $F^{n \times n}$,
  we can conclude that $W_1 \cap W_2$ is also a subspace of $F^{n \times n}$.
\end{example}

\begin{definition}\label{def:sum}
  Let $V$ be a vector space and let $S_1, S_2 \subseteq V$.
  Then the \emph{sum} of $S_1$ and $S_2$, denoted by $S_1 + S_2$, is the set
  \begin{equation*}
    \{x + y: x \in S_1\ \text{and}\ y \in S_2\}.
  \end{equation*}
\end{definition}

\begin{proposition}\label{prop:subspace-sum}
  Let $V$ be a vector space and let $W_1$ and $W_2$ be subspaces of $V$.
  Then the following statements are true.
  \begin{enumerate}
    \item $W_1 + W_2$ is a subspace of $V$.
    \item If $U$ is a subspace of $V$ with $W_1 \cup W_2 \subseteq U$,
      then $W_1 + W_2 \subseteq U$.
  \end{enumerate}
\end{proposition}
\begin{proof} \leavevmode
  \begin{enumerate}
    \item We have $0_V = 0_V + 0_V \in W_1 + W_2$.
      For each $x, y \in W_1 + W_2$ and for each $a \in F$, by \Cref{def:sum}
      there exist $x_1, y_1 \in W_1$ and $x_2, y_2 \in W_2$ such that
      $x = x_1 + x_2$ and $y = y_1 + y_2$.
      Thus,
      \begin{align*}
        ax + y
        &= a(x_1 + x_2) + (y_1 + y_2) \\
        &= (ax_1 + ax_2) + (y_1 + y_2) \\
        &= (ax_1 + y_1) + (ax_2 + y_2) \\
        &\in W_1 + W_2.
      \end{align*}
    \item Let $x$ be a vector in $W_1 + W_2$.
      Then by \Cref{def:sum} there exists $x_1 \in W_1$ and $x_2 \in W_2$ such
      that $x = x_1 + x_2$.
      We have $x_1 \in U$ since $W_1 \subseteq U$.
      Also, we have $x_2 \in U$ since $W_2 \subseteq U$.
      It follows that $x = x_1 + x_2 \in U$, and thus $W_1 + W_2 \subseteq U$.
      \qedhere
  \end{enumerate}
\end{proof}

\section{Spanning Sets}
\begin{definition}\label{def:summation}
  Let $(G, +)$ be an Abelian group.
  Then we define
  \begin{equation*}
    \sum_{i=m}^n a_i =
      \left\{
      \begin{array}{ll}
        \displaystyle
          \sum_{i=m}^{n-1} a_i + a_n & \text{if} \; m \leq n\\[1.5em]
        0_G & \text{if} \; m > n,\\
      \end{array}
      \right.
  \end{equation*}
  where $a_i \in G$ for each integer $i$ with $m \leq i \leq n$.
\end{definition}

\begin{definition}\label{def:linear-combination}
  Let $(V, +, \cdot)$ be a vector space over $F$.
  Let $S$ be a subset of $V$.
  Then a vector $x \in V$ is called a \emph{linear combination} of $S$ if
  there exist some nonnegative integer $n$,
  scalars $a_1, \dots, a_n \in F$,
  and vectors $x_1, \dots, x_n \in S$ such that
  \begin{equation*}
    x = \sum_{i=1}^n a_ix_i.
  \end{equation*}
\end{definition}
\begin{remark}
  Since $n$ can be zero, $0_V$ is a linear combination for all $S \subseteq V$.
\end{remark}
\begin{remark}
  Although $S$ can be infinite, the number of terms in the summation must be
  finite.
  For example, in the vector space $\mathbb{R}$ over $\mathbb{Q}$,
  although we have
  \begin{equation*}
    e = \sum_{k=0}^\infty \frac{1}{k!}
      = \frac{1}{1} + \frac{1}{1} + \frac{1}{2} + \frac{1}{6}
        + \frac{1}{24} + \cdots,
  \end{equation*}
  $e$ is still not a linear combination of $\mathbb{Q}$.
\end{remark}

\begin{definition}\label{def:span}
  Let $(V, +, \cdot)$ is a vector space over $F$.
  The \emph{span} of $S$, denoted $\mathrm{span}(S)$, is the set that
  consists of all linear combinations of $S$.
\end{definition}

\begin{theorem}\label{thm:span}
  Let $(V, +, \cdot)$ be a vector space over $F$.
  Let $S \subseteq V$. Then the following statements are true.
  \begin{enumerate}
    \item $\mathrm{span}(S)$ is a subspace of $V$.
    \item If $W$ is a subspace of $V$ such that $S \subseteq W$, then
      $\mathrm{span}(S) \subseteq W$.
  \end{enumerate}
\end{theorem}
\begin{proof} \leavevmode
\begin{enumerate}
  \item If $c \in F$ and $x, y \in \mathrm{span}(S)$, then there exist
    nonnegative integers $m, n$,
    scalars $a_1, \dots, a_m, b_1, \dots, b_n \in F$
    and vectors $x_1, \dots, x_m, y_1, \dots, y_n \in S$ such that
    \begin{equation*}
      \begin{array}{lll}
        \displaystyle x = \sum_{i=1}^m a_ix_i
        & \text{and}
        & \displaystyle y = \sum_{j=1}^n b_jy_j.
      \end{array}
    \end{equation*}
    Thus, we have
    \begin{align*}
      cx
      &= c(a_1x_1 + \cdots + a_mx_m) \\
      &= c(a_1x_1) + \cdots + c(a_mx_m) \\
      &= (ca_1)x_1 + \cdots + (ca_m)x_m \in \mathrm{span}(S)
    \end{align*}
    and
    \begin{equation*}
      x + y
        = a_1x_1 + \cdots a_mx_m + b_1y_1 + \cdots + b_ny_n
        \in \mathrm{span}(S).
    \end{equation*}
    Also, $0_V \in \mathrm{span}(S)$.
    Hence, $\mathrm{span}(S)$ is a subspace of $V$.
  \item If $x \in \mathrm{span}(S)$, then there exists an
    nonnegative integer $n$, scalars $a_1, \dots, a_n \in F$
    and vectors $x_1, \dots, x_n \in S$ such that
    \begin{equation*}
      x = \sum_{i=1}^m a_ix_i.
    \end{equation*}
    Thus, since $x_1, \dots, x_n \in W$, we have
    $x = a_1x_1 + \cdots + a_nx_n \in W$. \qedhere
\end{enumerate}
\end{proof}

\begin{definition}\label{def:spanning-set}
  A subset $S$ of a vector space $(V, +, \cdot)$ \emph{spans} $V$
  if $\mathrm{span}(S) = V$.
  In this case, we also say that $S$ is a \emph{spanning set} of $V$.
\end{definition}

\begin{example}
  $\{(0, 1, 1), (1, 0, 1), (1, 1, 0)\}$ is a spanning set of $\mathbb{R}^3$
  since for all $x, y, z \in \mathbb{R}$,
  \begin{equation*}
    (x, y, z)
      = \frac{-x+y+z}{2} \cdot (0, 1, 1)
      + \frac{x-y+z}{2} \cdot (1, 0, 1)
      + \frac{x+y-z}{2} \cdot (1, 1, 0).
  \end{equation*}
\end{example}

\section{Linearly Independent Sets}
\begin{definition}
  Let $(V, +, \cdot)$ be a vector space over $F$.
  Let $S$ be a subset of $V$.
  For scalars $a_1, \dots, a_n \in F$ and distinct vectors
  $x_1, \dots, x_n \in S$, we say that
  \begin{equation*}
    \sum_{i=1}^n a_ix_i = 0_V
  \end{equation*}
  is a \emph{trivial representation} of $0_V$ as a linear combination of $S$
  if $a_1 = \cdots = a_n = 0_F$.
\end{definition}

\begin{definition}
  Let $(V, +, \cdot)$ be a vector space over $F$.
  \begin{itemize}
    \item A subset $S$ of $V$ is called \emph{linearly dependent} if there
      exists a nontrivial representation of $0_V$ as a linear combination of
      $S$.
    \item A subset $S$ of $V$ is called \emph{linearly independent} if
      it is not linear dependent.
  \end{itemize}
\end{definition}

\begin{theorem}\label{thm:linear-dependence-first-equivalence}
  Let $(V, +, \cdot)$ be a vector space over $F$ and let $S \subseteq V$.
  Then $S$ is linearly independent if and only if there exists $x \in S$
  such that $x \in \mathrm{span}(S \setminus \{x\})$.
\end{theorem}
\begin{proof}
  ($\Rightarrow$) Because $S$ is linearly dependent, it follows that
  there exists a nontrivial representation
  \begin{equation*}
    a_1x_1 + a_2x_2 + \cdots + a_nx_n = 0_V
  \end{equation*}
  as a linear combination of $S$, where $a_1, \dots, a_n \in F$ are scalars
  and $x_1, \dots, x_n \in S$ are distinct vectors.
  Without loss of generality, let $a_1 \neq 0_F$.
  Then we have
  \begin{align*}
    x_1
    &= (-a_1)^{-1}(a_2x_2 + \cdots + a_nx_n) \\
    &= (-a_1)^{-1}a_2x_2 + \cdots + (-a_1)^{-1}a_nx_n \\
    &\in \mathrm{span}(S \setminus \{x_1\}).
  \end{align*}
  
  ($\Leftarrow$) Since $x \in \mathrm{span}(S \setminus \{x\})$, there exists
  scalars $a_1, \dots, a_n \in F$ and distinct vectors
  $x_1, \dots, x_n \in S \setminus \{x\}$ such that
  \begin{equation*}
    a_1x_1 + \cdots + a_nx_n = x.
  \end{equation*}
  Then
  \begin{equation*}
    (-1_F)x + a_1x_1 + \cdots + a_nx_n = 0_V
  \end{equation*}
  is a nontrivial representation of $0_V$ as a linear combination of $S$.
\end{proof}

\begin{theorem}\label{thm:linear-dependence-second-equivalence}
  Let $(V, +, \cdot)$ be a vector space over $F$.
  Let $S$ be a subset of $V$ and let $x$ be an element of $S$.
  Then $x \in \mathrm{span}(S \setminus \{x\})$ if and only if
  $\mathrm{span}(S) = \mathrm{span}(S \setminus \{x\})$.
\end{theorem}
\begin{proof}
  ($\Rightarrow$) Since $x \in \mathrm{span}(S \setminus \{x\})$
  and $S \setminus \{x\} \subseteq \mathrm{span}(S \setminus \{x\})$,
  we have
  \begin{equation*}
    S \subseteq \mathrm{span}(S \setminus \{x\})
    \quad \Rightarrow \quad
    \mathrm{span}(S) \subseteq \mathrm{span}(S \setminus \{x\})
  \end{equation*}
  by
  \Cref{thm:span}.
  Also, $\mathrm{span}(S \setminus \{x\}) \subseteq \mathrm{span}(S)$
  because $S \setminus \{x\} \subseteq S$.
  Thus, we can conclude that
  $\mathrm{span}(S \setminus \{x\}) = \mathrm{span}(S)$.

  ($\Leftarrow$) Since
  $x \in S \subseteq \mathrm{span}(S) = \mathrm{span}(S \setminus \{x\})$,
  we have $x \in \mathrm{span}(S \setminus \{x\})$.
\end{proof}

\begin{example}
  Let $S = \{1, 1+2x, 1+2x+3x^2, 1+2x+3x^2+4x^3\}$ be a subset of
  $\mathcal{P}_3(\mathbb{R})$.
  Then $S$ is linearly independent since the only solution to the
  following system of linear equations
  \begin{equation*}
    \setlength\arraycolsep{0pt}
    \begin{array}{r>{{}}c<{{}}r>{{}}c<{{}}r>{{}}c<{{}}r@{{}={}}r}
      a_1 & &      & &      & &      & 0 \\
      a_1 &+& 2a_2 & &      & &      & 0 \\
      a_1 &+& 2a_2 &+& 3a_3 & &      & 0 \\
      a_1 &+& 2a_2 &+& 3a_3 &+& 4a_4 & 0 \\
    \end{array}
  \end{equation*}
  is $a_1 = a_2 = a_3 = a_4 = 0$.
\end{example}

\begin{theorem}\label{thm:linear-independence-implication}
  Let $(V, +, \cdot)$ be a vector space, and let $R \subseteq S \subseteq V$.
  If $R$ is linearly dependent, then $S$ is linearly dependent.
\end{theorem}
\begin{proof}
  If $R$ is linearly dependent, then there exists $x \in R$ such that
  $x \in \mathrm{span}(R \setminus \{x\})$.
  By $R \subseteq S$, we have
  $R \setminus \{x\} \subseteq S \setminus \{x\}$.
  Since $x \in S$ and $x \in \mathrm{span}(S \setminus \{x\})$, $S$ is
  linearly dependent.
\end{proof}

\begin{corollary}
  Let $(V, +, \cdot)$ be a vector space, and let $R \subseteq S \subseteq V$.
  If $S$ is linearly independent, then $R$ is linearly independent.
\end{corollary}
\begin{proof}
  Suppose that $S$ is linearly independent.
  If $R$ is linearly dependent, then so is $S$ by
  \Cref{thm:linear-independence-implication}, contradiction.
  Thus, $R$ is linearly independent.
\end{proof}

\begin{theorem}\label{thm:linearly-independent-subset}
  Let $(V, +, \cdot)$ be a vector space.
  For each finite set $S \subseteq V$, there exists a linearly independent
  set $Q \subseteq S$ such that $\mathrm{span}(Q) = \mathrm{span}(S)$.
\end{theorem}
\begin{proof}
  The proof is by induction on $n = |S|$.
  The induction begins with $n = 0$, i.e., $S = \varnothing$.
  Since $\varnothing$ is linearly independent, we can choose
  $R = \varnothing$, and thus the theorem holds.

  Now suppose that the theorem is true for some integer $n \geq 0$,
  and we prove that the theorem holds for $n + 1$.
  If $S$ is linearly independent, then we can choose $Q = S$.
  Otherwise, there exists $x \in S$
  with $\mathrm{span}(S \setminus \{x\}) = \mathrm{span}(S)$ because
  $S$ is linearly dependent.
  Let $S' = S \setminus \{x\}$. Then there exists a linearly independent set
  $Q \subseteq S'$ such that $\mathrm{span}(Q) = \mathrm{span}(S')$ by
  induction hypothesis, implying
  $Q \subseteq S$ and $\mathrm{span}(Q) = \mathrm{span}(S)$.
\end{proof}

\section{Bases and Dimension}
\begin{definition}\label{def:basis}
  Let $(V, +, \cdot)$ be a vector space. A subset $S$ of $V$ is a \emph{basis}
  of $V$ if $S$ is not only a spanning set but also a linearly independent set
  of $V$.
\end{definition}

\begin{example}
  Following are some examples of bases.
  \begin{itemize}
    \item Since $\mathrm{span}(\varnothing) = \{0_V\}$ and $\varnothing$ is
      linearly independent, $\varnothing$ is a basis of $\{0_V\}$.
    \item Let $S = \{x_1, \dots, x_n\}$ be a subset of $F^n$ with
      $(x_i)_j = [\![i = j]\!]$ for all $i, j \in \{1, \dots, n\}$.
      Then $S$ is called the \emph{standard basis} of $F^n$.
    \item The set $S = \{1_F, x, x^2, \dots, x^n\}$ is the called the
      \emph{standard basis} of $\mathcal{P}_n(F)$.
  \end{itemize}
\end{example}

\begin{theorem}\label{thm:finite-basis-existence}
  Let $(V, +, \cdot)$ be a vector space over $F$.
  If there exists a finite set $S$ that spans $V$, then there is a subset
  $Q$ of $S$ that is a finite basis of $V$.
\end{theorem}
\begin{proof}
  By \Cref{thm:linearly-independent-subset}, there exists
  a linearly independent set $Q \subseteq S$
  such that $\mathrm{span}(Q) = \mathrm{span}(S) = V$.
  Thus, $Q$ is a finite basis of $V$.
\end{proof}

\begin{theorem}[Replacement Theorem]\label{thm:replacement}
  Let $(V, +, \cdot)$ be a vector space over $F$.
  Let $S$ be a finite set that spans $V$,
  and let $Q \subseteq V$ be a finite linearly independent set.
  Then $|Q| \leq |S|$, and there exists $R \subseteq S \setminus Q$ such that
  both $|Q \cup R| = |S|$ and $\mathrm{span}(Q \cup R) = V$ hold.
\end{theorem}
\begin{proof}
  The proof is based on induction on $|Q|$.
  The induction begins with $|Q| = 0$, i.e., $Q = \varnothing$.
  Choosing $R = S$, we have $Q \cup R = S$, and thus
  both $|Q \cup R| = S$ and $\mathrm{span}(Q \cup R) = V$ hold.

  Now suppose that the theorem is true for $|Q| = m$ with $m \geq 0$,
  and we prove that the theorem holds for $|Q|= m + 1$.
  Let $Q = \{x_1, \dots, x_{m+1}\}$ and let $Q' = Q \setminus \{x_{m+1}\}$.
  By induction hypothesis, there exists
  $R' = \{y_1, \dots, y_k\} \subseteq S \setminus Q'$
  such that $m + k = |S|$ and $\mathrm{span}(Q' \cup R') = V$.
  Since $Q' \cup R'$ spans $V$, there exists
  $a_1, \dots, a_m, b_1, \dots, b_k \in F$ such that
  \begin{equation*}
    x_{m+1} = \sum_{i=1}^m a_ix_i + \sum_{j=1}^k b_jy_j.
  \end{equation*}
  If $b_j = 0_F$ for all $j \in \{1, \dots, k\}$, then $x_{m+1}$ is a linear
  combination of $Q$, implying that $Q$ is linearly dependent, contradiction.
  Thus, there must exist some $j \in \{1, \dots, k\}$
  such that $b_j \neq 0_F$.
  Without loss of generality let $b_k \neq 0_F$.
  Also, let $R = \{y_1, \dots, y_{k-1}\}$.
  Then $|Q \cup R| = (m+1) + (k-1) = |S|$.
  Since $k \geq 1$, we have $|Q| \leq |S|$.
  Note that $(Q' \cup R') \setminus (Q \cup R) = \{y_k\}$.
  By
  \begin{equation*}
    y_k = (-b_k)^{-1}\left(
       \sum_{i=1}^m a_ix_i + (-1_F)x_{m+1} + \sum_{j=1}^{k-1} b_jy_j
    \right) \in \mathrm{span}(Q \cup R),
  \end{equation*}
  we have
  \begin{equation*}
    Q' \cup R'
      \subseteq Q \cup R \cup \{y_k\}
      \subseteq \mathrm{span}(Q \cup R).
  \end{equation*}
  Thus, by \Cref{thm:span} we have
  \begin{equation*}
    V = \mathrm{span}(Q' \cup R')
      \subseteq \mathrm{span}(Q \cup R)
      \subseteq V,
  \end{equation*}
  implying $\mathrm{span}(Q \cup R) = V$.
\end{proof}

\begin{corollary}
  Let $(V, +, \cdot)$ be a vector space over $F$ that is spanned by a finite
  set. Then every linearly independent subset of $V$ is finite.
\end{corollary}
\begin{proof}
  Suppose that $S$ is a finite spanning set of $V$ and that $Q$ is linearly
  independent.
  If $Q$ is infinite, then there exists $Q' \subseteq Q$ with
  $|Q'| = |S| + 1$.
  It follows that $Q'$ is linearly independent by
  \Cref{thm:linear-independence-implication}, and thus $|Q'| \leq |S|$ by
  \Cref{thm:replacement}, contradiction to $|Q'| = |S| + 1$.
  Therefore, $Q$ is finite.
\end{proof}

\begin{theorem}\label{thm:dimension}
  Let $(V, +, \cdot)$ be a vector space over $F$.
  If $V$ has a finite basis, then all bases of $V$ have the same size.
\end{theorem}
\begin{proof}
  Let $S$ be a finite basis of $V$ and let $Q$ be an arbitrary basis of $V$.
  Since $V = \mathrm{span}(S)$ and $Q$ is linearly independent, it
  follows that $Q$ is finite, and thus $|Q| \leq |S|$ by
  replacement theorem (\Cref{thm:replacement}).

  Also, since $V = \mathrm{span}(Q)$ and $S$ is linearly independent,
  we have $|S| \leq |Q|$ by replacement theorem (\Cref{thm:replacement}).
  Thus, $|Q| = |S|$.
\end{proof}

\begin{definition}
  A vector space $(V, +, \cdot)$ over $F$ is called \emph{finite-dimensional}
  if it has a finite basis.
  A vector space that is not finite-dimensional is called
  \emph{infinite-dimensional}.
\end{definition}

\begin{definition}
  The number of vectors in each basis of a finite-dimensional vector space $V$
  is called the \emph{dimension} of $V$ and is denoted by $\mathrm{dim}(V)$.
\end{definition}

\begin{example}
  We have $\mathrm{dim}(\{0_V\}) = 0$, $\mathrm{dim}(F^n) = n$, and
  $\mathrm{dim}(\mathcal{P}_n(F)) = n + 1$.
\end{example}

\begin{example}
  The dimension of a vector space depends on its field of scalars.
  \begin{itemize}
    \item If $V = \mathbb{C}$ is a vector space over $\mathbb{R}$, then
      $\mathrm{dim}(V) = 2$ since $\{1, i\}$ is a basis of $V$.
    \item If $W = \mathbb{C}$ is a vector space over $\mathbb{C}$, then
      $\mathrm{dim}(W) = 1$ since $\{1\}$ is a basis of $W$.
  \end{itemize}
\end{example}

\begin{theorem}\label{thm:basis-equivalence}
  Let $(V, +, \cdot)$ be a vector space over $F$.
  Then a subset of $V$ of $n = \mathrm{dim}(V)$ vectors is linearly
  independent if and only if it is a spanning set of $V$.
\end{theorem}
\begin{proof}
  ($\Rightarrow$) Suppose that $Q$ is linearly independent with $|Q| = n$.
  By replacement theorem (\Cref{thm:replacement}), there exists
  $R \subseteq S \setminus Q$ such that $|Q \cup R| = |S|$ and
  $\mathrm{span}(Q \cup R) = V$.
  Since $|Q| = |S|$, we have $|R| = 0$, i.e., $R = \varnothing$.
  Thus, $\mathrm{span}(Q) = V$.

  ($\Leftarrow$) Suppose that $S$ spans $V$ with $|S| = n$.
  By \Cref{thm:finite-basis-existence}, there is a subset $Q$ of $S$
  that is a basis of $V$.
  Then we have $|Q| = n$, implying $Q = S$.
  Thus, $S$ is a basis of $V$.
\end{proof}

\begin{theorem}\label{thm:subspace-dimension}
  Let $(V, +, \cdot)$ be a finite-dimensional vector space over $F$,
  and let $V'$ be a subspace of $V$. Then the following statements hold.
  \begin{enumerate}
    \item $\mathrm{dim}(V') \leq \mathrm{dim}(V)$.
    \item If $\mathrm{dim}(V') = \mathrm{dim}(V)$, then $V' = V$.
  \end{enumerate}
\end{theorem}
\begin{proof}
  Let $S$ be a basis of $V$ and let $S'$ be a basis of $V'$.
  \begin{enumerate}
    \item Since $S'$ is linearly independent and $V = \mathrm{span}(S)$,
      we have $|S'| \leq |S|$ by replacement theorem (\Cref{thm:replacement}).
      Thus, $\mathrm{dim}(V') \leq \mathrm{dim}(V)$.
    \item Since $S'$ is linearly independent and $|S'| = \mathrm{dim}(V)$,
      we have $\mathrm{span}(S') = V$ by \Cref{thm:basis-equivalence}.
      Thus, $V' = \mathrm{span}(S') = V$. \qedhere
  \end{enumerate}
\end{proof}