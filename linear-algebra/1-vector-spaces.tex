\chapter{Vector Spaces}
\section{Fields}
\begin{definition}
  \label{def:field}
  A \emph{field} is a set $F$ with two operations, called \emph{addition}
  (denoted by $+$) and \emph{multiplication} (denoted by $\cdot$), which
  satisfy the following axioms.
  \begin{enumerate}[leftmargin=3.5em]
    \item[(A 1)] If $a \in F$ and $b \in F$, then $a + b \in F$.
    \item[(A 2)] $a + b = b + a$ for all $a, b \in F$.
    \item[(A 3)] $(a + b) + c = a + (b + c)$ for all $a, b, c \in F$.
    \item[(A 4)] There is an element $0_F$ in $F$ such that $0_F + a = a$ for
      all $a \in F$. 
    \item[(A 5)] For each $a \in F$ there is an element $-a$ in $F$ such that
      $a + (-a) = 0_F$.
    \item[(M 1)] If $a \in F$ and $b \in F$, then $a \cdot b \in F$.
    \item[(M 2)] $a \cdot b = b \cdot a$ for all $a, b \in F$.
    \item[(M 3)] $(a \cdot b) + c = a + (b \cdot c)$ for all $a, b, c \in F$.
    \item[(M 4)] There is an element $1_F$ in $F \setminus \{0_F\}$ such that
      $1_F \cdot a = a$ for all $a \in F$.
    \item[(M 5)] For each $a \in F \setminus \{0_F\}$ there is an element
      $a^{-1}$ in $F$ such that $a \cdot a^{-1} = 1_F$.
    \item[(D)] $a \cdot (b + c) = a \cdot b + a \cdot c$ for all
      $a, b, c \in F$.
  \end{enumerate}
\end{definition}

\begin{example}
  The set $\mathbb{Q}$ of rational numbers, the set $\mathbb{R}$ of real
  numbers, and the set $\mathbb{C}$ of complex numbers are fields.
\end{example}

\begin{proposition}\label{prop:field-addition}
  Let $F$ be a field with $a, b, c \in F$.
  \begin{enumerate}
    \item If $a + b = a + c$, then $b = c$.
    \item If $a + b = a$, then $b = 0_F$.
    \item If $a + b = 0_F$, then $b = -a$.
    \item $-(-a) = a$.
  \end{enumerate}
\end{proposition}
\begin{proof} \leavevmode
  \begin{enumerate}
    \item It can be proved by
      \begin{align*}
        b
        &= 0_F + b      \\
        &= (-a + a) + b \\
        &= -a + (a + b) \\
        &= -a + (a + c) \\
        &= (-a + a) + c \\
        &= 0_F + c      \\
        &= c.
      \end{align*}
    \item By applying (a), it follows from $a + b = a + 0_F$ that $b = 0_F$.
    \item By applying (a), it follows from $a + b = a + (-a)$ that $b = -a$.
    \item Since $-a + a = 0_F$, we have $a = -(-a)$ by (c). \qedhere
  \end{enumerate}
\end{proof}

\begin{proposition}\label{prop:field-multiplication}
  Let $F$ be a field with $a, b, c \in F$ and $a \neq 0_F$.
  \begin{enumerate}
    \item If $a \cdot b = a \cdot c$, then $b = c$.
    \item If $a \cdot b = a$, then $b = 1_F$.
    \item If $a \cdot b = 1_F$, then $b = a^{-1}$.
    \item $(a^{-1})^{-1} = a$.
  \end{enumerate}
\end{proposition}
\begin{proof}
  The proof is omitted since it is similar to that of
  \Cref{prop:field-addition}.
\end{proof}

\begin{proposition}\label{prop:field-operation}
  Let $F$ be a field with $a, b \in F$.
  \begin{enumerate}
    \item $0_F \cdot a = 0_F$.
    \item $(-a) \cdot b = -(a \cdot b) = a \cdot (-b)$.
    \item $(-a) \cdot (-b) = a \cdot b$.
  \end{enumerate}
\end{proposition}
\begin{proof} \leavevmode
  \begin{enumerate}
    \item Since
      \begin{equation*}
        0_F \cdot a + 0_F \cdot a
        = (0_F + 0_F) \cdot a
        = 0_F \cdot a,
      \end{equation*}
      we have $0_F \cdot a = 0_F$ by \Cref{prop:field-addition} (b).
    \item Since
      \begin{equation*}
        (-a) \cdot b + a \cdot b
        = (-a + a) \cdot b
        = 0_F \cdot b
        = 0_F,
      \end{equation*}
      we have $(-a) \cdot b = -(a \cdot b)$ by \Cref{prop:field-addition} (c).
      The other half can be proved similarly.
    \item By applying (b) twice, we have
      \begin{equation*}
        (-a) \cdot (-b)
        = -(a \cdot (-b))
        = -(-(a \cdot b))
        = a \cdot b. \qedhere
      \end{equation*}
  \end{enumerate}
\end{proof}

\section{Vector Spaces}
\begin{definition}\label{def:vector-space}
  A \emph{vector space} over a field $F$ is a set $V$ with two operations,
  called \emph{addition} (denoted by $+$) and \emph{scalar multiplication}
  (denoted by $\cdot$), which satisfy the following axioms.
  \begin{enumerate}[leftmargin=3.5em]
    \item[(V 1)] If $x \in V$ and $y \in V$, then $x + y \in V$.
    \item[(V 2)] $x + y = y + x$ for all $x, y \in V$. 
    \item[(V 3)] $(x + y) + z = x + (y + z)$ for all $x, y, z \in V$.
    \item[(V 4)] There is an element $0_V$ in $V$ such that $0_V + x = x$ for
      all $x \in V$.
    \item[(V 5)] For each $x \in V$ there is an element $-x$ such that
      $x + (-x) = 0_V$.
    \item[(V 6)] If $a \in F$ and $x \in V$, then $a \cdot x \in V$. 
    \item[(V 7)] $(a \cdot b) \cdot x = a \cdot (b \cdot x)$ for all
      $a, b \in F$ and $x \in V$. 
    \item[(V 8)] $1_F \cdot x = x$ for all $x \in V$.
    \item[(V 9)] $a \cdot (x + y) = a \cdot x + a \cdot y$ for all $a \in F$
      and $x, y \in V$.
    \item[(V 10)] $(a + b) \cdot x = a \cdot x + b \cdot x$ for all
      $a, b \in F$ and $x \in V$.
  \end{enumerate}
\end{definition}

\begin{remark} \leavevmode
  \begin{itemize}
    \item For simplification, we usually write $ax$ instead of $a \cdot x$.
    \item The elements $0_V$ is usually called the \emph{additive identity} of
      $V$, and $-x$ is called the \emph{additive inverse} of $x$ in $V$.
    \item \emph{Subtraction} can be defined using additive inverses.
  \end{itemize}
\end{remark}

\begin{examples}
  \leavevmode
  \begin{itemize}
    \item A field is a vector space over itself, e.g., $\mathbb{R}$ is a vector
      space over $\mathbb{R}$.
    \item $\mathbb{C}$ is a vector space over $\mathbb{R}$.
    \item $\mathbb{R}$ is a vector space over $\mathbb{Q}$.
  \end{itemize}
\end{examples}

\begin{examples}
  Let $F$ be a field.
  \begin{itemize}
    \item The set of \emph{$n$-tuples} with entries from $F$, denoted $F^n$,
    is a vector space over $F$.
    \item The set of all $m \times n$ \emph{matrices} with entries from $F$,
    denoted $F^{m \times n}$, is a vector space over $F$.
    \item The set of \emph{polynomials} with coefficients from $F$, denoted
    $\mathcal{P}(F)$, is a vector space over $F$.
  \end{itemize}
\end{examples}

\begin{proposition}\label{prop:vector-space-addition}
  Let $V$ be a vector space with $x, y, z \in F$.
  \begin{enumerate}
    \item If $x + y = x + z$, then $y = z$.
    \item If $x + y = x$, then $y = 0_V$.
    \item If $x + y = 0_V$, then $y = -x$.
    \item $-(-x) = x$.
  \end{enumerate}
\end{proposition}
\begin{proof}
  The proof is omitted since it is similar to that of
  \Cref{prop:field-addition}.
\end{proof}

\begin{proposition}\label{prop:vector-space-operation}
  Let $V$ be a vector space over a field $F$ with $x \in V$ and $a \in F$.
  \begin{enumerate}
    \item $0_F \cdot x = 0_V$.
    \item $a \cdot 0_V = 0_V$.
    \item $(-a) \cdot x = -(a \cdot x) = a \cdot (-x)$.
  \end{enumerate}
\end{proposition}
\begin{proof}
  The proof is omitted since it is similar to that of
  \Cref{prop:field-operation}.
\end{proof}

\section{Subspaces}
\begin{definition}
  \label{def:subspace}
  Let $V$ be a vector space over $F$.
  A \emph{subspace} of $V$ is a subset $W$ of $V$ such that $W$ is a vector
  space over $F$, where addition and scalar multiplication are the same as
  those defined on $V$.
\end{definition}

\begin{theorem}
  \label{thm:subspace}
  Let $V$ be a vector space over $F$ and let $W \subseteq V$.
  Then $W$ is a subspace of $V$ if and only if $0_V \in W$ and $ax + y \in W$
  for any $a \in F$ and $x, y \in W$.
\end{theorem}
\begin{proof}
  ($\Rightarrow$)
  Straightforward.
  ($\Leftarrow$)
  It suffices to prove the closeness of addition and scalar multiplication,
  and the existence of additive inverses.
  For any $a \in F$ and $x, y \in W$, we have
  \begin{equation*}
    a \cdot x = a \cdot x + 0_W \in W
    \quad \text{and} \quad
    x + y = 1_F \cdot x + y \in W.
  \end{equation*}
  Furthermore, we have
  \begin{equation*}
    x + (-1_F) \cdot x
    = 1_F \cdot x + (-1_F) \cdot x
    = (1_F + (-1_F)) \cdot x
    = 0_F \cdot x
    = 0_V,
  \end{equation*}
  for any $x \in W$, which completes the proof.
\end{proof}

\begin{example}
  The set $\mathcal{P}_n(F)$ of polynomials in $\mathcal{P}(F)$ with
  degree less than or equal to $n$ is a subspace of $\mathcal{P}(F)$.
\end{example}

\begin{definition}
  Let $V$ be a vector space and let $S_1$ and $S_2$ be subsets of $V$.
  Then the \emph{sum} of $S_1$ and $S_2$ is
  \begin{align*}
    S_1 + S_2 &= \{x + y: \text{$x \in S_1$ and $y \in S_2$}\}.
  \end{align*}
\end{definition}

\begin{theorem}
  \label{thm:subspace-sum}
  Let $V$ be a vector space and let $W_1$ and $W_2$ be subspaces of $V$.
  Then $W_1 + W_2$ is the minimal subspace of $V$ that contains $W_1 \cup W_2$.
\end{theorem}
\begin{proof}
  First we prove that $W_1 + W_2$ is a subspace of $V$.
  We have $0_V = 0_V + 0_V \in W_1 + W_2$, and for any $a \in F$,
  $x_1, y_1 \in W_1$ and $x_2, y_2 \in W_2$, we have
  \begin{align*}
    a(x_1 + x_2) + (y_1 + y_2)
    &= ax_1 + ax_2 + y_1 + y_2 \\
    &= (ax_1 + y_1) + (ax_2 + y_2) \\
    &\in W_1 + W_2.
  \end{align*}
  Thus, it follows from \Cref{thm:subspace} that $W_1 + W_2$ is a subspace of
  $V$.

  Now we prove the minimality.
  Suppose that $W$ is a subspace of $V$ that contains $W_1 \cup W_2$.
  For any $x_1 \in W_1$ and $x_2 \in W_2$, we have $x_1 + x_2 \in W$.
  Thus, $W_1 + W_2 \subseteq W$, completing the proof.
\end{proof}

\section{Spanning Sets}
\begin{definition}
  \label{def:linear-combination}
  Let $V$ be a vector space over $F$ and let $S \subseteq V$.
  A vector $x \in V$ is called a \emph{linear combination} of $S$ if $x = 0_V$
  or there exist scalars $a_1, \dots, a_n \in F$ and vectors
  $x_1, \dots, x_n \in S$ such that
  \begin{equation*}
    x = \sum_{i=1}^n a_ix_i = a_1x_1 + a_2x_2 + \cdots + a_nx_n.
  \end{equation*}
  The set of all linear combinations of $S$ is called the \emph{span} of $S$,
  denoted by $\spn(S)$.
\end{definition}

\begin{theorem}\label{thm:span}
  Let $V$ be a vector space over $F$ and let $S \subseteq V$.
  Then the following statements are true.
  \begin{enumerate}
    \item $\spn(S)$ is a subspace of $V$.
    \item If $U$ is a subspace of $V$ such that $S \subseteq U$, then
      $\spn(S) \subseteq U$.
  \end{enumerate}
\end{theorem}
\begin{proof}
  \leavevmode
  \begin{enumerate}
    \item Let $c \in F$ and $x, y \in \spn(S)$.
      Then there exist scalars $a_1, \dots, a_n \in F$
      and vectors $x_1, \dots, x_n \in S$ such that
      \begin{equation*}
        x = a_1x_1 + \cdots + a_nx_n.
      \end{equation*}
      Also, there exist scalars $b_1, \dots, b_n \in F$
      and vectors $y_1, \dots, y_m \in S$ such that
      \begin{equation*}
        y = b_1y_1 + \cdots + b_ny_m.
      \end{equation*}
      Thus, we have
      \begin{align*}
        cx + y
        &= c(x_1 + \cdots + x_n) + (y_1 + \cdots + y_m) \\
        &= cx_1 + \cdots + cx_n + y_1 + \cdots + y_m \\
        &\in \spn(S).
      \end{align*}
      Furthermore, $0_V \in \spn(S)$.
      Hence, $\spn(S)$ is a subspace of $V$ by \Cref{cor:subspace}.
    \item Let $x \in \spn(S)$.
      Then there exist scalars $a_1, \dots, a_n \in F$
      and vectors $x_1, \dots, x_n \in S$ such that
      \begin{equation*}
        x = a_1x_1 + \cdots + a_nx_n.
      \end{equation*}
      Since $S \subseteq U$, we have $x_1, \dots, x_n \in U$, and it follows that
      $x = a_1x_1 + \cdots + a_nx_n \in U$ due to the closeness of $U$.
      Thus, $\spn(S) \subseteq U$. \qedhere
  \end{enumerate}
\end{proof}

\begin{definition}\label{def:spanning-set}
  Let $V$ be a vector space and let $S \subseteq V$.
  If $\spn(S) = V$, then $S$ is called a \emph{spanning set} of $V$, and we
  also say $S$ \emph{spans} $V$.
\end{definition}

\begin{example}
  $\{(0, 1, 1), (1, 0, 1), (1, 1, 0)\}$ is a spanning set of $\mathbb{R}^3$
  since for any $x, y, z \in \mathbb{R}$,
  \begin{equation*}
    (x, y, z)
      = \frac{-x+y+z}{2} \cdot (0, 1, 1)
      + \frac{x-y+z}{2} \cdot (1, 0, 1)
      + \frac{x+y-z}{2} \cdot (1, 1, 0).
  \end{equation*}
\end{example}

\begin{proposition}\label{prop:span}
  Let $V$ be a vector space and let $R, S \subseteq V$.
  \begin{enumerate}
    \item $S \subseteq \spn(S)$.
    \item If $R \subseteq S$, then $\spn(R) \subseteq \spn(S)$.
    \item $S = \spn(S)$ if and only if $S$ is a subspace of $V$.
    \item $\spn(R \cup S) = \spn(R) + \spn(S)$.
  \end{enumerate}
\end{proposition}
\begin{proof}
  \leavevmode
  \begin{enumerate}
    \item Straightforward.
    \item It is true since a linear combination of a subset of $S$ is also a
      linear combination of $S$.
    \item $(\Rightarrow)$ Straightforward from \Cref{thm:span} (a).

      $(\Leftarrow)$ Note that any linear combination of $S$ is in $S$ due to
      closeness of addition and scalar multiplication in $S$.
      Thus, $\spn(S) \subseteq S$, and it follows that $S = \spn(S)$.
    \item Since $R \subseteq \spn(R)$ and $S \subseteq \spn(S)$, we have
      $R \cup S \subseteq \spn(R) + \spn(S)$. Thus, by \Cref{thm:span}, we have
      $\spn(R \cup S) \subseteq \spn(R) + \spn(S)$.
      On the other side, since
      \begin{equation*}
        \spn(R) \subseteq \spn(R \cup S)
        \quad \text{and} \quad
        \spn(S) \subseteq \spn(R \cup S),
      \end{equation*}
      we can conclude that $\spn(R) \cup \spn(S) \subseteq \spn(R \cup S)$.
      Thus, $\spn(R) + \spn(S) \subseteq \spn(R \cup S)$
      by \Cref{prop:subspace-sum}. \qedhere
  \end{enumerate}
\end{proof}

\section{Linearly Independent Sets}
\begin{definition}\label{def:linear-independence}
  Let $V$ be a vector space over a field $F$ and let $S \subseteq V$.
  \begin{itemize}
    \item $S$ is \emph{linearly dependent} if there exist scalars
      $a_1, a_2, \dots, a_n \in F \setminus \{0_F\}$ and distinct vectors
      $x_1, x_2, \dots, x_n \in S$ for some positive integer $n$ such that
      \begin{equation*}
        a_1x_1 + a_2x_2 + \cdots + a_nx_n = 0_V.
      \end{equation*}
    \item $S$ is \emph{linearly independent} if it is not linearly dependent.
  \end{itemize}
\end{definition}

\begin{remark}
  \leavevmode
  \begin{itemize}
    \item Note that $\varnothing$ is linearly independent.
  \end{itemize}
\end{remark}

\begin{theorem}\label{thm:linear-dependence-equivalence}
  Let $V$ be a vector space over a field $F$ and let $S \subseteq V$.
  Then the following statements are equivalent.
  \begin{enumerate}
    \item $S$ is linearly dependent.
    \item There exists $x \in S$ with $x \in \spn(S \setminus \{x\})$.
    \item There exists $x \in S$ with $\spn(S) = \spn(S \setminus \{x\})$.
  \end{enumerate}
\end{theorem}
\begin{proof}
  \leavevmode
  \begin{enumerate}[(i)]
    \item First we assume (a) and prove (b). Suppose that
      \begin{equation*}
        a_0x_0 + a_1x_1 + \cdots + a_nx_n = 0_V,
      \end{equation*}
      where $a_0, a_1, \dots, a_n$ are nonzero scalars and
      $x_0, x_1, \dots, x_n$ are distinct vectors. Then
      \begin{align*}
        x_0
        &= (-a_0)^{-1}(a_1x_1 + \cdots + a_nx_n) \\
        &= ((-a_0)^{-1}a_1)x_1 + \cdots + ((-a_0)^{-1}a_n)x_n \\
        &\in \spn(S \setminus \{x_0\}).
      \end{align*}
    \item Then we assume (b) and prove (c). Since
      \begin{equation*}
        x \in \spn(S \setminus \{x\})
        \quad \text{and} \quad
        S \setminus \{x\} \subseteq \spn(S \setminus \{x\}),
      \end{equation*}
      we have $S \subseteq \spn(S \setminus \{x\})$.
      Thus, $\spn(S) \subseteq \spn(S \setminus \{x\})$ by \Cref{thm:span},
      and we can conclude that $\spn(S) = \spn(S \setminus \{x\})$.
    \item Then we assume (c) and prove (b). It is straightforward
      since $x \in S \subseteq \spn(S) = \spn(S \setminus \{x\})$.
    \item Finally we assume (b) and prove (a).
      Without loss of generality, let $a_1, \dots, a_n \in F$ be nonzero
      scalars and $x_1, \dots, x_n \in S \setminus \{x\}$ be distinct vectors
      such that $x = a_1x_1 + \cdots + a_nx_n$. Then we have
      \begin{equation*}
        (-1_F)x + a_1x_1 + \cdots + a_nx_n = 0_V,
      \end{equation*}
      which completes the proof. \qedhere
  \end{enumerate}
\end{proof}

\begin{example}
  Let $S = \{(0, 1, 1), (1, 0, 1), (1, 1, 0)\}$ be a subset of $\mathbb{R}^3$.
  Suppose that $a_1, a_2, a_3 \in \mathbb{R}$ are scalars such that
  \begin{equation*}
    a_1(0, 1, 1) + a_2(1, 0, 1) + a_3(1, 1, 0) = (0, 0, 0).
  \end{equation*}
  Then we have the following system of equations.
  \begin{equation*}
    \setlength\arraycolsep{0pt}
    \begin{array}{r>{{}}c<{{}}r>{{}}c<{{}}r@{{}={}}r}
          & & a_2 &+& a_3 & 0 \\
      a_1 & &     &+& a_3 & 0 \\
      a_1 &+& a_2 & &     & 0 \\
    \end{array}
  \end{equation*}
  Since the only solution to this system of equations is $a_1 = a_2 = a_3 = 0$,
  we can conclude that $S$ is linearly independent by
  \Cref{def:linear-independence}.
\end{example}

\begin{example}
  Let $S = \{(1, 1, 1), (0, 1, 1), (1, 0, 1), (1, 1, 0)\}$ be a subset of
  $\mathbb{R}^3$.
  We can conclude that $S$ is linearly dependent since
  \begin{equation*}
    (1, 1, 1)
    = \frac{1}{2} \cdot (0, 1, 1) + \frac{1}{2} \cdot (1, 0, 1)
      + \frac{1}{2} \cdot (1, 1, 0).
  \end{equation*}
\end{example}

\begin{proposition}\label{prop:linear-independence-subset}
  Let $V$ be a vector space and let $R, S$ be subsets of $V$ with
  $R \subseteq S$.
  \begin{enumerate}
    \item If $R$ is linearly dependent, then so is $S$.
    \item If $S$ is linearly independent, then so is $R$.
  \end{enumerate}
\end{proposition}
\begin{proof}
  \leavevmode
  \begin{enumerate}
    \item Suppose that $R$ is linearly dependent. Then by
      \Cref{def:linear-independence} there exists $x \in R$
      such that $x \in \spn(R \setminus \{x\})$. Also, we have
      $R \setminus \{x\} \subseteq S \setminus \{x\}$ since $R \subseteq S$.
      Thus, $x \in \spn{S \setminus \{x\}}$, and it follows that $S$ is
      linearly dependent.
    \item Straightforward from (a). \qedhere
  \end{enumerate}
\end{proof}

\section{Bases and Dimension}
\begin{definition}\label{def:basis}
  A \emph{basis} for a vector space $V$ is a linearly independent subset of $V$
  that spans $V$.
\end{definition}

\begin{examples}
  \leavevmode
  \begin{itemize}
    \item $\varnothing$ is a basis for $\{0_V\}$.
    \item $\{e_1, \dots, e_n\}$ is a basis for $F^n$, where $e_i$ is the
      $n$-tuple whose $i$-th component is $1_F$ and the other components are
      all $0_F$.
    \item $\{E_{ij} : 1 \leq i \leq m\ \text{and}\ 1 \leq j \leq n\}$ is a
      basis for $F^{m \times n}$, where $E_{ij}$ is the matrix whose
      $(i, j)$-entry is $1_F$ and the other entries are all $0_F$.
    \item $\{t^0, t^1, t^2, \dots, t^n\}$ is a basis for $\mathcal{P}_n(F)$.
    \item $\{t^0, t^1, t^2, \dots\}$ is a basis for $\mathcal{P}(F)$.
  \end{itemize}
\end{examples}

\begin{proposition}\label{prop:finite-basis-existence}
  Let $V$ be a vector space.
  If there exists a finite set $S$ that spans $V$, then there is a subset
  $Q$ of $S$ that is a finite basis of $V$.
\end{proposition}
\begin{proof}
  The proof is by induction on $|S|$.
  For the induction basis, suppose that $|S| = 0$, i.e., $S = \varnothing$.
  Then the proposition holds since one can choose $Q = \varnothing$ as a basis
  for $V$.

  Now assume the induction hypothesis that the proposition holds for $|S| = n$
  with $n \geq 0$.
  If $S$ is linearly independent, then we can choose $Q = S$ as a basis for
  $V$.
  Otherwise, there exists $x \in S$ with $\spn(S \setminus \{x\}) = \spn(S)$,
  i.e., $S \setminus \{x\}$ spans $V$.
  Thus, by induction hypothesis there is a subset $Q$ of $S \setminus \{x\}$
  that is a basis for $V$, which completes the proof.
\end{proof}

\begin{theorem}[Replacement Theorem]\label{thm:replacement}
  Let $V$ be a vector space over a field $F$.
  Let $S$ be a finite set that spans $V$,
  and let $Q \subseteq V$ be a finite linearly independent set.
  Then $|Q| \leq |S|$, and there exists $R \subseteq S \setminus Q$ such that
  both $|Q \cup R| = |S|$ and $\mathrm{span}(Q \cup R) = V$ hold.
\end{theorem}
\begin{proof}
  The proof is based on induction on $|Q|$.
  The theorem holds for $|Q| = 0$, i.e., $Q = \varnothing$, since we have
  $|\varnothing| \leq |S|$, $|\varnothing \cup S| = |S|$ and
  $\spn(\varnothing \cup S) = V$.

  Now suppose that the theorem is true for $|Q| = m$ with $m \geq 0$,
  and we prove that the theorem holds for $|Q|= m + 1$.
  Let $Q = \{x_1, \dots, x_{m+1}\}$ and let $Q' = \{x_1, \dots, x_m\}$.
  By induction hypothesis, there exists
  $R' = \{y_1, \dots, y_k\} \subseteq S \setminus Q'$
  such that $|Q'| + |R'| = |S|$ and $\mathrm{span}(Q' \cup R') = V$.
  Since $Q' \cup R'$ spans $V$, there exists
  $a_1, \dots, a_m, b_1, \dots, b_k \in F$ such that
  \begin{equation*}
    x_{m+1} = \sum_{i=1}^m a_ix_i + \sum_{j=1}^k b_jy_j.
  \end{equation*}
  If $b_j = 0_F$ for all $j \in \{1, \dots, k\}$, then
  $x_{m+1} \in \spn(Q') = \spn(Q \setminus\{x_{m+1}\})$,
  implying that $Q$ is linearly dependent, contradiction.
  Thus, there must exist some $j \in \{1, \dots, k\}$
  such that $b_j \neq 0_F$.s
  Without loss of generality, suppose that $b_k \neq 0_F$ with $k \geq 1$.
  Also, let $R = \{y_1, \dots, y_{k-1}\}$.
  Then $|Q \cup R| = (m+1) + (k-1) = |S|$, and we have $|Q| \leq |S|$.
  It follows that
  \begin{equation*}
    Q' \cup R'
    \quad \subseteq \quad Q \cup R \cup \{y_k\}
    \quad \subseteq \quad \mathrm{span}(Q \cup R),
  \end{equation*}
  where the second inclusion holds because
  \begin{equation*}
    y_k = (-b_k)^{-1}\left(
       \sum_{i=1}^m a_ix_i + (-1_F)x_{m+1} + \sum_{j=1}^{k-1} b_jy_j
    \right) \in \mathrm{span}(Q \cup R).
  \end{equation*}
  Then, we have
  \begin{equation*}
    V = \mathrm{span}(Q' \cup R')
      \subseteq \mathrm{span}(Q \cup R)
      \subseteq V.
  \end{equation*}
  by \Cref{thm:span}.
  Thus, $\mathrm{span}(Q \cup R) = V$, which completes the proof.
\end{proof}

\begin{corollary}\label{cor:infinite-dimensional}
  Let $V$ be a vector space and $Q$ be a linearly independent subset of $V$
  that is infinite.
  Then each spanning set of $V$ is infinite.
\end{corollary}
\begin{proof}
  Suppose that there is a finite set $S$ that spans $V$.
  Let $Q'$ be a subset of $Q$ with $|Q'| = |S| + 1$.
  By \Cref{prop:linear-independence-subset}, we can conclude that $Q'$ is also
  linearly independent.
  Thus, we have $|Q'| \leq |S|$ by replacement theorem
  (\Cref{thm:replacement}), contradiction.
\end{proof}

\begin{corollary}\label{cor:dimension}
  Let $V$ be a vector space.
  If $V$ has a finite basis, then each basis for $V$ has the same size.
\end{corollary}
\begin{proof}
  Let $S$ be a finite basis for $V$ and $Q$ an arbitrary basis for $V$.
  Since $V = \spn(S)$ and $Q$ is linearly independent, it follows that $Q$ is
  finite by \Cref{cor:infinite-dimensional}, and thus we have $|Q| \leq |S|$.
  Also, since $V = \spn(Q)$ and $S$ is linearly independent, we have
  $|S| \leq |Q|$.
  Thus, $|Q| = |S|$.
\end{proof}

\begin{definition}\label{def:dimension}
  Let $V$ be a vector space.
  \begin{itemize}
    \item $V$ is \emph{finite-dimensional} if it has a finite basis.
      In this case, the number of vectors in each basis for $V$ is called the
      \emph{dimension} of $V$, denoted by $\dim(V)$.
    \item $V$ is \emph{infinite-dimensional} if it is not finite-dimensional.
  \end{itemize}
\end{definition}

\begin{remark}
  \leavevmode
  \begin{itemize}
    \item If a vector space has a linearly independent subset that is infinite,
      we can conclude that it is infinite-dimensional by
      \Cref{cor:infinite-dimensional}.
  \end{itemize}
\end{remark}

\begin{examples}
  One can find the dimension of a vector space by any basis it admits.
  \begin{itemize}
    \item $\dim(\{0_V\}) = 0$.
    \item $\dim(F^n) = n$.
    \item $\dim(F^{m \times n}) = mn$.
    \item $\dim(\mathcal{P}_n(F)) = n + 1$.
    \item $\mathcal{P}(F)$ is infinite-dimensional.
  \end{itemize}
\end{examples}

\begin{examples}
  Note that the dimension of a vector space depends on its field of scalars.
  \begin{itemize}
    \item Let $V = \mathbb{C}$ be a vector space over $\mathbb{R}$.
      Then we have $\dim(V) = 2$ since $\{1, i\}$ is a basis for $V$.
    \item Let $W = \mathbb{C}$ be a vector space over $\mathbb{C}$.
      Then we have $\dim(W) = 1$ since $\{1\}$ is a basis for $V$.
  \end{itemize}
\end{examples}

\begin{proposition}\label{prop:basis-equivalence}
  Let $V$ be a vector space.
  Then a subset of $V$ of $n = \dim(V)$ vectors is linearly
  independent if and only if it is a spanning set of $V$.
\end{proposition}
\begin{proof}
  ($\Rightarrow$) Suppose that $Q$ is linearly independent with $|Q| = n$.
  By replacement theorem (\Cref{thm:replacement}), there exists
  $R \subseteq S \setminus Q$ such that $|Q \cup R| = |S|$ and
  $\mathrm{span}(Q \cup R) = V$.
  Since $|Q| = |S|$, we have $|R| = 0$, i.e., $R = \varnothing$.
  Thus, $\mathrm{span}(Q) = V$.

  ($\Leftarrow$) Suppose that $S$ spans $V$ with $|S| = n$.
  By \Cref{prop:finite-basis-existence}, there is a subset $Q$ of $S$
  that is a basis of $V$.
  Then we have $|Q| = n$, implying $Q = S$.
  Thus, $S$ is a basis for $V$.
\end{proof}

\begin{proposition}\label{prop:unique-coordinate}
  Let $V$ be a finite-dimensional vector space.
  Let $S = \{x_1, \dots, x_n\}$ be a basis for $V$.
  Then for each $x \in V$, there exist a unique $n$-tuple
  $(a_1, \dots, a_n) \in F^n$ with
  \begin{equation*}
    x = a_1x_1 + \cdots + a_nx_n.
  \end{equation*}
\end{proposition}
\begin{proof}
  Since $x \in \spn(S)$, there exist scalars $a_1, \dots, a_n \in F$ such that
  \begin{equation*}
    x = a_1x_1 + \cdots + a_nx_n.
  \end{equation*}
  Now we prove the uniqueness.
  Let $b_1, \dots, b_n \in F$ be scalars with
  \begin{equation*}
    x = b_1x_1 + \cdots + b_nx_n.
  \end{equation*}
  Then we have
  \begin{equation*}
    0_V = (a_1 - b_1)x_1 + \cdots + (a_n - b_n)x_n,
  \end{equation*}
  and it follows that $(a_1 - b_1, a_2 - b_2, \dots, a_n - b_n) = 0_{F^n}$
  since $S$ is linearly independent.
  Thus, $(a_1, \dots, a_n) = (b_1, \dots, b_n)$.
\end{proof}

\begin{proposition}\label{prop:subspace-dimension}
  Let $V$ be a finite-dimensional vector space.
  Let $V'$ be a subspace of $V$.
  Then the following statements are true.
  \begin{enumerate}
    \item $\mathrm{dim}(V') \leq \mathrm{dim}(V)$.
    \item If $\mathrm{dim}(V') = \mathrm{dim}(V)$, then $V' = V$.
  \end{enumerate}
\end{proposition}
\begin{proof}
  Let $S$ and $S'$ be bases for $V$ and $V'$, respectively.
  \begin{enumerate}
    \item Since $S'$ is linearly independent and $V = \mathrm{span}(S)$,
      we have $|S'| \leq |S|$ by replacement theorem (\Cref{thm:replacement}).
      Thus, $\mathrm{dim}(V') \leq \mathrm{dim}(V)$.
    \item Since $S'$ is linearly independent and $|S'| = \mathrm{dim}(V)$,
      we have $\mathrm{span}(S') = V$ by \Cref{prop:basis-equivalence}.
      Thus, $V' = \mathrm{span}(S') = V$. \qedhere
  \end{enumerate}
\end{proof}

\begin{example}
  Let $W$ be the set of $n \times n$ diagonal matrices, which is a subspace of
  $F^{n \times n}$.
  Then one can verify that $\{E_{ii}: 1 \leq i \leq n\}$ is a basis for $W$,
  where $E_{ij}$ is the matrix whose $(i, j)$-entry is $1_F$ and the other
  entries are $0_F$.
  Thus, $\dim(W) = n$.
\end{example}