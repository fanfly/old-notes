\chapter{Discrete Random Variables}
\section{Discrete Random Variables}
\begin{definition}
  Let $X: \Omega \to \mathbb{R}$ be a function in a probability space
  $(\Omega, \Sigma, P)$.
  Then $X$ is called a \emph{random variable} if for each
  $S \in \mathcal{B}$, its preimage
  \begin{equation*}
    X^{-1}(S) = \{\omega \in \Omega : X(\omega) \in S\}
  \end{equation*}
  is an element of $\Sigma$.
  \begin{itemize}
    \item A random variable is \emph{discrete} if its range is countable.
    \item A random variable is \emph{continuous} if its range is uncountable.
  \end{itemize}
\end{definition}
\begin{remark}
  Since each $S \in \mathcal{B}$ is mapped to a event $X^{-1}(S) \in \Sigma$,
  we will use conditions related to ramdom variables to denote events.
  For example,
  \begin{equation*}
    P(-1 \leq X \leq 1) = P(\{\omega \in \Omega: -1 \leq X(\omega) \leq 1\}).
  \end{equation*}
\end{remark}

\begin{definition}
  Let $X$ be a random variable in a probability space $(\Omega, \Sigma, P)$.
  The \emph{probability mass function} $p_X: \mathbb{R} \to \mathbb{R}$ of $X$
  is defined as
  \begin{equation*}
    p_X(x) = P(X = x)
  \end{equation*}
  for each $x \in \mathbb{R}$.
\end{definition}

\section{Expectation and Variance}
\begin{definition}
  Let $X$ be a discrete random variable in a probability space
  $(\Omega, \Sigma, P)$.
  Then the \emph{expectation} of $X$, denoted by $E[X]$, is defined as
  follows.
  \begin{itemize}
    \item If $X$ is nonnegative, i.e, $X(\omega) \geq 0$ for each
      $\omega \in \Omega$, then
      \begin{equation*}
        E[X] = \sum_{x \in X(\Omega)} x \cdot p_X(x).
      \end{equation*}
    \item Otherwise, we define $E[X] = E[X^+] - E[X^-]$,
      where $X^+ = \max\{X, 0\}$ and $X^- = \max\{-X, 0\}$.
  \end{itemize}
\end{definition}

\begin{theorem}
  Let $X$ and $Y$ be discrete random variables in a probability space
  $(\Omega, \Sigma, P)$.
  If both $E[X]$ and $E[Y]$ exist, then the following statements are true.
  \begin{enumerate}
    \item $E[aX] = aE[X]$ for $a \in \mathbb{R}$.
    \item $E[X + Y] = E[X] + E[Y]$.
  \end{enumerate}
\end{theorem}
\begin{proof} \leavevmode
  \begin{enumerate}
    \item First, suppose that $a \geq 0$. If $X$ is nonnegative, then
      so is $aX$. Thus, we have
      \begin{equation*}
        E[aX]
        = \sum_{x \in X(\Omega)} ax \cdot p_X(x)
        = aE[X].
      \end{equation*}
      If $X$ is not nonnegative, by the fact that $(aX)^+ = aX^+$ and
      $(aX)^- = aX^-$, we have
      \begin{align*}
        E[aX]
        = E[aX^+] - E[aX^-]
        = aE[X^+] - aE[X^-]
        = aE[X].
      \end{align*}
      since $X^+$ and $X^-$ are nonnegative.
      Thus the statement holds for $a \geq 0$.

      For the case that $a < 0$, note that since $(-X)^+ = X^-$ and
      $(-X)^- = X^+$, it follows that
      \begin{equation*}
        E[-X]
        = E[X^-] - E[X^+]
        = -E[X].
      \end{equation*}
      Thus, we have
      \begin{equation*}
        E[aX]
        = E[(-a)(-X)]
        = -aE[-X]
        = aE[X].
      \end{equation*}
    \item To be completed. \qedhere
  \end{enumerate}
\end{proof}

\begin{definition}
  Let $X$ be a discrete random variable in a probability space
  $(\Omega, \Sigma, P)$.
  Then the \emph{variance} of $X$ is defined as
  \begin{equation*}
    \mathrm{Var}(X) = E[(X - E[X])^2].
  \end{equation*}
\end{definition}

\begin{theorem}
  Let $X$ be a discrete random variable.
  Then $\mathrm{Var}(X) = E[X^2] - (E[X])^2$.
\end{theorem}
\begin{proof}
  It is proved by
  \begin{align*}
    \mathrm{Var}(X)
    &= E[(X - E[X])]^2 \\
    &= E[X^2 - 2X \cdot E[X] + (E[X])^2] \\
    &= E[X^2] - 2E[X] \cdot E[X] + (E[X])^2 \\
    &= E[X^2] - (E[X])^2. \qedhere
  \end{align*}
\end{proof}

\section{Bernoulli and Binomial Random Variables}
\begin{definition}
  Let $0 \leq p \leq 1$.
  A random variable $X$ is called a \emph{Bernoulli random variable} with
  parameter $p$ if $p_X(1) = p$ and $p_X(0) = 1 - p$.
\end{definition}

\begin{theorem}
  Let $X$ be a Bernoulli random variable with parameter $p$.
  \begin{enumerate}
    \item $E[X] = p$.
    \item $\mathrm{Var}(X) = p(1 - p)$.
  \end{enumerate}
\end{theorem}
\begin{proof}
  Since $p(0) + p(1) = 1$, we have $p(x) = 0$ for $x \notin \{0, 1\}$.
  \begin{enumerate}
    \item We have
      \begin{equation*}
        E[X] = \sum_{x\,:\,p_X(x) > 0} x \cdot p_X(X)
             = 1 \cdot p + 0 \cdot (1 - p)
             = p.
      \end{equation*}
    \item By (a), we have
      \begin{align*}
        \mathrm{Var}(X)
        &= E[X^2] - (E[X])^2 \\
        &= (1^2 \cdot p + 0^2 \cdot (1-p)) - p^2 \\
        &= p - p^2 \\
        &= p(1-p). \qedhere
      \end{align*}
  \end{enumerate}
\end{proof}

\begin{definition}
  Let $n$ be a nonnegative integer and $0 \leq p \leq 1$.
  A random variable $X$ is called a \emph{binomial random variable} with
  parameter $(n, p)$, if
  \begin{equation*}
    p_X(x) = \binom{n}{x} p^x (1-p)^{n-x}
  \end{equation*}
  for each $x \in \{0, \dots, n\}$.
\end{definition}

\begin{theorem}
  Let $X$ be a binomial random variable with parameter $(n, p)$.
  \begin{enumerate}
    \item $E[X] = np$.
    \item $\mathrm{Var}(X) = np(1 - p)$.
  \end{enumerate}
\end{theorem}
\begin{proof}
  We have $p(x) = 0$ for $x \notin \{0, \dots, n\}$ because
  \begin{equation*}
    \sum_{x=0}^n \binom{n}{x} p^x (1-p)^{n-x} = (p + (1-p))^n = 1.
  \end{equation*}
  Also, we have the fact that
  \begin{align*}
    E[X^k]
    &= \sum_{x=0}^n x^k \binom{n}{x} p^x (1-p)^{n-x} \\
    &= \sum_{x=1}^n x^k \binom{n}{x} p^x (1-p)^{n-x} \\
    &= np \sum_{x=1}^n x^{k-1} \binom{n-1}{x-1} p^{x-1} (1-p)^{n-x} \\
    &= np \sum_{y=0}^{n-1} (y+1)^{k-1} \binom{n-1}{y} p^y (1-p)^{(n-1)-y}
       \tag{$\ast$}
  \end{align*}
  holds for positive integer $k$.
  \begin{enumerate}
    \item By ($\ast$), the expectation of $X$ is given by
      \begin{align*}
        E[X]
        = np\sum_{y=0}^{n-1} \binom{n-1}{y} p^y (1-p)^{(n-1)-y}
        = np.
      \end{align*}
    \item By ($\ast$), we have
      \begin{align*}
        E[X^2]
        = np\sum_{y=0}^{n-1} (y+1)\binom{n-1}{y} p^y (1-p)^{(n-1)-y}
        = np((n-1)p+1).
      \end{align*}
      Thus, the variance of $X$ is given by
      \begin{align*}
        \mathrm{Var}(X)
        &= E[X^2] - (E[X])^2 \\
        &= np((n-1)p + 1) - (np)^2 \\
        &= np(1 - p). \qedhere
      \end{align*}
  \end{enumerate}
\end{proof}

\section{Poisson Random Variables}
\begin{theorem}
  Let $\lambda > 0$.
  For integer $n \geq \lambda$, let $X_n$ be a binomial random variable with
  parameter $(n, \lambda/n)$. Then for nonnegative integer $x$, we have
  \begin{equation*}
    \lim_{n \to \infty}p_{X_n}(x) = e^{-\lambda} \cdot \frac{\lambda^x}{x!}.
  \end{equation*}
\end{theorem}
\begin{proof}
  For $n \geq \lambda$, we have
  \begin{align*}
    p_{X_n}(x)
    &= \frac{n!}{(n-x)! \cdot x!} \cdot \left(\frac{\lambda}{n}\right)^x \left(\frac{n-\lambda}{n}\right)^{n-x} \\
    &= \frac{\lambda^x}{x!} \cdot \frac{n!}{(n-x)! \cdot (n-\lambda)^x} \cdot \left(\frac{n-\lambda}{n}\right)^n \\
    &= \frac{\lambda^x}{x!} \cdot \prod_{i=1}^{x} \frac{n+1-i}{n-\lambda} \cdot \left(1 - \frac{\lambda}{n}\right)^n.
  \end{align*}
  Thus,
  \begin{equation*}
    \lim_{n\to\infty} p_{X_n}(x)
    = \frac{\lambda^x}{x!} \cdot \prod_{i=1}^x \left(\lim_{n\to\infty} \frac{n+1-i}{n-\lambda}\right) \cdot \lim_{n\to\infty}\left(1 - \frac{\lambda}{n}\right)^n
    = \frac{\lambda^x}{x!} \cdot e^{-\lambda}. \qedhere
  \end{equation*}
\end{proof}

\begin{definition}
  Let $\lambda > 0$.
  A random variable $X$ is called a \emph{Poisson random variable} with
  parameter $\lambda$, if
  \begin{equation*}
    p_X(x) = e^{-\lambda} \cdot \frac{\lambda^x}{x!}
  \end{equation*}
  holds for any nonnegative integer $x$.
\end{definition}

\begin{theorem}
  Let $X$ be a Poisson random variable with parameter $\lambda$.
  \begin{enumerate}
    \item $E[X] = \lambda$.
    \item $\mathrm{Var}(X) = \lambda$.
  \end{enumerate}
\end{theorem}
\begin{proof}
  We have $p_X(x) = 0$ for $x \notin \{0, 1, 2, \dots\}$ because
  \begin{equation*}
    \sum_{x=0}^\infty p_X(x)
    = e^{-\lambda}\sum_{x=0}^\infty \frac{\lambda^x}{x!}
    = e^{-\lambda} \cdot e^{\lambda}
    = 1.
  \end{equation*}
  where the second equality follows from the fact that
  $e^t = \sum_{k=0}^\infty t^k/k!$ for $t \in \mathbb{R}$.
  \begin{enumerate}
    \item We have
      \begin{equation*}
        E[X]
        = \sum_{x=0}^\infty x \cdot \frac{e^{-\lambda}\lambda^x}{x!}
        = \sum_{x=1}^\infty x \cdot \frac{e^{-\lambda}\lambda^x}{x!}
        = \lambda e^{-\lambda}\sum_{x=1}^\infty \frac{\lambda^{x-1}}{(x-1)!}
        = \lambda e^{-\lambda}\sum_{y=0}^\infty \frac{\lambda^y}{y!}
        = \lambda.
      \end{equation*}
    \item Since
      \begin{align*}
        E[X^2]
        &= \sum_{x=0}^\infty x^2 \cdot \frac{e^{-\lambda}\lambda^x}{x!} \\
        &= \sum_{x=1}^\infty x^2 \cdot \frac{e^{-\lambda}\lambda^x}{x!} \\
        &= \lambda \sum_{x=1}^\infty x \cdot \frac{e^{-\lambda}\lambda^{x-1}}{(x-1)!} \\
        &= \lambda \sum_{y=0}^\infty (y+1) \cdot \frac{e^{-\lambda}\lambda^y}{y!} \\
        &= \lambda(\lambda + 1),
      \end{align*}
      we have
      \begin{equation*}
        \mathrm{Var}(X)
        = E[X^2] - (E[X])^2
        = \lambda(\lambda + 1) - \lambda^2
        = \lambda. \qedhere
      \end{equation*}
  \end{enumerate}
\end{proof}

\section{Geometric and Negative Binomial Random Variables}
\begin{definition}
  Let $0 \leq p \leq 1$.
  A random variable $X$ is called a \emph{geometric random variable}
  with parameter $p$, if
  \begin{equation*}
    p_X(x) = p \cdot (1-p)^{x-1}
  \end{equation*}
  holds for any positive integer $x$.
\end{definition}

\begin{definition}
  Let $r$ be a nonnegative integer and $0 \leq p \leq 1$.
  A random variable $X$ is called a \emph{negative binomial random variable}
  with parameter $(r, p)$, if
  \begin{equation*}
    p_X(x) = \binom{x-1}{r-1}p^r(1-p)^{x-r}
  \end{equation*}
  holds for any integer $x \geq r$.
\end{definition}

\begin{theorem}
  Let $X$ be a negative binomial random variable with parameter $(r, p)$.
  \begin{enumerate}
    \item $E[X] = r/p$.
    \item $\mathrm{Var}(X) = r(1-p)/p^2$.
  \end{enumerate}
\end{theorem}
\begin{proof}
  We have $p_X(x) = 0$ for $x \notin \{r, r+1, r+2, \dots\}$ because
  \begin{align*}
    \sum_{x=r}^\infty p_X(x)
    &= \sum_{x=r}^\infty \binom{x-1}{r-1} p^r (1-p)^{x-r} \\
    &= \sum_{x=r}^\infty \binom{-r}{x-r} p^r (-(1-p))^{x-r} \\
    &= p^r \sum_{y=0}^\infty \binom{-r}{y} (-(1-p))^y \\
    &= p^r (1-(1-p))^{-r} \\
    &= 1,
  \end{align*}
  where the second equality follows from
  \begin{equation*}
    \binom{x-1}{r-1}
    = \binom{x-1}{x-r}
    = \binom{-r}{x-r} \cdot (-1)^{x-r}.
  \end{equation*}
  \begin{enumerate}
    \item To be completed.
    \item To be completed. \qedhere
  \end{enumerate}
\end{proof}

\section{Hypergeometric Random Variables}
\begin{definition}
  Let $n, K, N$ be nonnegative integers with $n \leq N$ and $K \leq N$.
  A random variable $X$ is called a \emph{hypergeometric random variable}
  with parameter $(n, K, N)$ if
  \begin{equation*}
    p_X(x) = \frac{\binom{K}{x}\binom{N-K}{n-x}}{\binom{N}{n}}
  \end{equation*}
  holds for any integer $x \in \{0, 1, \dots, K\}$.
\end{definition}
\begin{remark}
  A hypergeometric random variable with parameter $(1, K, N)$ is a Bernoulli
  random variable with parameter $K/N$.
\end{remark}
\begin{remark}
  A hypergeometric random variable $X$ with parameter $(n, K, N)$ can be seen
  as the number of successes in $n$ draws without replacement from a
  population of size $N$ that contains $K$ objects that represent success.
\end{remark}
\begin{remark}
  If $N$ and $K$ are large compared to $n$, then a hypergeometric random
  variable $X$ with parameter $(n, K, N)$ behaves like a binomial
  random variable with parameter $(n, K/N)$.
\end{remark}