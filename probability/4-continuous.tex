\chapter{Continuous Random Variables}
\section{Continuous Random Variables}
\begin{definition}
  A random variable $X$ is a \emph{continuous random variable}
  if there exists a nonnegative function $f_X: \mathbb{R} \to \mathbb{R}$
  such that
  \begin{equation*}
    P(X \in S) = \int_S f_X(x) dx
  \end{equation*}
  holds for any $S \in \mathcal{B}$.
  The function $f_X$ is called a \emph{probability density function} of $X$.
\end{definition}

\begin{theorem}
  Let $X$ be a continuous random variable in a probability space
  $(\Omega, \Sigma, P)$.
  Then $p_X(a) = 0$ for $a \in \mathbb{R}$.
\end{theorem}
\begin{proof}
  It is proved by
  \begin{equation*}
    p_X(a) = P(X = a) = \int_a^a f_X(x)dx = 0. \qedhere
  \end{equation*}
\end{proof}

\begin{definition}
  The \emph{cumulative distribution function} $F_X$ of a random variable $X$
  is defined by
  \begin{equation*}
    F_X(x) = P(X \leq x)
  \end{equation*}
  for all $x \in \mathbb{R}$.
\end{definition}

\begin{theorem}
  Let $X$ be a continuous random variable in a probability space
  $(\Omega, \Sigma, P)$.
  If $f_X$ is a probability density function of $X$, then
  \begin{equation*}
    F_X(x) = \int_{-\infty}^x f_X(t) dt.
  \end{equation*}
  holds for all $x \in \mathbb{R}$.
\end{theorem}
\begin{proof}
  For all $x \in \mathbb{R}$, we have
  \begin{equation*}
    F_X(x) = P(X \leq x)
           = \int_{-\infty}^x f_X(t) dt. \qedhere
  \end{equation*}
\end{proof}

\section{Expectation and Variance}

\section{Uniform Random Variables}
\begin{definition}
  Let $a, b$ be real numbers with $a < b$.
  A continuous random variable $X$ is called a \emph{uniform random variable}
  with parameters $(a, b)$ if the function $f_X: \mathbb{R} \to \mathbb{R}$
  with
  \begin{equation*}
    f_X(x) = \left\{
    \begin{array}{ll}
      \dfrac{1}{b-a}, &\ \text{if}\ a \leq x \leq b \\[1em]
      0,              &\ \text{otherwise}
    \end{array}
    \right.
  \end{equation*}
  for $x \in \mathbb{R}$ is a probability density function of $X$.
\end{definition}

\section{Normal Random Variables}
\begin{definition}
  Let $\mu, \sigma$ be real numbers with $\sigma \geq 0$.
  A continuous random variable $X$ is called a \emph{normal random variable}
  with parameters $(\mu, \sigma^2)$ if the function
  $f_X: \mathbb{R} \to \mathbb{R}$ with
  \begin{equation*}
    f_X(x) = \frac{1}{\sqrt{2\pi}\sigma}e^{-(x-\mu)^2/(2\sigma^2)}
  \end{equation*}
  for $x \in \mathbb{R}$ is a probability density function of $X$.
\end{definition}