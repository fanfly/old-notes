\chapter{Jointly Distributed Random Variables}
\section{Jointly Distributed Random Variables}
\begin{definition}
  Let $X$ and $Y$ be random variables on a probability space
  $(\Omega, \Sigma, P)$.
  \begin{itemize}
    \item The function $p_{X,Y}: \mathbb{R}^2 \to \mathbb{R}$ with
      \begin{equation*}
        p_{X,Y}(x, y) = P(X = x\ \text{and}\ Y = y)
      \end{equation*}
      for $x, y \in \mathbb{R}$ is the
      \emph{joint probability mass function} of $X$ and $Y$.
    \item If there exists a nonnegative function
      $f_{X,Y}: \mathbb{R}^2 \to \mathbb{R}$ such that
      \begin{equation*}
        \iint\limits_S f_{X,Y}(x, y)\,dx\,dy
        = P((X, Y) \in S)
      \end{equation*}
      holds for all $S \in \mathcal{B}^2$, then $f_{X,Y}$ is a
      \emph{joint probability density function} of $X$ and $Y$.
    \item The function $F_{X,Y}: \mathbb{R}^2 \to \mathbb{R}$ with
      \begin{equation*}
        F_{X,Y}(x, y) = P(X \leq x\ \text{and}\ Y \leq y)
      \end{equation*}
      for $x, y \in \mathbb{R}$ is the
      \emph{joint cumulative distribution function} of $X$ and $Y$.
  \end{itemize}
\end{definition}
\begin{definition}
  Let $X_1, X_2, \dots, X_n$ be random variables on a probability space
  $(\Omega, \Sigma, P)$.
  \begin{itemize}
    \item The function $p: \mathbb{R}^n \to \mathbb{R}$ with
      \begin{equation*}
        p(x_1, \dots, x_n)
        = P(X_i = x_i\ \text{for}\ i \in \{1, \dots, n\})
      \end{equation*}
      for $x_1, \dots, x_n \in \mathbb{R}$ is the
      \emph{joint probability mass function} of $X_1, \dots, X_n$.
    \item If there exists a nonnegative function
      $f: \mathbb{R}^n \to \mathbb{R}$ such that
      \begin{equation*}
        \idotsint\limits_S f(x_1, \dots, x_n)\,dx_1 \cdots dx_n
        = P((X_1, \dots, X_n) \in S)
      \end{equation*}
      holds for all $S \in \mathcal{B}^n$, then $f$ is a
      \emph{joint probability density function} of $X_1, \dots, X_n$.
    \item The function $F: \mathbb{R}^n \to \mathbb{R}$ with
      \begin{equation*}
        F(x_1, \dots, x_n)
        = P(X_i \leq x_i\ \text{for}\ i \in \{1, \dots, n\})
      \end{equation*}
      for $x_1, \dots, x_n \in \mathbb{R}$ is the
      \emph{joint cumulative distribution function} of $X_1, \dots, X_n$.
  \end{itemize}
\end{definition}

\section{Independent Random Variables}
\begin{definition}
  Let $X$ and $Y$ be random variables on a probability space
  $(\Omega, \Sigma, P)$.
  Then $X$ and $Y$ are \emph{independent} if the events $X \in S_1$ and
  $Y \in S_2$ are independent for any $S_1, S_2 \in \mathcal{B}$.
\end{definition}

\begin{definition}
  Let $X_1, X_2, \dots, X_n$ be random variables on a probability space
  $(\Omega, \Sigma, P)$.
  Then $X_1, X_2, \dots, X_n$ are \emph{independent} if the events
  $X_1 \in S_1$, $X_2 \in S_2$, \dots, $X_n \in S_n$ are independent
  for any $S_1, S_2, \dots, S_n \in \mathcal{B}$.
\end{definition}

\section{Sums of Independent Random Variables}
\begin{theorem}
  Let $X$ and $Y$ be independent continuous random variables and $Z = X + Y$.
  Then $f_Z: \mathbb{R} \to \mathbb{R}$ with
  \begin{equation*}
    f_Z(z) = \int_{-\infty}^\infty f_X(x) \cdot f_Y(z-x)\,dx
  \end{equation*}
  for $z \in \mathbb{R}$ is a probability density function of $Z$, where $f_X$
  and $f_Y$ are probability density functions of $X$ and $Y$, respectively.
\end{theorem}
\begin{proof}
  We have
  \begin{align*}
    F_Z(z)
    &= P(X + Y \leq z) \\
    &= \int_{-\infty}^\infty \int_{-\infty}^{z-x} f_{X,Y}(x, y)\,dy\,dx \\
    &= \int_{-\infty}^\infty \int_{-\infty}^z f_{X,Y}(x, u-x)\,du\,dx
       \tag{$u = x + y$} \\
    &= \int_{-\infty}^z \int_{-\infty}^\infty f_{X,Y}(x, u-x)\,dx\,du.
  \end{align*}
  Thus,
  \begin{align*}
    \frac{d}{dz}F_Z(z)
    = \int_{-\infty}^\infty f_{X,Y}(x, z-x)\,dx
    = \int_{-\infty}^\infty f_X(x) \cdot f_Y(z-x)\,dx
    = f_Z(z),
  \end{align*}
  implying $f_Z$ is a probability density function of $Z$.
\end{proof}