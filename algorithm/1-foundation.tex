\chapter{Foundations}
\section{Computational Problems and Algorithms}
\begin{definition}
  A \emph{computational problem} is a relation
  \begin{equation*}
    P \subseteq X \times Y,
  \end{equation*}
  where $X$ is called the set of \emph{instances} and $Y$ is called the sets of
  \emph{solutions}.
\end{definition}

\begin{definition}
  We will assume the \emph{random-access machine (RAM)} model of computation as
  our implementaion technology for most of this note.
  In this model, we have an infinite sequence of $w$-bit words, and we assume
  $w = \lceil c \lg n \rceil$ for some constant $c \geq 1$, where $n$ is the
  input size.
  We can perform some basic operations on these words, including
  \begin{itemize}
    \item arithmetic operations (e.g., addition, subtraction, multiplication,
    division),
    \item data movement operations (e.g., load, store, copy), and
    \item control operations (e.g., branch, subroutine call, return).
  \end{itemize}
\end{definition}

\begin{definition}
  Given a computational model, an \emph{algorithm} is defined as a finite
  sequence of basic operations that transforms a given input into a unique
  output.
  \begin{itemize}
    \item We say that an algorithm \emph{solves} a computational problem
    $P \subseteq X \times Y$ if it transforms every instance $x \in X$ into a
    solution $y \in Y$ such that $(x, y) \in P$.
    \item The \emph{running time} of an algorithm on a specific input is
    defined as the number of basic operations performed.
  \end{itemize}
\end{definition}